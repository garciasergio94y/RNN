---
title: 'Aplicación del aprendizaje automático al laboratorio de diagnóstico clínico:
  Detección temprana de series analíticas fuera de control en análisis inmunoquímicos
  de muestras de sangre mediante algoritmos de Machine Learning'
author: "Sergio García Muñoz"
date: "`r Sys.Date()`"
output: html_document
---

# LSTM_onehot_samples

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Instalación y carga de paquetes requeridos:

```{r, include=FALSE}
# Definir los nombres de los paquetes que se desean instalar
packages <- c("readr", "readxl", "purrr", "dplyr", "knitr",
              "filesstrings",
              "stringr", "tidyr", "lubridate", "ggplot2", "caret",
              "tictoc", "wavelets", "reticulate", "abind",
              "tensorflow", "tfdatasets", "keras", "ModelMetrics")

# Función para instalar paquetes si no están ya instalados
install_packages <- function(package) {
  if (!require(package, character.only = TRUE)) {
    install.packages(package)
    }
}

# Aplicar la función para cada uno de los paquetes
lapply(packages, install_packages)

```

```{r}
require(readr)
require(readxl)
require(purrr)
require(dplyr)
require(knitr)
require(filesstrings)
require(stringr)
require(tidyr)
require(lubridate)
require(ggplot2)
require(caret)
require(wavelets)
require(tictoc)
require(reticulate)
require(abind)
require(tensorflow)
require(tfdatasets)
require(keras)
require(ModelMetrics)
```

## Definición de rutas y directorios

```{r directory definition}
# Definir el directorio donde se encuentran los archivos de datos:
workingdir <- getwd()
datadir <- file.path(workingdir, "Datos/daily_s")
eventdir <- file.path(workingdir, "Datos/daily_s/Event")
eventdir_old <- file.path(workingdir, "Datos/daily_s/Event/old")
lotdir <- file.path(workingdir, "Datos/daily_s/Lot")
lotdir_old <- file.path(workingdir, "Datos/daily_s/Lot/old")
qcdir <- file.path(workingdir, "Datos/daily_s/qc")
qcdir_old <- file.path(workingdir, "Datos/daily_s/qc/old")
resultsdir <- file.path(workingdir, "Resultados")
figuresdir <- file.path(workingdir, "Resultados/Figuras")

# Definir el intervalo de tiempo en segundos entre las lecturas
interval <- 60
```

# Definición de funciones:

## Función ```read_lot```.

Lee los archivos csv de lotes y crear un data frame.

```{r read_lot}

read_lot <- function(CLC=NULL) {
  # Crea una lista de archivos existentes en datadir            correspondientes a los lotes diarios:
  files_list <- list.files(lotdir, pattern = "Lote_",
                           full.names = T) 
  
  # Especifica los tipos de datos que contiene cada columna:
  col_types <- cols(
  PACIENTE_EDAD = col_integer(),
  RESULTADO = col_double()
  )
  
  # Especifica que el separador decimal es ",":
  locale_decimal_comma <- locale(decimal_mark = ",")
  
  # Crea una lista de dataframes leyendo cada archivo:
  df_list <- map(files_list, 
                 ~read_delim(.x, 
                             delim = "|",
                             col_select = -8,
                             col_types = col_types,
                             trim_ws = T,
                             locale
                             =locale_decimal_comma))
  
  # Crea el dataframe final:
  df <- bind_rows(df_list)
  
  # Convierte múltiples formatos de fecha y hora de la columna         FECHA_RECEPCIÓN en formato POSIXct, cambia formato fecha y         nombre de columna:
  
  df[[1]] <- dmy_hms(df[[1]])
  colnames(df)[1] <- "TIEMPO_MUESTRA"
  
  # Elimina una parte del string del nombre del analizador no utilizable:
  df[[2]] <- substring(df[[2]],10, 21)
  
  # Filtra los resultados según el CLC introducido si es distinto de nulo:
  if (!is.null(CLC)){
    df %<>% filter(., CODIGO_PRUEBA == CLC)
  }
  
  # Mueve el archivo leído a la carpeta old:
  # file.move(files_list, lotdir_old, overwrite = TRUE)
  
  return(df)
}
```

## Función ```read_lotX```.

Lee los archivos csv de lotes con distintos formatos de fecha y crear un data frame.

```{r read_lotX}

read_lotX <- function(CLC=NULL) {
  # Crea una lista de archivos existentes en datadir            correspondientes a los lotes diarios:
  files_list <- list.files(lotdir, pattern = "LoteX_",
                           full.names = T) 
  
  # Especifica los tipos de datos que contiene cada columna:
  col_types <- cols(
  PACIENTE_EDAD = col_integer(),
  RESULTADO = col_double()
  )
  
  # Especifica que el separador decimal es ",":
  locale_decimal_comma <- locale(decimal_mark = ",")
  
  # Crea una lista de dataframes leyendo cada archivo:
  df_list <- map(files_list, 
                 ~read_delim(.x, 
                             delim = "|",
                             col_select = -8,
                             col_types = col_types,
                             trim_ws = T,
                             locale
                             =locale_decimal_comma))
  
  # Crea el dataframe final:
  df <- bind_rows(df_list)
  
  # Convierte múltiples formatos de fecha y hora de la columna         FECHA_RECEPCIÓN en formato POSIXct, cambia formato fecha y nombre    de columna:
  
  df[[1]] <- mdy_hm(df[[1]], tz=Sys.timezone())
  colnames(df)[1] <- "TIEMPO_MUESTRA"
  
  # Elimina una parte del string del nombre del analizador   no utilizable:
  df[[2]] <- substring(df[[2]],10, 21)
  
  # Filtra los resultados según el CLC introducido si es distinto de nulo:
  if (!is.null(CLC)){
    df %<>% filter(., CODIGO_PRUEBA == CLC)
  }
  
  # Mueve el archivo leído a la carpeta old:
  # file.move(files_list, lotdir_old, overwrite = TRUE)
  
  return(df)
}
```

## Función ```read_event```.

Lectura de los archivos csv de eventos y crear un data frame.

```{r read_event}

read_event <- function() {
  # Crea una lista de archivos existentes en datadir            correspondientes a los eventos de QC diarios:
  files_list <- list.files(eventdir, pattern = "Evento_",
                           full.names = T) 
  
  # Especifica los tipos de datos que contiene cada columna:
  col_types <- cols(
  FECHA = col_character(),
  LOTE = col_integer())
  
  # Crea una lista de dataframes leyendo cada archivo:
  df_list <- map(files_list,
                 ~read_delim(.x,
                             delim = "|",
                             col_types = col_types, 
                             col_select = -c("LOTE"),
                             trim_ws = T))
  
  # Crea el dataframe final:
  df <- bind_rows(df_list)
  
  # Convierte la fecha leída como string en formato POSIXct y cambia   nombre a columna:
  df[[1]] <- as.POSIXct(gsub(",", ".", df[[1]]),
               format = "%d/%m/%y %H:%M:%S",
               tz=Sys.timezone())
  colnames(df)[[1]] <- "TIEMPO_EVENTO"
  
  # Elimina substring no deseado en el nombre del            analizador:
  df[[2]] <- substring(df[[2]],10, 21)

  # Mueve el archivo leído a la carpeta old:
  #file.move(files_list, eventdir_old, overwrite = TRUE)
  
  return(df)
}
```

## Función ```read_qc```.

Lee los archivos xls de valores de QC para cada técnica y cada máquina, añade una columna con la id del equipo, otra con el código CLC de la prueba y crea un data frame final. La función contiene una condición if diseñada por el diferente formato de exportación de los archivos del control en uso y el histórico de controles inactivos.

```{r read_qc}

read_qc <- function() {
  # Crear una lista de archivos existentes en datadir que contienen los valores de QC, tanto en uso como inactivos:
  files_list <- list.files(qcdir, pattern = "CLC",
                           full.names = T) 
  
  # Lista vacía para contener los dataframes originados en el loop:
  list_df <- list()
 
  # Loop for para crear una lista de dataframes leyendo cada           archivo de qc:
  for(i in 1:length(files_list)){
    data <- read_xls(files_list[i], skip = 4)
  
    # Buscar y extraer el id del equipo en la primera fila:
    dev_ids <- c("DXI800 num 1", "DXI800 num 2",
             "DXI800 num 3")
    first_rows <- read_xls(files_list[i], 
                        col_names = F, n_max = 3) 
    match_dev <- which(grepl(paste(dev_ids, 
                                   collapse = "|"),
                                   first_rows))
    start_pos <- regexpr("(?<=DxI)\\S+",
                         first_rows[match_dev],
                         perl = TRUE)
    dev <- substring(first_rows[match_dev], start_pos)
    
    # Extraer el código CLC del nombre del archivo
    start_pos <- regexpr("(?<=CLC)\\S+",
                         files_list[i],
                         perl = TRUE)
    end_pos <- regexpr("\\)", files_list[i], start_pos)
    clc <- substring(files_list[i], start_pos, end_pos - 1)
    
    # Añadir como columnas "ANALIZADOR" y "CODIGO_PRUEBA" las           cadenas extraídas:
    data %<>% mutate("ANALIZADOR"= substr(dev, 13, 24),
                     "CODIGO_PRUEBA"= paste("CLC",clc, sep = "")) %>%
              relocate("ANALIZADOR", "CODIGO_PRUEBA", .after = 5)
    
    # En caso de que el archivo de datos sea del histórico, la    identificación del control no se encuentra como columna, sino en la cabecera, ya que los archivos han sido generados para cada  nivel de control individualmente. 
    # Identificación de este tipo de archivo, extracción de la identificación y creación de la columna "Control": 
    
    if(!("Control" %in% colnames(data))) {
      # Extraer el nombre del control de la fila 3, columna D:
      cont <- read_xls(files_list[i], 
                        col_names = F, range = "D2:D2")
      data %<>% mutate("Control"= as.character(cont)) %>%
              relocate("Control", .after = 4)
      }
    
    # Agrega el dataframe actual a la lista:
    list_df[[i]] <- data
  }  
    
  for (i in seq_along(list_df)) {
  if (!is.numeric(list_df[[i]][[3]])) {
    list_df[[i]][[3]] <- as.numeric(as.character(list_df[[i]][[3]]))
  }
}

  # Unir todos los dataframes en uno solo:
  df <- bind_rows(list_df)
  
  # Cambia el nombre a la columna Lote de reactivos:
  colnames(df)[10] <- "LOTE_REACTIVO"
  
  # Fusiona las columnas de los valores de concentraciones         encontrados en distintas unidades en una sola columna y cambia el tipo de valor a numérico:
   df <- unite(df, "Resultado", c(3, 13:21),
                          sep = "", na.rm = T)
   df[[3]] %<>% as.numeric()
   
  # Reordena el dataframe:
   df %<>% relocate("Sup/Inf Media", .after = 4)
   
  # Cambia el nombre a la columna Lote de reactivos:
   colnames(df)[3] <- "QC_RESULT"
  
  # Elimina filas con valores NA en LOTE_REACTIVO:
  
   df <- df[complete.cases(df$LOTE_REACTIVO), ]
  
   # Mueve el archivo leído a la carpeta old:
  # file.move(files_list, qc_old, overwrite = TRUE)
  
  return(df)
}
```

## Función ```figure_ext```.

Extracción de datos numéricos de la columna media y sd.

```{r figure_ext}

figure_ext <- function(df, col_number) {
  # Extrae números presentes en la columna especificada:
  mean_val <- c()
  sd_val <- c()
  for (i in 1:nrow(df[col_number])){
    numbers <- str_extract_all(df[i, col_number],
                             "\\d+(\\.\\d+)?") 
  
    # Convertir a decimal y hallar media y sd:
    numbers <- as.double(unlist(numbers))
    mean_val <- rbind(mean_val, 
                        (numbers[1] + numbers[2])/2)
    sd_val <- rbind(sd_val, 
                      (numbers[2] - numbers[1])/2)
  }
  
  # Crear nuevas columnas media y sd:
  
  df %<>% mutate("Media" = mean_val, "SD" = sd_val) %>%
          relocate("Media", "SD", .after =3)
  
  # Borrar columna original
  df[[col_number]] <- NULL
  
  # Devolver el dataframe modificado
  return(df)
}
```

## Función ```merge_date```.

Fusiona columnas Fecha y Hora en una nueva columna Fecha_hora y borra las columnas individuales.

```{r merge_date}

  merge_date <- function(df){
    df %<>%
      mutate("TIEMPO_QC" = 
               as.POSIXct(paste(Fecha, Hora),
                          format = "%d/%m/%y %H:%M:%S",
                          tz=Sys.timezone()),
             .before = 1)
    # Borrar columnas originales
    df[["Fecha"]] <- NULL
    df[["Hora"]] <- NULL
        
  return(df)
}
```

## Función ```merge_feat_qc```.

Función que une los dataframes features día D y qc_results día D+1 en uno solo mediante Left outer join.

```{r merge_feat_qc}

merge_feat_qc <- function(df1, df2){
  # Crear timestamps con solo la fecha: 
  df1$rounded_date <- floor_date(as.Date(df1[[1]])+1,
                                 unit = "day")
  df2$rounded_date <- floor_date(as.Date(df2[[1]]),
                                 unit = "day")
  
  # Crear un nuevo dataframe con merge (equivale a left outer join):
  merged_df <- merge(df1, df2, 
                     by = c("rounded_date", "ANALIZADOR",
                            "CODIGO_PRUEBA", "LOTE_REACTIVO"))
  
  # Borrar los timestamps con solo fecha:
  merged_df$rounded_date <- NULL
  merged_df$rounded_date <- NULL
  
  return(merged_df)

}

```

## Función ```merge_feat_qc_event```.

Une los dataframes feat_qc y events a día D en uno solo mediante Left outer join. Añade la posibilidad de filtrar por códigos de prueba para seguir trabajando con un subconjunto de las mismas.

```{r merge_feat_qc_event}

merge_featqc_event <- function(df1, df2, CLC=NULL){
  
  # Crear rounded_date sin hora, solo fecha:
  df1$rounded_date <- floor_date(as.Date(df1[[10]]),
                                 unit = "day")
  df2$rounded_date <- floor_date(as.Date(df2[[1]]),
                                 unit = "day")
  
  # Crear un nuevo dataframe con merge (equivale a left outer join):
  merged_df <- merge(df1, df2, 
                     by = c("rounded_date", "ANALIZADOR",
                            "CODIGO_PRUEBA"), all.x = T)  %>%
               unique()                      
  
  # Borrar rounded_date:
  merged_df$rounded_date <- NULL
  
  # Filtrado en caso de que se requiera por CODIGO_PRUEBA:
  if (!is.null(CLC)){
    merged_df %<>% filter(., CODIGO_PRUEBA == CLC)
  }
  
  return(merged_df)

}
```

## Función ```wt```.

Función para la aplicación de la transformación wavelet de tipo Maximal Overlap Discrete Wavelet Transformation (MODWT) que devuelve los coeficientes de wavelet y los coeficientes de escalado.

```{r wavelet transform function definition}
wt <- function(x, wavefun){
  mod <- modwt(x, wavefun, boundary = "periodic")
return(mod)
}
```

## Función ```wavelet_tr```.

Aplica la transformación wavelet a un dataframe df agrupado según variables agrupadas y devuelve un dataframe con los coeficientes y escalas para cada observación.

```{r wavelet transform}

wavelet_tr <- function(df) {
  # Aplicar la función wt a cada grupo por separado
  df_wavelet <- df %>%
    group_by(ANALIZADOR, CODIGO_PRUEBA) %>%
    mutate(
      across(RESULTADO, ~ cbind(.x, as.data.frame(wt(.x, "la20")@W),
                                as.data.frame(wt(.x, "la20")@V)))
    ) %>%
    ungroup() %>%
    unnest(cols = RESULTADO)
  
  # Reordenar columnas
  df_wavelet <- df_wavelet %>%
    relocate(starts_with("RESULTADO"), .after = 1) %>%
    relocate(starts_with("W"), .after = 2) %>%
    relocate(starts_with("V"), .before = PACIENTE_SEXO)
  
  # Sustituir valores NA en los coeficientes wavelet por un valor arbitrario -99999 para su posterior filtrado
  wavelet_cols <- grep("^[WV]", colnames(df_wavelet), value = TRUE)
  
  return(df_wavelet)
}
```

## Función ```normalize_train```.

Para la normalización de las variables continuas del dataset de entrenamiento y sustitución de valores NA en los coeficientes wavelet por un valor numérico arbitrario fácilmente enmascarable. Devuelve también una lista de vectores de medias y de desviaciones estándar que luego se usan como argumentos en para las funciones de normalización de los datasets de validación y test. De esta forma, el escalado y normalización se realiza con la misma media y desviación del set de entrenamiento para los otros dos datasets.

```{r train data normalization and wavelet NA replace}

normalize_train <- function(data) {
   
  # Identificar las columnas que empiezan por W y la columna 'RESULTADO'
  wavelet_cols <- grep("^[WV]", colnames(data), value = TRUE)
  result_col <- "RESULTADO"
  
  # Calcular medias y desviaciones estándar por grupos de pruebas y   analizador:
  means <- data %>%
    group_by(ANALIZADOR, CODIGO_PRUEBA) %>%
    summarise(across(c(result_col, wavelet_cols),
                     ~mean(., na.rm = T))) 
  
  std_devs <- data %>% 
    group_by(ANALIZADOR, CODIGO_PRUEBA) %>% 
    summarise(across(c(result_col, wavelet_cols),
                     ~sd(., na.rm = T)))
  
  # Normalizar las columnas de resultado y coeficientes wavelet
  norm_data <- data %>%
    group_by(ANALIZADOR, CODIGO_PRUEBA) %>%
    mutate(across(c(result_col, wavelet_cols), scale))%>%
    mutate(across(wavelet_cols,
                           ~ifelse(is.na(.), -99999, .))) %>%
    ungroup()
  
  # Devolver el dataframe escalado y las medias y desviaciones estándar:
  
  return(list(scaled_data = as.data.frame(norm_data), 
              means = means, stdev = std_devs))
}
```

## Función ```norm_test_val```.

Análoga a ```normalize_train```, pero con la diferencia de que usa la media y desviación estándar del dataset de entrenamiento para validar los datasets de validación y test:

```{r val and test data normalization and wavelet NA replace}

norm_test_val <- function(data, means, stds) {
  
  # Identificar las columnas numéricas y seleccionarlas
  wavelet_cols <- grep("^[WV]", colnames(data), value = TRUE)
  result_col <- c("RESULTADO")
  test_cols <- "CODIGO_PRUEBA"
  cols <- c(result_col, wavelet_cols, test_cols)
  
  # Seleccionar medias y desviaciones de resultados y coeficientes   wavelet:
  means_r <- train_n$means
  stds_r <- train_n$stdev
  means_wl <- train_n$means[wavelet_cols]
  stds_wl <- train_n$stdev[wavelet_cols]
  
  # Normalizar por cada prueba las columnas de resultado y coeficientes wavelet usando las medias y desviaciones estándar de train suministradas:
  means <- data %>%
    group_by(ANALIZADOR, CODIGO_PRUEBA) %>%
    left_join(means_r, by = c("ANALIZADOR", "CODIGO_PRUEBA"), 
              suffix = c(".val", ".means")) %>%
     ungroup()
   
  stds <- data %>%
    group_by(ANALIZADOR, CODIGO_PRUEBA) %>%
     left_join(stds_r, by = c("ANALIZADOR", "CODIGO_PRUEBA"),
               suffix = c(".val", ".sd")) %>%
    ungroup()
     
    
  norm_data <- data %>%
    mutate(., RESULTADO.st =
             (.[,result_col]-
                select(means,
                       starts_with(result_col) &
                       ends_with(".means"))) /
                select(stds,
                       starts_with(result_col) &
                       ends_with(".sd"))
           ) %>%
    mutate(WAVELETS.st =
             across(all_of(wavelet_cols),
                    ~ (. - select(means, 
                                  starts_with(cur_column()) &
                                    ends_with(".means"))[[1]]) /
                      select(stds,
                             starts_with(cur_column()) &
                               ends_with(".sd"))[[1]])
           ) %>%
    select(-c(result_col, wavelet_cols)) %>%
    unnest(c(WAVELETS.st, RESULTADO.st)) %>%
    relocate(starts_with("RESULTADO"), .after = 1) %>%
    relocate(starts_with("W"), .after = 2) %>%
    relocate(starts_with("V"), .before = PACIENTE_SEXO) %>%
    mutate(across(wavelet_cols,
                           ~ifelse(is.na(.), -99999, .)))
    
  names(norm_data)[2]<-result_col
  
  return(as.data.frame(norm_data))
}

```

## Función ```code_cat_var```.

Convierte las variables categóricas tipo string en vectores numéricos y realiza una codificción one-hot, creando una lista que retiene en un vector la información de los niveles de los factores. 

```{r one-hot coding functionq1}

code_cat_var <- function(data) {
  # Eliminación de la columna CODIGO_PRUEBA si solo tiene una          categoría:
  if(length(unique(data$CODIGO_PRUEBA))==1){
    data %<>% select(., -CODIGO_PRUEBA)
    }
  
  # Eliminación de los casos con sexo indeterminado, debido a que son   muy minoritarios y crean una variable innecesaria:
  
  data %<>% filter(PACIENTE_SEXO != "U")
  
  # Detección de strings:
  cat_vars <- which(sapply(data, is.character))
  # Conversión a factores:
  data_cat <- lapply(data[,cat_vars], as.factor)
  # Extracción de niveles:
  levels <- lapply(data_cat, levels)
  
  # Codificar one-hot todas las variables factor:
  encoded_cols <- lapply(seq_along(data_cat), function(i) {
    cols <- model.matrix(~ factor(data_cat[[i]]) - 1)
    colnames(cols) <- paste0(names(data)[cat_vars[i]], "_",
                             levels[[i]])
    cols
    })
  
  # Unir columnas codificadas con conjunto de datos original:
  if (!"CODIGO_PRUEBA" %in% names(data)) {
    data_encoded <- bind_cols(data %>% 
                                select(-all_of(cat_vars)),
                                encoded_cols)
    } else {
      data_encoded <-  bind_cols(data %>%
                                   select(c(-all_of(cat_vars),
                                            -"CODIGO_PRUEBA")),
                                 encoded_cols)
      }
  
    # Reordenar columnas:
  data_encoded <- data_encoded %>%
  relocate(CTRL, .after = everything())
  
  return(list(data = data_encoded, levels = levels))
}
```

## Función ```unique_lab_cases```.

Crea un índice de casos únicos de combinaciones de variables categóricas.

```{r unique_lab_cases}

unique_lab_cases <- function(df, categorical_vars) {
  unique_groups <- unique(df[categorical_vars])
  
  code <- apply(unique_groups, 1, function(row) {
    paste0(ifelse(row == 1, 1, 0), collapse = "")
  })
  unique_groups %<>% mutate(., code)
  coded_df <- df %>%
    merge(unique_groups, ., by = colnames(unique_groups)
          [-length(colnames(unique_groups))])
  coded_df <- coded_df[, c(colnames(df), "code")]
  
  return(coded_df)
}

```

## Función ```create_lstm_data.1```.

Diseñada para la creación de lotes de series temporales de longitud k (lookback). Basada en la función presentada en Chollet François, Deep learning with Python. Shelter Island, NY: Manning Publications Co; 2018. 335 p. Cap. 6.3.2 

```{r, k length time series generator function}

create_lstm_data.1 <- 
  function(data, lookback, delay, min_index, max_index,
           shuffle = FALSE, batch_size, step = 1,
           predseries) {
    
  if (is.null(max_index)) max_index <- nrow(data) - delay
  i <- min_index + lookback
  gen <- function() {
    if (shuffle) {
      rows <- sample(c((min_index+lookback):max_index), 
                     size = batch_size)
    } else {
      if (i + batch_size >= max_index)
        i <<- min_index + lookback
      rows <- c(i:min(i+batch_size, max_index))
      i <<- i + length(rows)
    }
    samples <- array(0, dim = c(length(rows),
                                lookback / step,
                                dim(data)[[-1]]))
    targets <- array(0, dim = c(length(rows)))
    for (j in 1:length(rows)) {
      indices <- seq(rows[[j]] - lookback, rows[[j]],
                     length.out = dim(samples)[[2]])
      samples[j,,] <- data[indices, ]
      targets[[j]] <- data[rows[[j]] + delay, predseries]
    }
    list(samples, targets)
  }
  return(gen)
}

```

## Funciones ```train_gen``` y ```val_gen```.

Funciones generadoras globales de series para train y validation, Estas funciones generan de manera indefinida series con los datos de entrenamiento y validación, usando de manera consecutiva las funciones generadoras individuales almacenadas junto a los subconjuntos de datos con las posibles combinaciones de variables categóricas. Si se llega a la generación última se repite el proceso desde la primera. Incluyen un TryCatch para evitar la parada del modelo si se encuentran secuencias de datos vacías.  

### Secuenciales:

```{r custom generator}
index_train <- 1

train_gen_seq <- function() {
  if (index_train > length(train_k)) {
    index_train <<- 1
  }
  
  sequence <- NULL
  counter <- 0
  
  while (is.null(sequence) && counter < length(train_k)) {
    sequence <- tryCatch(
      train_k[[index_train]](),
      error = function(e) {
        warning("Empty sequence error occurred in the generator.
                Trying the next one.")
        return(NULL)  # Devuelve NULL si ocurre un error en la función generadora
      }
    )
    
    index_train <<- index_train + 1
    counter <<- counter + 1
  }
  
  return(sequence)
}


index_val <- 1

val_gen_seq <- function() {
  if (index_val > length(val_k)) {
    index_val <<- 1
  }
  
  sequence <- NULL
  counter <- 0
  
  while (is.null(sequence) && counter < length(val_k)) {
    sequence <- tryCatch(
      val_k[[index_val]](),
      error = function(e) {
        warning("Empty sequence error occurred in the generator.
                Trying the next one.")
        return(NULL)  # Devuelve NULL si ocurre un error en la                               función generadora
      }
    )
    
    index_val <<- index_val + 1
    counter <<- counter + 1
  }
  
  return(sequence)
}

index_test <- 1

test_gen_seq <- function() {
  if (index_test > length(test_k)) {
    test_val <<- 1
  }
  
  sequence <- NULL
  counter <- 0
  
  while (is.null(sequence) && counter < length(test_k)) {
    sequence <- tryCatch(
      test_k[[index_test]](),
      error = function(e) {
        warning("Empty sequence error occurred in the generator.
                Trying the next one.")
        return(NULL)  # Devuelve NULL si ocurre un error en la                               función generadora
      }
    )
    
    index_test <<- index_test + 1
    counter <<- counter + 1
  }
  
  return(sequence)
}

```

### Aleatorizadas:

```{r random custom generator}

train_gen <- function() {
  indices <- sample(length(train_k))
  
  for (i in indices) {
    sequence <- tryCatch(
      train_k[[i]](),
      error = function(e) {
        warning("Empty sequence error occurred in the generator.
                Trying the next one.")
        return(NULL)
      }
    )
    
    if (!is.null(sequence)) {
      return(sequence)
    }
  }
  
  NULL
}


val_gen <- function() {
  indices <- sample(length(val_k))
  
  for (i in indices) {
    sequence <- tryCatch(
      val_k[[i]](),
      error = function(e) {
        warning("Empty sequence error occurred in the generator.
                Trying the next one.")
        return(NULL)
      }
    )
    
    if (!is.null(sequence)) {
      return(sequence)
    }
  }
  
  NULL
}

test_gen <- function() {
  indices <- sample(length(val_k))
  
  for (i in indices) {
    sequence <- tryCatch(
      test_k[[i]](),
      error = function(e) {
        warning("Empty sequence error occurred in the generator.
                Trying the next one.")
        return(NULL)
      }
    )
    
    if (!is.null(sequence)) {
      return(sequence)
    }
  }
  
  NULL
}
```

# Carga y preprocesado de los datos 

## Lectura de los datos de medidas de muestras y características (features), eventos de QC y QC.

```{r training and testing data read, include=FALSE}

  # Leer los archivos Excel de Lotes y crear un data frame  feautures
  features <- unique(rbind(read_lot(), read_lotX()))
  features <- features[order(features[[1]]), ]
  length(which(is.na(features$TIEMPO_MUESTRA)))
  features <- features[!is.na(features$TIEMPO_MUESTRA),]
  
  head(features)
  summary(features)

  #Leer los archivos de eventos de QC y crear el data frame events:
  events <- unique(read_event())
  length(which(is.na(events$TIEMPO_EVENTO)))
  
  head(events)
  summary(events)
  
  #Leer los archivos de resultados de QC y crear el data frame qc_results, fusión de fecha y hora en una sola columna y separación de datos de media y desviación estándar en columnas individuales y como valores numéricos:
  qc_results <- read_qc()

  qc_results <- qc_results %>% merge_date()

  length(which(is.na(qc_results$TIEMPO_QC)))
  
  qc_results <- qc_results[!is.na(qc_results$TIEMPO_QC),]
  
  qc_results <- figure_ext(qc_results, 3)
  
 
  
  head(qc_results)
  summary(qc_results)


  
```

## Unión de datos de features, eventos de QC y QC.

```{r merge dataframe}
# Unir los tres dataframes features, qc_results y events en uno solo. Selección de pruebas opcional.
feat_qc <- merge_feat_qc(features, qc_results) 

featqc_event <- merge_featqc_event(feat_qc, events,)

```
## Creación de los conjuntos de targets por selección según criterios específicos.

```{r filter targets, message=FALSE}
# Creación de los targets mediante filtrado de casos que cumplan las condiciones necesarias para considerar una serie fuera de control, asignando valor 1 a la variable CTRL para cada caso en que se cumplan estos criterios.

# Criterio 1: Para cada unidad temporal de TIEMPO_QC, ANALIZADOR, CODIGO_PRUEBA y LOTE_REACTIVO:
# EVENTO==CAL AND
# Al menos 1 flag != NA

targets_1 <- featqc_event %>%
  filter(EVENTO == "CAL") %>%
  mutate(DIA = as.Date(TIEMPO_QC)) %>%
  group_by(DIA, ANALIZADOR, CODIGO_PRUEBA,
           LOTE_REACTIVO) %>%
  filter(any(!is.na(Flags))) %>%
  select(-DIA) %>%
  mutate(CTRL = 1)

# Criterio 2: Para cada unidad temporal de TIEMPO_QC, ANALIZADOR, CODIGO_PRUEBA y LOTE_REACTIVO:
# Al menos 1 flag !NA AND
# Observaciones !NA

targets_2 <- featqc_event %>%
  mutate(DIA = as.Date(TIEMPO_QC)) %>%
  group_by(DIA, ANALIZADOR, CODIGO_PRUEBA, LOTE_REACTIVO) %>%
  filter(any(!is.na(Flags)) &
           any(!is.na(Observaciones))) %>%
  select(-DIA) %>%
  mutate(CTRL = 1)

# Criterio 3: Para cada unidad temporal de TIEMPO_QC, ANALIZADOR, CODIGO_PRUEBA y LOTE_REACTIVO:
# >=2 flags 12s, 12.5s o 13s dentro del día

targets_3 <- featqc_event %>%
  mutate(DIA = as.Date(TIEMPO_QC)) %>%
  group_by(DIA, ANALIZADOR, CODIGO_PRUEBA, 
           LOTE_REACTIVO, Control) %>%
  filter(sum(str_detect(Flags, "\\b(12s|12.5s|13s)\\b")) >= 2) %>%
  ungroup() %>%
  mutate(CTRL = 1)


```

## Fusión de targets.

```{r targets dataframe, message=FALSE}

# Unir los 3 targets anteriores en uno solo y con el conjunto de datos featqc_event. Se añade el valor 0 a la variable CTRL en los casos no presentes en targets, que serán los que se consideran controlados.

targets <- rbind(targets_1, targets_2, targets_3)
  
data <- merge(featqc_event, 
              targets[, c("TIEMPO_QC", "ANALIZADOR",
                            "CODIGO_PRUEBA", "LOTE_REACTIVO",
                            "CTRL")],
              by = c("TIEMPO_QC", "ANALIZADOR",
                     "CODIGO_PRUEBA", "LOTE_REACTIVO"),
              all.x=T) %>%
  unique() %>%
  arrange(match(row.names(.), row.names(featqc_event))) %>%
  mutate(CTRL = replace(CTRL, is.na(CTRL), 0)) %>%
  select(colnames(featqc_event), CTRL)

```

## Creación de un diccionario de correspondencia entre códigos y nombres de pruebas. Eliminación de nombres de prueba con caracteres especiales

```{r inmunochemical tests names}

# Pruebas incluídas en los datos:
  iqtests <- unique(data.frame(data$NOMBRE_PRUEBA.x,
                               data$CODIGO_PRUEBA))
  
  # Corrección de caracteres incorrectos en NOMBRE_PRUEBA:
  iqtests$data.NOMBRE_PRUEBA.x <- 
    iconv(iqtests$data.NOMBRE_PRUEBA.x, to = "UTF-8")
  iqtests <- iqtests[complete.cases(iqtests),]

  # Sustitución en data de los nombres de prueba incorrectos:
  data$NOMBRE_PRUEBA.x <-
    iqtests$data.NOMBRE_PRUEBA.x[match(data$CODIGO_PRUEBA,
                                       iqtests$data.CODIGO_PRUEBA)]
```

## Selección de características de interés y transformación wavelet de los datos de resultados de medidas.

```{r features selection and wavelet transformation}
data_sel <- subset(data, 
                   select = c(TIEMPO_MUESTRA, PACIENTE_SEXO,
                              PACIENTE_EDAD, RESULTADO, ANALIZADOR,
                              CODIGO_PRUEBA, NOMBRE_PRUEBA.x, CTRL))

# Aplicar la transformación wavelet a la variable RESULTADOS para cada grupo de técnica:


# Aplicar la función wavelet_tr al dataframe data_sel
data_wavelet <- wavelet_tr(data_sel)


# Escalograma procalcitonina (CLC00638):
result_pct <- data_sel %>%
  filter(CODIGO_PRUEBA == "CLC00638" & 
           ANALIZADOR == "DXI800 num 1") %>%
  select(RESULTADO)

plot(result_pct$RESULTADO, xlab = "día", ylab = "PCT, ng/mL", main = "Resultados Procalcitonina a lo largo del tiempo de estudio")

plot.modwt(wt(result_pct$RESULTADO, "la20"))
```

## Transformación de la variable edad en variable categórica por agrupación (binning) en rangos de edades:

```{r binning age transformation}
# Aplicar binning a la variable edad:

# Definir los límites para el binning de la variable edad
age_limits <- c(0, 18, 30, 60, Inf)

# Aplicar el binning a la variable edad :
data_sel_tr <- data_wavelet %>%
  mutate(EDAD_BIN = as.character(cut(PACIENTE_EDAD,
                        age_limits, labels = c("Niño", "Joven",
                                           "Adulto", "Anciano"))),
         .after = "PACIENTE_EDAD") %>%
  select(-"PACIENTE_EDAD") %>%
  replace_na(list(EDAD_BIN = "Desconocido"))
data_sel_tr <- as.data.frame(data_sel_tr)
head(data_sel_tr)
```

## Guardado de datos en archivo csv.

```{r save csv}
write.csv(data_sel_tr, row.names = F,
          file = file.path(resultsdir, "data_sel_tr"))

```

## Carga de datos desde archivo csv.

```{r data_sel_tr read csv, eval=FALSE}

data_sel_tr <- read.csv(file.path(resultsdir, "data_sel_tr"),
                        sep = ",")

# Convertir fechas en formato POSIXct:
data_sel_tr[,1] %<>% as.POSIXct(tz = "Europe/Madrid")
```

## Descriptiva de variables y gráficos de datos.

```{r data description, eval=FALSE}

# Estructura de datos:
str(data_sel)

k1 <- kable(table(data$NOMBRE_PRUEBA.x, data$CTRL), format = "latex",
      escape = T, 
      col.names = c("TEST", "CONTROLADO/NO CONTROLADO"))

sink(file.path(resultsdir, "tabla_targets.txt"))
cat(k1)
sink()


# Plots resultados por prueba divididos por valor de la variable de proceso controlado (CTRL):

ggplot(data , mapping = aes(x=TIEMPO_MUESTRA,
                                    y=RESULTADO,
                                    color = as.factor(CTRL))) +
         geom_point(size=0.25) +
         facet_wrap(~NOMBRE_PRUEBA.x, scales = "free", ncol = 4)+    
         theme(strip.background = element_blank()) +
         theme(strip.text = element_text(size=8)) +
         scale_color_manual(values = c("blue", "red"),
                             name = "CTRL")

ggsave(file.path(figuresdir, "QC_state.jpeg"), 
       width = 30, height = 20, units = "cm", dpi = 300, 
       device = "jpeg")

# Plots resultados QC a lo largo del tiempo divididos por material de control
test_list <- unique(data.frame(featqc_event$NOMBRE_PRUEBA.x,
                    featqc_event$CODIGO_PRUEBA)) 
colnames(test_list) <- c("NOMBRE_PRUEBA", "CODIGO_PRUEBA")


for (i in seq_along(test_list$CODIGO_PRUEBA)) {
  ggplot_qc_DxI_1 <- subset(qc_results,
                        CODIGO_PRUEBA == test_list$CODIGO_PRUEBA[i])
  plot <- ggplot(ggplot_qc_DxI_1, mapping =
                   aes(x=TIEMPO_QC,
                       y=QC_RESULT,
                       color = Control)) +
          geom_point(size=0.5) +
          theme_bw() +
          labs(x="Date",y="Resultado QC") +
          scale_x_datetime(date_breaks = "1 month", date_labels =
                             "%b",
                   limits = as.POSIXct(c("2022-07-01 00:00:00",
                                        "2023-03-01 23:59:59"))) + 
          ggtitle(paste(test_list$NOMBRE_PRUEBA[i])) +
          theme(plot.title = element_text(size="10",
                                          face="bold",hjust = 0)) +
          theme(axis.title.x = element_text(size="10")) +
          theme(axis.title.y = element_text(size="10")) +
          theme(axis.text.y = element_blank()) +
          theme(axis.text.x = element_text(size=10)) +
          theme(axis.ticks.y = element_blank()) +
          theme(axis.ticks.x = element_blank()) +
          theme(panel.border = element_blank()) +
          theme(plot.margin = margin(t = 1, r = 1, b = 1, l = 1,
                                     unit = "cm")) +
          scale_color_manual(values = c("red", "blue", "green", 
                                        "black", "orange", "cyan"),
                             name = "Control")
  
  ggsave(plot,
         file=paste(test_list$NOMBRE_PRUEBA[i], "qc.png", sep='_'),
         path = file.path(figuresdir, "Controles"),
         width=15, height = 10, units=c("cm"))  
}

#Plot frecuencia de resultados obtenidos por prueba:

ggplot(data, aes(x=NOMBRE_PRUEBA.x)) +
  geom_bar(fill="blue") +
  labs(title="Frecuencia de medidas por NOMBRE_PRUEBA",
       x="NOMBRE_PRUEBA", y="frecuencia") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))

ggsave("frequency.png", width = 30, height = 20, units = "cm", dpi = 400,
       path = file.path(figuresdir), device = "png"
       )

# Plot distribución casos}

plot_ctrl_dist <- function(ds, clc){
  # Crea una lista vacía para almacenar los plots:
  plot_list <- list()
  
  count <- 0 # Contador de plots

  # Genera plots de frecuencia de casos distribuido para cada CODIGO_PRUEBA suministrado en la variable clc y separados según valor de variable CTRL:
for (i in seq(1, length(clc), by = 4)){
  
  g <- ds[ds$CODIGO_PRUEBA %in% clc[i:(i+3)], ] %>%
  ggplot(., aes(x = as.Date(TIEMPO_MUESTRA))) +
  geom_bar() +
  labs(x = "TIEMPO", y = "FRECUENCIA") +
  scale_x_date(date_labels = "%b %Y", date_breaks = "1 month") +
  coord_cartesian(ylim = c(0, 100)) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  facet_grid(CTRL~NOMBRE_PRUEBA.x)
  
  count <- count +1
  
  plot_list[[count]] <- g
}
# Guarda archivos png con los plots en el directorio de resultados:
for (i in seq_along(plot_list)) {
  filename <- paste0("Distribución_CTRL_",
                     deparse(substitute(ds)), "_", i, ".png")
  ggsave(filename, plot = plot_list[[i]], device = "png", 
         width = 30, height = 20, units = "cm", dpi = 300,
         path = file.path(figuresdir)
         )
}

}

plot_ctrl_dist(data_sel_tr, iqtests$data.CODIGO_PRUEBA)
```


## División de los datos en sets de entrenamiento, validación y test. 

```{r time division points}
# Se divide el dataset en 50% de datos para entrenamiento, 25% para test y 25% para validación. Para evitar fraccionar las unidades temporales diarias usamos percentiles de la secuencia temporal en días: 

time_point_50 <- as.POSIXct(as.Date(
  quantile(data_sel_tr[,1], 0.5)))

time_point_75 <- as.POSIXct(as.Date(
  quantile(data_sel_tr[,1], 0.75)))

time_point_50
time_point_75
```

```{r training dataset subsetting}
# Muestra de entrenamiento:
train <- data_sel_tr %>%
  subset(.[[1]] <= time_point_50)

table(train$NOMBRE_PRUEBA.x, train$CTRL)
```


```{r validation dataset subsetting}
# Muestra de validación:
val <- data_sel_tr %>%
  subset(
    .[[1]] > time_point_50 &
    .[[1]] <= time_point_75)

table(val$NOMBRE_PRUEBA.x, val$CTRL)
```


```{r test dataset subsetting}
# Muestra de test:
test <- data_sel_tr %>%
  subset(.[[1]] > time_point_75)
  
table(test$NOMBRE_PRUEBA.x, test$CTRL)
```


### Downsampling:

```{r downsampling majority class}

# Seleccionar entradas en los que no existan casos con la categoría minoritaria CTRL = 1

selected_cases_0 <- data_sel_tr %>%
  group_by(as.Date(TIEMPO_MUESTRA)) %>%
  filter(!any(CTRL == 1))

# Seleccionar el resto de entradas (sí existan casos con la categoría minoritaria CTRL = 1):
selected_cases_1 <- data_sel_tr %>%
  group_by(as.Date(TIEMPO_MUESTRA)) %>%
  filter(any(CTRL == 1))

# Downsampling seleccionando una cantidad equilibrada de muestras de la categoría mayoritaria de CTRL para cada grupo de CODIGO_PRUEBA:
downsampled_data <- selected_cases_0 %>%
  group_by(CODIGO_PRUEBA) %>%
  # Selección aleatoria del 1% de casos con CTRL = 0: 
  slice_sample(prop = 0.01, replace = FALSE) %>%
  ungroup()

data_sel_tr_ds <- bind_rows(downsampled_data, selected_cases_1)
data_sel_tr_ds %<>% arrange(TIEMPO_MUESTRA) %>%
  as.data.frame(.) %>% .[,-length(.)]

# Verificar el resultado
table(data_sel_tr_ds$NOMBRE_PRUEBA.x, data_sel_tr_ds$CTRL)
#plot_ctrl_dist(data_sel_tr_ds, iqtests$data.CODIGO_PRUEBA)

```

```{r dataset division with downsampled data}

time_point_50 <- as.POSIXct(as.Date(
  quantile(data_sel_tr_ds[,1], 0.5)))

time_point_75 <- as.POSIXct(as.Date(
  quantile(data_sel_tr_ds[,1], 0.75)))

# Muestra de entrenamiento:
train <- data_sel_tr_ds %>%
  subset(.[[1]] <= time_point_50)

val <- data_sel_tr_ds %>%
  subset(.[[1]] > time_point_50 &
         .[[1]] <= time_point_75)

test <- data_sel_tr_ds %>%
  subset(.[[1]] > time_point_75)

table(train_ds$NOMBRE_PRUEBA.x, train_ds$CTRL)
table(val_ds$NOMBRE_PRUEBA.x, val_ds$CTRL)
table(test_ds$NOMBRE_PRUEBA.x, test_ds$CTRL)
```


### Exclusión del modelo general de las técnicas que no alcanzan representación de las categorías CTRL = 0/1 en todos los datasets.

```{r CLC exclusion}

# Filtrado por código de pruebas que en alguno de los data set no tienen representación en la categoría CTRL = 1, y creación de una lista.
clc_sel <- list()

clc_sel[[1]] <- train_ds %>%
  group_by(CODIGO_PRUEBA) %>%
  filter(all(CTRL != 1))

clc_sel[[2]] <- val_ds %>%
  group_by(CODIGO_PRUEBA) %>%
  filter(all(CTRL != 1))

clc_sel[[3]] <- test_ds %>%
  group_by(CODIGO_PRUEBA) %>%
  filter(all(CTRL != 1))

# Unión de los elementos de la lista y extracción de los CODIGO_PRUEBA que las componen:
clc_sel %<>% do.call(rbind, .)
clc_other <- unique(clc_sel$CODIGO_PRUEBA)

# Subsetting de los dataframes de train, validation y test eliminando las entradas correspondientes a las pruebas encontradas en el paso anterior:
train_ds %<>% subset(., !(CODIGO_PRUEBA %in% clc_other))
val_ds %<>% subset(., !(CODIGO_PRUEBA %in% clc_other))
test_ds %<>% subset(., !(CODIGO_PRUEBA %in% clc_other))

# Distribución tras la eliminación de estas pruebas

table(train_ds$NOMBRE_PRUEBA.x, train_ds$CTRL)
table(val_ds$NOMBRE_PRUEBA.x, val_ds$CTRL)
table(test_ds$NOMBRE_PRUEBA.x, test_ds$CTRL)
```

### Downsampling de los datasets de entrenamiento, validacíon y prueba para las pruebas con baja representación de la categoría CTRL = 1.

```{r other CLC downsampling majority class}
# Exclusión de la prueba Ac. anti tiroglobulina (CLC00415) por no tener ningún caso con CTRL = 1:

clc_other <- clc_other[-which(clc_other=="CLC00415")]

# Subsetting del dataset original:
data_sel_tr_other <- data_sel_tr %>%
  subset(., (CODIGO_PRUEBA %in% clc_other))

# Seleccionar entradas en los que no existan casos con la categoría minoritaria CTRL = 1

selected_cases_0 <- data_sel_tr_other %>%
  group_by(as.Date(TIEMPO_MUESTRA)) %>%
  filter(!any(CTRL == 1))

# Seleccionar el resto de entradas (sí existen casos con la categoría minoritaria CTRL = 1):
selected_cases_1 <- data_sel_tr_other %>%
  group_by(as.Date(TIEMPO_MUESTRA)) %>%
  filter(any(CTRL == 1))

# Downsampling seleccionando una cantidad equilibrada de muestras de la categoría mayoritaria de CTRL para cada grupo de CODIGO_PRUEBA:
downsampled_data <- selected_cases_0 %>%
  group_by(CODIGO_PRUEBA) %>%
  # Selección aleatoria del 1% de casos con CTRL = 0: 
  slice_sample(prop = 0.01, replace = FALSE) %>%
  ungroup()

data_sel_tr_otherds <- bind_rows(
  downsampled_data[,-length(downsampled_data)],
  selected_cases_1[,-length(selected_cases_1)])

data_sel_tr_otherds %<>% arrange(TIEMPO_MUESTRA) %>%
  as.data.frame(.)

# Verificar el resultado
table(data_sel_tr_otherds$NOMBRE_PRUEBA.x, data_sel_tr_otherds$CTRL)
plot_ctrl_dist(data_sel_tr_other, clc_other)

```

### Creación de datasets individuales para las pruebas con distribución desigual de la variable CTRL a lo largo del tiempo.

Estas pruebas se van a tratar con modelos individualizados. Al contener las series temporales que se creen datos de una única prueba, se eliminará la separación en días para tratar todo el dataset como una serie temporal continua. El estado de la variable CTRL se escogerá a partir de la muestra k de la serie temporal, asumiendo que dada la baja frecuencia de series fuera de control y al uso de valores de lookback no excesivamente altos, no se mezclarán valores de una serie controlada con otros de una descontrolada.

```{r}
# Creación de una lista con todos los dataframes que contienen los valores de cada prueba:

data_set_list <- list()

data_set_list[1] <- data_sel_AFP <- data_sel_tr %>%
  subset(CODIGO_PRUEBA == "CLC00813")
data_set_list[2] <- data_sel_HCG <- data_sel_tr %>%
  subset(CODIGO_PRUEBA == "CLC11768")
data_set_list[3] <- data_sel_cortisol <- data_sel_tr %>%
  subset(CODIGO_PRUEBA == "CLC11977")
data_set_list[4] <- data_sel_PTH <- data_sel_tr %>%
  subset(CODIGO_PRUEBA == "CLC11015")
data_set_list[5] <- data_sel_CA199 <- data_sel_tr %>%
  subset(CODIGO_PRUEBA == "CLC00817")
data_set_list[6] <- data_sel_E2 <- data_sel_tr %>%
  subset(CODIGO_PRUEBA == "CLC11837")
data_set_list[7] <- data_sel_testo <- data_sel_tr %>%
  subset(CODIGO_PRUEBA == "CLC11854")


time_point_50 <- as.POSIXct(as.Date(
  quantile(data_sel_tr_other[,1], 0.5)))
time_point_50

time_point_75 <- as.POSIXct(as.Date(
  quantile(data_sel_tr_other[,1], 0.75)))
time_point_75

# Muestra de entrenamiento:
train_other <- data_sel_tr_other %>%
  subset(.[[1]] <= time_point_50)

val_other <- data_sel_tr_other %>%
  subset(.[[1]] > time_point_50 &
         .[[1]] <= time_point_75)

test_other <- data_sel_tr_other %>%
  subset(.[[1]] > time_point_75)

```

```{r CTRL distribution in other datasets}
# Distribución de la variable CTRL en los nuevos datasets

table(train_other$NOMBRE_PRUEBA.x, train_other$CTRL)
table(val_other$NOMBRE_PRUEBA.x, val_other$CTRL)
table(test_other$NOMBRE_PRUEBA.x, test_other$CTRL)
```

## Normalización de los datasets.

```{r datasets normalization}

train_n <- train %>% normalize_train()

val_n <- val %>% norm_test_val(means = train_n$means,
                                  stds = train_n$stdev)

test_n <- test %>% norm_test_val(means = train_n$means,
                                  stds = train_n$stdev)

```


## Codificación de variables mediante one-hot.

### Con wavelets.

```{r one-hot coding with wavelets}
train_code <- code_cat_var(subset(train_n$scaled_data,
                                  select = -NOMBRE_PRUEBA.x))

val_code <- code_cat_var(subset(val_n,
                                  select = -NOMBRE_PRUEBA.x))

test_code <- code_cat_var(subset(test_n,
                                  select = -NOMBRE_PRUEBA.x))

```

### Sin wavelets.

```{r}
train_code <- code_cat_var(subset(train_n$scaled_data,
                                  select = -c(3:20, NOMBRE_PRUEBA.x)))

val_code <- code_cat_var(subset(val_n,
                                  select = -c(3:20, NOMBRE_PRUEBA.x)))

test_code <- code_cat_var(subset(test_n,
                                  select = -c(3:20, NOMBRE_PRUEBA.x)))
```


## Preparación de series temporales de longitud k agrupadas por categorías y por fechas en los datasets de training y validación. 

```{r trainig set case grouping and k length time series creation}
# Creación de series temporales de tamaño k con los datos de training:

# Creación de los códigos identificativos únicos de combinaciones de variables categóricas para el set de training usando la función unique_lab_cases e indicando las posiciones de las variables categóricas en train_code$data que se deseen incluir (en este caso el código de prueba): 

categorical_vars <- c(12:38) # Todas las técnicas, sin wavelets
categorical_vars <- c(12:30) # Técnicas seleccionadas, sin wavelets
 


train_indx <- unique_lab_cases(train_code$data,
                               categorical_vars = categorical_vars)
head(train_indx)

# Extraemos los códigos para realizar un diccionario de traducción de códigos únicos a índices numéricos:
code_values <- unique(train_indx[,"code"])
code_dictionary <- data.frame(code = code_values)
code_dictionary %<>%  
  mutate(., index = as.numeric(factor(code_values, 
                                      levels = code_values)))

# Sustituimos los códigos de train_indx por sus índices numéricos:
train_indx$index <- code_dictionary$index[match(train_indx$code, code_dictionary$code)]
#index_values <- unique(train_indx[,"index"])
train_indx$code <- NULL
train_indx %<>% arrange(., train_indx$TIEMPO_MUESTRA)

# Creamos todas las posibles combinaciones de códigos y fechas:

train_indx$TIEMPO_MUESTRA %<>% as.Date()

combinations_train <- expand.grid(code = code_values,
                            date = unique(train_indx$TIEMPO_MUESTRA))

# Traducimos los códigos a índices:

combinations_train$index <-
  code_dictionary$index[match(combinations_train$code,
                              code_dictionary$code)]

# Creación de lista de funciones generadoras de las series temporales de longitud k por categoría de CODIGO_PRUEBA del dataset de entrenamiento. Se generan series para cada día de observación conteniendo los resultados pertenecientes a cada una de las categorías elegidas:

# Hiperparámetros para generar las series temporales:
lookback <- 20 # Longitud de la secuencia previa al target.
delay <- 0 # Se escoge 0 debido a que los targets futuros ya se                 colocaron en la misma fila que la muestra del día                  anterior.
batch_size <- 400 # Tamaño de batch.
min_index <- 1 # Indice de partida de la serie temporal.
predseries <- length(train_indx) - 2 # Posición en el dataset de la variable a predecir (CTRL) una vez eliminados TIEMPO_MUESTRA e index.

train_k <- mapply(function(idx, date) {
  data <- as.matrix(
  train_indx[train_indx[,"TIEMPO_MUESTRA"] == date &
               train_indx[,"index"] == idx,
             - c(1, length(train_indx))])
  max_index <- nrow(data)-delay
  
  create_lstm_data.1(
    data = data,
    lookback = lookback,
    delay = delay,
    batch_size = batch_size,
    min_index = min_index,
    max_index = max_index,
    predseries = predseries)
},
idx = combinations_train$index,
date = combinations_train$date,
SIMPLIFY = T
) 


# Cálculo del número de steps de training:
train_steps <- round((nrow(train_code$data) - lookback) / batch_size)

```

```{r Validation set case grouping and k length time series creation}
# Creación de los índices identificativos de combinaciones de variables categóricas para el set de validación:
val_indx <- unique_lab_cases(val_code$data, 
                             categorical_vars)

# Usamos el diccionario de códigos creado anteriormente: 
val_indx$index <- code_dictionary$index[match(val_indx$code,
                                            code_dictionary$code)]
val_indx$code <- NULL
val_indx %<>% arrange(., val_indx$TIEMPO_MUESTRA)

# Generar todas las combinaciones de index y date para el set de validación:

val_indx$TIEMPO_MUESTRA %<>% as.Date()

combinations_val <- expand.grid(code = code_values,
                            date = unique(val_indx$TIEMPO_MUESTRA))

# Traducimos los códigos a índices:

combinations_val$index <-
  code_dictionary$index[match(combinations_val$code,
                              code_dictionary$code)]
head(val_indx)
head(combinations_val)

# Creación de lista de funciones generadoras del dataset de validación:

val_k <- mapply(function(index, date) {
  data <- as.matrix(
  val_indx[val_indx[,"TIEMPO_MUESTRA"] == date &
               val_indx[,"index"] == index,
             - c(1, length(train_indx))])
  max_index <- nrow(data)-delay
  
  create_lstm_data.1(
    data = data,
    lookback = lookback,
    delay = delay,
    batch_size = batch_size,
    min_index = min_index,
    max_index = max_index,
    predseries = predseries)
  },
  index = combinations_val$index,
  date = combinations_val$date,
  SIMPLIFY = T
)  

# Cálculo del número de steps de validación:
val_steps <- round((nrow(val_code$data)-lookback) / batch_size)
```

```{r Test set case grouping and k length time series creation}
# Mismo procedimiento que para el set de validación:

test_indx <- unique_lab_cases(test_code$data, 
                             categorical_vars)

test_indx$index <- code_dictionary$index[match(test_indx$code,
                                            code_dictionary$code)]
test_indx$code <- NULL

test_indx %<>% arrange(., test_indx$TIEMPO_MUESTRA)

test_indx$TIEMPO_MUESTRA %<>% as.Date()

combinations_test <- expand.grid(code = code_values,
                            date = unique(test_indx$TIEMPO_MUESTRA))

# Traducimos los códigos a índices:

combinations_test$index <-
  code_dictionary$index[match(combinations_test$code,
                              code_dictionary$code)]
head(val_indx)
head(combinations_val)

# Creación de lista de funciones generadoras del dataset de validación:

test_k <- mapply(function(index, date) {
  data <- as.matrix(
  test_indx[test_indx[,"TIEMPO_MUESTRA"] == date &
               test_indx[,"index"] == index,
             - c(1, length(test_indx))])
  max_index <- nrow(data)-delay
  
  create_lstm_data.1(
    data = data,
    lookback = lookback,
    delay = delay,
    batch_size = batch_size,
    min_index = min_index,
    max_index = max_index,
    predseries = predseries)
  },
  index = combinations_test$index,
  date = combinations_test$date,
  SIMPLIFY = T
)  

# Cálculo del número de steps de validación:
test_steps <- round((nrow(test_code$data)-lookback) / batch_size)
```


# Definición del modelo de Machine Learning. Red Neuronal Recurrente LSTM (Long-short term memory).

```{r model definition}
set.seed(123)

nkp <- dim(train_gen()[[1]])

k <- nkp[2]
p <- nkp[3]

input_shape <- c(NULL, k, p)

 
```

```{r custom metrics functions}
# Funciones para calcular precisión, recall y F1 como métricas para los modelos:
  
  K <- backend()

  precision <- function(y_true, y_pred) {
    # Verdaderos positivos: 
    true_positives <- sum(K$round(K$clip(y_true * y_pred, 0, 1)))
    
    # Posibles positivos: 
    possible_positives <- sum(K$round(K$clip(y_true, 0, 1)))
    
    # Positivos predichos:
    predicted_positives <- sum(K$round(K$clip(y_pred, 0, 1)))
    
    # Precisión: 
    precision <- true_positives / (predicted_positives + K$epsilon())
    
  return(precision)
  }
  
  recall <- function(y_true, y_pred) {
    # Verdaderos positivos: 
    true_positives <- sum(K$round(K$clip(y_true * y_pred, 0, 1)))
    
    # Posibles positivos: 
    possible_positives <- sum(K$round(K$clip(y_true, 0, 1)))
    
    # Recall:
    recall <- true_positives / (possible_positives + K$epsilon())
    
  return(recall)
}

  f1_score <- function(y_true, y_pred) {
    # Verdaderos positivos: 
    true_positives <- sum(K$round(K$clip(y_true * y_pred, 0, 1)))
    
    # Posibles positivos: 
    possible_positives <- sum(K$round(K$clip(y_true, 0, 1)))
    
    # Positivos predichos:
    predicted_positives <- sum(K$round(K$clip(y_pred, 0, 1)))
    
    # Precisión: 
    precision <- true_positives / (predicted_positives + K$epsilon())
    
    # Recall:
    recall <- true_positives / (possible_positives + K$epsilon())
    
    # F1 score:
    f1_score <- 2 * precision * recall / (precision + recall +
                                            K$epsilon())
  return(f1_score)
}

```

```{r model compilation and callbacks}
model_1 %<>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = list("binary_accuracy",
                 f1_score)
  )
   
callbacks <- list(
  callback_model_checkpoint(file.path(resultsdir, "lstm_1.keras",
                                     save_best_only = TRUE)),
  callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.5,
                                patience = 20, min_lr = 0.00001),
  callback_early_stopping(monitor = "val_loss", patience = 50,
                          verbose = 1),
  callback_tensorboard(log_dir = resultsdir,  histogram_freq = 1,
                      write_graph = TRUE)
  )
```


```{r model fit, eval=FALSE}
history_1 <- model_1 %>% fit_generator(
  generator = train_gen,
  steps_per_epoch = train_steps,
  epochs = 10,
  validation_data = val_gen,
  validation_steps = val_steps,
  callbacks = callbacks
)


plot(history_1)
```

## Evaluación

```{r, eval=FALSE}
set.seed(123)

# Evaluación del modelo:
path <- file.path(resultsdir, "lstm_1.keras/TRUE")

model <- load_model_hdf5(path, 
                         custom_objects = list('f1_score' =
                                                 f1_score),
                         compile = T) 
evaluation_result <- evaluate(model, test_gen, steps = test_steps)

# Obtener el nombre de las métricas
metric_names <- names(evaluation_result)

# Crear un vector para almacenar los resultados
evaluation_results <- length(metric_names)

# Almacenar los resultados en el vector
for (i in 1:length(metric_names)) {
  evaluation_results[i] <- evaluation_result[[i]]
}

```

### Línea base de sentido común

```{r common sense baseline}

# Función para calcular la exactitud mínima del modelo, basada en la predicción de todos los casos con el target más frecuente

evaluate_naive_method <- function(data_df, ctrl_colname) { 
  # Cálculo de la moda de CTRL:
  ctrl_mode <- as.numeric(names(sort(table(
    data_df[[ctrl_colname]]),
    decreasing = TRUE)[1]))

  # Cálculo de accuracy:
  actual_ctrl <- data_df[[ctrl_colname]]
  predicted_ctrl <- rep(ctrl_mode, length(actual_ctrl))
  accuracy <- sum(actual_ctrl == predicted_ctrl) /
    length(actual_ctrl)

  print(accuracy)
}

# Cálculo de accuracy basado en predecir con la moda:
accuracy_train <- evaluate_naive_method(train_n$scaled_data, "CTRL")
accuracy_val <- evaluate_naive_method(val_n, "CTRL")

sprintf("Training naive accuracy: %.2f", accuracy_train)
sprintf("Validation naive accuracy: %.2f", accuracy_val)

# Comparación con rendimiento obtenido:

binary_accuracy <- evaluation_results[2]
f1 <- evaluation_results[3]

# Imprimir los resultados de evaluación:

sink(file.path(resultsdir, "model_1_LSTM_onehot_samples.txt"))

naive <- evaluate_naive_method(train_n$scaled_data, "CTRL")

for (i in 1:length(metric_names)) {
  metric <- metric_names[i]
  value <- evaluation_results[i]
  cat(sprintf("%s: %f\n", metric, value))
}

eval_model <- cbind(train_n$stdev[[2]], train_n$stdev[[1]],
                    train_n$stdev[[3]], 
                    naive*train_n$stdev[[3]],
                    mae*train_n$stdev[[3]])

colnames(eval_model) <- c("CODIGO_PRUEBA", "ANALIZADOR",
                          "SD","NAIVE_MAE*SD","MAE*SD")
eval_model

# Cerrar el archivo de salida
sink()

```

### Evaluación por categorías.

```{r model evaluation per category}
# Función para seleccionar los índices en test_indx que corresponden a una combinación determinada de CODIGO_PRUEBA, NIVEL Y ANALIZADOR: 

index_select <- function() {
  
  # Opciones para elegir CLC, NIVEL y ANALIZADOR:
  clc_opt <- test_code$levels$CODIGO_PRUEBA
  level_opt <- test_code$levels$NIVEL
  anal_opt <- test_code$levels$ANALIZADOR
  
  # Introducción de la fecha por el usuario:
  #date <- readline(
   # "fecha del día anterior a la predicción (aaaa-mm-dd): ")
  #if(!date %in% as.Date(test_code$data$TIEMPO_QC))
   #  print("fecha no incluida en dataset")
  
  # Introducción de prueba, nivel y analizador:
  clc <- clc_opt[menu(clc_opt, 
                      title = "Código CLC de prueba: ",
                      graphics = T)]
  level <- level_opt[menu(level_opt, 
                          title = "Nivel de QC: ",
                          graphics = T)]
  anal <- anal_opt[menu(anal_opt, 
                        title = "Analizador: ",
                        graphics = T)]
    
  cat("Indice correspondiente a la prueba: ", clc, "\n", 
  "nivel: ", level, "\n",
  "analizador: ", anal, "\n")
  
  # Conversión de las variables introducidas en índice:
  
  col_clc <- which(grepl(clc, names(test_indx)))-16
  col_level <- which(grepl(paste0("NIVEL_", level), names(test_indx)))-16
  col_anal <- which(grepl(anal, names(test_indx)))-16
  pos <- list(col_clc, col_level, col_anal)
  
  code <- rep(0, length(categorical_vars+2))
  
  for (i in seq_along(pos)) {
    code[pos[[i]]] <- 1 
  }
  code <- paste0(as.character(code), collapse = "")
  
  index <- code_dictionary$index[match(code,
                                       code_dictionary$code)]
  print(index)
  return(index)
  
}

index_pred <- index_select()

  test_pred <- lapply(index_pred, function(idx) {
    create_lstm_data.2(
      data = as.matrix(test_indx[test_indx[,"index"] == idx,
                                 -c(1, 3:16, length(test_indx))]),
      lookback = lookback,
      delay = delay,
      batch_size = batch_size,
      min_index = min_index,
      max_index = 
      nrow(as.matrix(test_indx[test_indx[,"index"] == idx,]))-delay-1,
      predseries = predseries)
  }
  )

```


```{r model prediction and metrics}

# Obtener las series y sus predicciones utilizando la función generadora de tests por categorías y obtener la predicción del modelo:
x <- test_pred[[1]]()[[1]]
y_pred <-  predict(model, x = x)
y_true <- test_pred[[1]]()[[2]]

metrics <- list(
  MSE <- mse(actual = y_true, predicted = y_pred),
  MAE <- mae(actual = y_true, predicted = y_pred),
  Correlación <- cor(y_pred, y_true)
)
names(metrics) <- c("MSE", "MAE", "Coef. correlación")
metrics

# Crear el gráfico de dispersión entre y_pred e y_true
plot(y_true, y_pred, pch = 16, col = "blue", xlab = "y_true", ylab = "y_pred", 
     main = "Gráfico de dispersión: y_true vs. y_pred")

# Calcular el coeficiente de correlación
correlacion <- cor(y_true, y_pred)

# Agregar el coeficiente de correlación al gráfico
texto_cor <- paste("Correlación:", round(correlacion, 2))
mtext(texto_cor, side = 3, line = -2.5)


```

```{r all metrics by categories}
# Calcular las métricas de todas las categorías:

global_metrics <- list()

for(i in seq_along(test_k)){
  
  # Calcula x, y_true e y_pred a partir de la primera iteración de las funciones generadoras y el modelo:
  x <- test_k[[i]]()[[1]]
  y_true <- test_k[[i]]()[[2]]
  y_pred <-  predict(model, x = x)

  metrics <- list(
  MSE <- mse(actual = y_true, predicted = y_pred),
  MAE <- mae(actual = y_true, predicted = y_pred),
  Correlación <- cor(y_pred, y_true)
  )

  names(metrics) <- c("MSE", "MAE", "Coef. correlación")
  global_metrics[[i]] <- metrics

}

names(global_metrics) <- index_values

# Crear un gráfico para cada elemento de las sublistas en global_metrics
for (i in 1:3) {
  plot(1:length(global_metrics), 
       sapply(global_metrics, "[[", i),
       pch = 16, 
       xlab = "Index",
       ylab = names(global_metrics[[1]][i])
  )
}


```

```{r index search}
# Función para hacer la búsqueda inversa de analizador, prueba y nivel a partir de un index:

reverse_index_select <- function(index) {
  # Buscar el código correspondiente al índice
  code <- code_dictionary$code[match(index, code_dictionary$index)]
  
  # Extraer los dígitos de code
  digits <- strsplit(code, "")[[1]]
  
  # Encontrar las posiciones de los 1 en la secuencia de dígitos
  pos_anal <- which(digits == "1")[1]
  pos_clc <- which(digits == "1")[2]
  pos_level <- which(digits == "1")[3]
  
  # Obtener los valores de anal, clc y level
  anal <- names(test_indx)[16 + pos_anal]
  clc <- names(test_indx)[16 + pos_clc]
  level <- names(test_indx)[16 + pos_level]
  
  # Devolver los valores encontrados
  result <- list(CÓDIGO_PRUEBA = substr(clc, 15, 22),
                 NIVEL = level, 
                 ANALIZADOR = anal)
  return(result)
}




```

```{r problematic cases search}

# Búsqueda de casos con métricas por debajo de la media:

limit_mse <- mse
limit_mae <- mae
limit_coef <- 0.7

cases <- c()  # Lista para almacenar los índices

for (i in seq_along(global_metrics)) {
  
  sublist <- global_metrics[[i]]  
  
  # Extracción de métricas:
  mse_i <- sublist$MSE
  mae_i <- sublist$MAE
  coef_i <- sublist$`Coef. correlación` 
  
  if (mse_i >= limit_mse & mae_i >= limit_mae &
      coef_i <= limit_coef) {
    cases <- c(cases, i)  # Agregar el índice a la lista
  }
}

# Imprimir los índices encontrados

print(cases)

# Transformar índices a información de casos:

selection <-list()

for(i in seq_along(cases))
selection[[i]] <- reverse_index_select(cases[i])


```


```{r loop, eval=FALSE}
# Iniciar el loop que se ejecutará periódicamente para leer los archivos csv en la fase de puesta en producción
while (TRUE) {
  tic() # iniciar contador de tiempo
  
  # Leer los archivos Excel de Lotes y crear un data frame    feautures
  features <- read_lot()
    
  
```

```{r end loop, eval=FALSE}

  
  # Esperar el intervalo de tiempo definido antes de la siguiente lectura
  Sys.sleep(intervalo - toc())
}

```

