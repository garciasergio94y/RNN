
> library(tfruns)

> library(keras)

> # Hyperparameter flags ---------------------------------------------------
> 
> FLAGS <- flags(
+   flag_numeric("lookback", 20),
+   flag_numeric(" ..." ... [TRUNCATED] 

> # Generator functions------------------------------------------------------
> train_data <- as.matrix(train_code[,-1])

> lookback <- FLAGS$lookback

> batch_size <- FLAGS$batch_size

> delay <- 0

> min_index <- 0

> predseries <- ncol(train_data)

> train_k <-  create_lstm_data.1(
+   data = train_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_ .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> train_steps <- round((nrow(train_data)-lookback) / batch_size) 

> wrap_train <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- train_k(),
+       error = function(e) {
+ .... [TRUNCATED] 

> #########################################################################
> 
> val_data <- as.matrix(val_code[,-1])

> val_k <- create_lstm_data.1(
+   data = val_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_index .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> val_steps <- round((nrow(val_data)-lookback) / batch_size) 

> wrap_val <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- val_k(),
+       error = function(e) {
+     .... [TRUNCATED] 

> ##########################################################################
> 
> test_data <- as.matrix(test_code[,-1])

> test_k <-  create_lstm_data.1(
+   data = test_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_in .... [TRUNCATED] 

> wrap_test <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- test_k(),
+       error = function(e) {
+   .... [TRUNCATED] 

> # Cálculo del número de steps de test:
> test_steps <- round((nrow(test_data)-lookback) / batch_size) 

> #######################################################################
> 
> 
> # Model parameters-------------------------------------------------- .... [TRUNCATED] 

> static_features = 4  # Variables estáticas (ANALIZADOR, CODIGO_PRUEBA,

>                       #EDAD_BIN Y PACIENTE_SEXO )
> lstm_units = FLAGS$lstm_units # Número de unidades en la capa LSTM 

> conv_filters = FLAGS$conv_filters # Número de filtros capa convolucional

> conv_kernels = FLAGS$conv_kernels # Número de kernels

> vocabulary_size = length(unique(train_code$PACIENTE_SEXO)) +
+   length(unique(train_code$EDAD_BIN))+
+   length(unique(train_code$ANALIZADOR))+
+   .... [TRUNCATED] 

> # Número de niveles codificados de las variables estáticas.
> 
> epochs <- 15

> optimizer = optimizer_adam(learning_rate = 0.001)

> loss = loss_sigmoid_focal_crossentropy(alpha = 0.8, 
+                                        gamma = 4, 
+                                        r .... [TRUNCATED] 

> # Define Model ------------------------------------------------------------
> 
> # Modelo LSTM:
> 
> # Capa de entrada de datos dinámicos:
> dynamic .... [TRUNCATED] 

> #dynamic_input_layer <- layer_masking(mask_value =
> #-99999)(dynamic_input_layer)
> # Capa de entrada de datos estáticos: 
> static_input_layer <-  .... [TRUNCATED] 
