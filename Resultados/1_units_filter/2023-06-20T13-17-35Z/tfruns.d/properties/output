
> library(tfruns)

> library(keras)

> # Hyperparameter flags ---------------------------------------------------
> 
> FLAGS <- flags(
+   flag_numeric("lookback", 20),
+   flag_numeric(" ..." ... [TRUNCATED] 

> # Generator functions------------------------------------------------------
> train_data <- as.matrix(train_code[,-1])

> lookback <- FLAGS$lookback

> batch_size <- FLAGS$batch_size

> delay <- 0

> min_index <- 0

> predseries <- ncol(train_data)

> train_k <-  create_lstm_data.1(
+   data = train_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_ .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> train_steps <- round((nrow(train_data)-lookback) / batch_size) 

> wrap_train <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- train_k(),
+       error = function(e) {
+ .... [TRUNCATED] 

> #########################################################################
> 
> val_data <- as.matrix(val_code[,-1])

> val_k <- create_lstm_data.1(
+   data = val_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_index .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> val_steps <- round((nrow(val_data)-lookback) / batch_size) 

> wrap_val <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- val_k(),
+       error = function(e) {
+     .... [TRUNCATED] 

> ##########################################################################
> 
> test_data <- as.matrix(test_code[,-1])

> test_k <-  create_lstm_data.1(
+   data = test_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_in .... [TRUNCATED] 

> wrap_test <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- test_k(),
+       error = function(e) {
+   .... [TRUNCATED] 

> # Cálculo del número de steps de test:
> test_steps <- round((nrow(test_data)-lookback) / batch_size) 

> #######################################################################
> 
> 
> # Model parameters-------------------------------------------------- .... [TRUNCATED] 

> static_features = 4  # Variables estáticas (ANALIZADOR, CODIGO_PRUEBA,

>                       #EDAD_BIN Y PACIENTE_SEXO )
> lstm_units = FLAGS$lstm_units # Número de unidades en la capa LSTM 

> conv_filters = FLAGS$conv_filters # Número de filtros capa convolucional

> conv_kernels = FLAGS$conv_kernels # Número de kernels

> vocabulary_size = length(unique(train_code$PACIENTE_SEXO)) +
+   length(unique(train_code$EDAD_BIN))+
+   length(unique(train_code$ANALIZADOR))+
+   .... [TRUNCATED] 

> # Número de niveles codificados de las variables estáticas.
> 
> epochs <- 15

> optimizer = optimizer_adam(learning_rate = 0.001)

> loss = loss_sigmoid_focal_crossentropy(alpha = 0.8, 
+                                        gamma = 4, 
+                                        r .... [TRUNCATED] 

> # Define Model ------------------------------------------------------------
> 
> # Modelo LSTM:
> 
> # Capa de entrada de datos dinámicos:
> dynamic .... [TRUNCATED] 

> #dynamic_input_layer <- layer_masking(mask_value =
> #-99999)(dynamic_input_layer)
> # Capa de entrada de datos estáticos: 
> static_input_layer <-  .... [TRUNCATED] 

> # Capa a de embedding para los datos estáticos:
> embedding_layer <- layer_embedding(
+   input_dim = vocabulary_size,
+   output_dim = lstm_units)( .... [TRUNCATED] 

> # Capa lambda para seleccionar los embeddings que modificarán los estados ocultos de la capa LSTM:
> embedding_layer <- layer_lambda(f = \(x) x[,1,] .... [TRUNCATED] 

> # Capa convolucional:
> conv_layer <- layer_conv_1d(filters = conv_filters,
+                             kernel_size = conv_kernels,
+              .... [TRUNCATED] 

> # Capa LSTM que recibe los valores de estado oculto de la capa
> # convolución con las características que ésta haya extraído:
> lstm_layer_1 <- lay .... [TRUNCATED] 

> lstm_layer_2 <- layer_lstm(units = lstm_units,
+                             name = "lstm_2", 
+                             dropout = 0.3,
+        .... [TRUNCATED] 

> dense1 <- layer_dense(units = dense_u,
+                       activation='relu')(lstm_layer_2)

> # Capa de salida con una unidad de activación lineal para la predicción:  
> output_layer <- layer_dense(units = 1,
+                             ac .... [TRUNCATED] 

> # Create model-------------------------------------------------------------
> model <- 
+   keras_model(
+     inputs  = list(dynamic_input_layer, s .... [TRUNCATED] 

> # Compile-----------------------------------------------------------------
> model %<>% compile(
+   optimizer = optimizer,
+   loss      = loss,
+  .... [TRUNCATED] 

> # Save naïve model--------------------------------------------------------
> 
> file_name <- paste0("lstm_QC_", 0, ".keras") 

> save_model_weights_hdf5(model, 
+                         file.path(resultsdir, "naive_model.h5"),
+                         overwrite = T) 

> # Callbacks---------------------------------------------------------------
> 
> # Callbacks
> 
> callbacks <- list(
+   callback_model_checkpoint(fi .... [TRUNCATED] 

> # Define metric rmse ------------------------------------------------------
> 
> K <- keras::backend()

> rmse <- function(y_pred, y_true) {
+   sq_diff <- K$square(y_pred - y_true)
+   mean_sq_diff <- K$mean(sq_diff)
+   rmse_value <- K$sqrt(mean_sq_dif .... [TRUNCATED] 

> attr(rmse, "py_function_name") <- "rmse"

> # Train model--------------------------------------------------------------
> 
> history <- model %>%
+   fit(wrap_train,
+       epochs = epochs,
+ .... [TRUNCATED] 
