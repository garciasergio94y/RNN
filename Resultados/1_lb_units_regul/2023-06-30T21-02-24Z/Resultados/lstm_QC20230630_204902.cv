
Cross Validation Results:

> library(tfruns)

> library(keras)

> # Hyperparameter flags ---------------------------------------------------
> 
> FLAGS <- flags(
+   flag_numeric("lookback", 10),
+   flag_numeric(" ..." ... [TRUNCATED] 

> # Generator functions------------------------------------------------------
> # Con wavelets:
> # train_data <- as.matrix(train_code[,-c(1,19)])
> # .... [TRUNCATED] 

> lookback <- FLAGS$lookback

> batch_size <- FLAGS$batch_size

> delay <- 1

> min_index <- 1

> predseries <- 1

> train_k <-  create_lstm_data.2(
+   data = train_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_ .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> train_steps <- round((nrow(train_data)-lookback) / batch_size) 

> wrap_train <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- train_k(),
+       error = function(e) {
+ .... [TRUNCATED] 

> #########################################################################
> # Con wavelets:
> # val_data <- as.matrix(val_code[,-c(1, 19)])
> # Sin  .... [TRUNCATED] 

> val_k <- create_lstm_data.2(
+   data = val_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_index .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> val_steps <- round((nrow(val_data)-lookback) / batch_size) 

> wrap_val <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- val_k(),
+       error = function(e) {
+     .... [TRUNCATED] 

> ##########################################################################
> # Con wavelets:
> # test_data <- as.matrix(test_code[,-c(1, 19)])
> # S .... [TRUNCATED] 

> test_k <-  create_lstm_data.2(
+   data = test_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_in .... [TRUNCATED] 

> wrap_test <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- test_k(),
+       error = function(e) {
+   .... [TRUNCATED] 

> # Cálculo del número de steps de test:
> test_steps <- round((nrow(test_data)-lookback) / batch_size) 

> #######################################################################
> 
> 
> # Model parameters-------------------------------------------------- .... [TRUNCATED] 

> # dynamic_features = 15 # Variables dinámicas (QC_RESULT + wavelets)
> static_features = 3  # Variables estáticas (ANALIZADOR, CODIGO_PRUEBA Y NIVEL .... [TRUNCATED] 

> lstm_units = FLAGS$lstm_units # Número de unidades en la capa LSTM 

> vocabulary_size = 7 # Número de niveles codificados de las variables estáticas.

> epochs <- 20

> optimizer <- "adam"

> loss <- "mae"

> # Define Model ------------------------------------------------------------
> 
> # Modelo LSTM:
> 
> # Capa de entrada de datos dinámicos:
> dynamic .... [TRUNCATED] 

> #dynamic_input_layer <- layer_masking(mask_value =
> #-99999)(dynamic_input_layer)
> # Capa de entrada de datos estáticos: 
> static_input_layer <-  .... [TRUNCATED] 

> # Aplanado de la capa de datos estáticos antes del embedding:
> flatten_static_layer <- layer_flatten()(static_input_layer)

> # Capa a de embedding para los datos estáticos:
> embedding_layer <- layer_embedding(
+   input_dim = vocabulary_size,
+   output_dim = lstm_units)( .... [TRUNCATED] 

> # Capa lambda para seleccionar los embeddings que modificarán los estados ocultos de la capa LSTM:
> embedding_layer <- layer_lambda(f = \(x) x[,1,] .... [TRUNCATED] 

> # Capa LSTM que recibe dos valores de estado oculto de la capa de embeddings con la información de las variables estáticas:
> lstm_layer_b1 <- layer .... [TRUNCATED] 

> #lstm_layer_b2 <- layer_lstm(units = lstm_units,
>  #                           name = "lstm_2", 
>   #                          dropout = FLAGS$dro .... [TRUNCATED] 

> library(tfruns)

> library(keras)

> # Hyperparameter flags ---------------------------------------------------
> 
> FLAGS <- flags(
+   flag_numeric("lookback", 10),
+   flag_numeric(" ..." ... [TRUNCATED] 

> # Generator functions------------------------------------------------------
> # Con wavelets:
> # train_data <- as.matrix(train_code[,-c(1,19)])
> # .... [TRUNCATED] 

> lookback <- FLAGS$lookback

> batch_size <- FLAGS$batch_size

> delay <- 1

> min_index <- 1

> predseries <- 1

> train_k <-  create_lstm_data.2(
+   data = train_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_ .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> train_steps <- round((nrow(train_data)-lookback) / batch_size) 

> wrap_train <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- train_k(),
+       error = function(e) {
+ .... [TRUNCATED] 

> #########################################################################
> # Con wavelets:
> # val_data <- as.matrix(val_code[,-c(1, 19)])
> # Sin  .... [TRUNCATED] 

> val_k <- create_lstm_data.2(
+   data = val_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_index .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> val_steps <- round((nrow(val_data)-lookback) / batch_size) 

> wrap_val <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- val_k(),
+       error = function(e) {
+     .... [TRUNCATED] 

> ##########################################################################
> # Con wavelets:
> # test_data <- as.matrix(test_code[,-c(1, 19)])
> # S .... [TRUNCATED] 

> test_k <-  create_lstm_data.2(
+   data = test_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_in .... [TRUNCATED] 

> wrap_test <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- test_k(),
+       error = function(e) {
+   .... [TRUNCATED] 

> # Cálculo del número de steps de test:
> test_steps <- round((nrow(test_data)-lookback) / batch_size) 

> #######################################################################
> 
> 
> # Model parameters-------------------------------------------------- .... [TRUNCATED] 

> # dynamic_features = 15 # Variables dinámicas (QC_RESULT + wavelets)
> static_features = 3  # Variables estáticas (ANALIZADOR, CODIGO_PRUEBA Y NIVEL .... [TRUNCATED] 

> lstm_units = FLAGS$lstm_units # Número de unidades en la capa LSTM 

> vocabulary_size = 7 # Número de niveles codificados de las variables estáticas.

> epochs <- 20

> optimizer <- "adam"

> loss <- "mae"

> # Define Model ------------------------------------------------------------
> 
> # Modelo LSTM:
> 
> # Capa de entrada de datos dinámicos:
> dynamic .... [TRUNCATED] 

> #dynamic_input_layer <- layer_masking(mask_value =
> #-99999)(dynamic_input_layer)
> # Capa de entrada de datos estáticos: 
> static_input_layer <-  .... [TRUNCATED] 

> # Aplanado de la capa de datos estáticos antes del embedding:
> flatten_static_layer <- layer_flatten()(static_input_layer)

> # Capa a de embedding para los datos estáticos:
> embedding_layer <- layer_embedding(
+   input_dim = vocabulary_size,
+   output_dim = lstm_units)( .... [TRUNCATED] 

> # Capa lambda para seleccionar los embeddings que modificarán los estados ocultos de la capa LSTM:
> embedding_layer <- layer_lambda(f = \(x) x[,1,] .... [TRUNCATED] 

> # Capa LSTM que recibe dos valores de estado oculto de la capa de embeddings con la información de las variables estáticas:
> lstm_layer_b1 <- layer .... [TRUNCATED] 

> #lstm_layer_b2 <- layer_lstm(units = lstm_units,
>  #                           name = "lstm_2", 
>   #                          dropout = FLAGS$dro .... [TRUNCATED] 

> # Capa de salida con una unidad de activación lineal para la predicción:  
> output_layer <- layer_dense(units = 1,
+                             ac .... [TRUNCATED] 

> # Create model-------------------------------------------------------------
> model <- 
+   keras_model(
+     inputs  = list(dynamic_input_layer, s .... [TRUNCATED] 

> # Compile-----------------------------------------------------------------
> model %<>% compile(
+   optimizer = optimizer,
+   loss      = loss,
+  .... [TRUNCATED] 

> # Save naïve model--------------------------------------------------------
> 
> # file_name <- paste0("lstm_QC_", 0, ".keras") 
> 
> save_model_weig .... [TRUNCATED] 

> # Callbacks---------------------------------------------------------------
> current_time <- format(Sys.time(), "%Y%m%d%H%M%S")

> file_name <- paste0("lstm_QC_op",current_time, ".keras")

> callbacks <- list(
+   callback_model_checkpoint(file.path(resultsdir, file_name),
+                             monitor = "rmse",
+                 .... [TRUNCATED] 

> # Define metric rmse ------------------------------------------------------
> 
> K <- keras::backend()

> rmse <- function(y_pred, y_true) {
+   sq_diff <- K$square(y_pred - y_true)
+   mean_sq_diff <- K$mean(sq_diff)
+   rmse_value <- K$sqrt(mean_sq_dif .... [TRUNCATED] 

> attr(rmse, "py_function_name") <- "rmse"

> # Train model--------------------------------------------------------------
> 
> history <- model %>%
+   fit(wrap_train,
+       epochs = epochs,
+ .... [TRUNCATED] 

> plot(history)

> # Evaluate model-----------------------------------------------------------
> 
> evaluation_result <- evaluate(model, wrap_test, steps = test_steps)

> evaluation_result
     loss       mae       mse      rmse 
0.6159209 0.6159209 0.7073588 0.8162165 

> library(tfruns)

> library(keras)

> # Hyperparameter flags ---------------------------------------------------
> 
> FLAGS <- flags(
+   flag_numeric("lookback", 10),
+   flag_numeric(" ..." ... [TRUNCATED] 

> # Generator functions------------------------------------------------------
> # Con wavelets:
> # train_data <- as.matrix(train_code[,-c(1,19)])
> # .... [TRUNCATED] 

> lookback <- FLAGS$lookback

> batch_size <- FLAGS$batch_size

> delay <- 1

> min_index <- 1

> predseries <- 1

> train_k <-  create_lstm_data.2(
+   data = train_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_ .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> train_steps <- round((nrow(train_data)-lookback) / batch_size) 

> wrap_train <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- train_k(),
+       error = function(e) {
+ .... [TRUNCATED] 

> #########################################################################
> # Con wavelets:
> # val_data <- as.matrix(val_code[,-c(1, 19)])
> # Sin  .... [TRUNCATED] 

> val_k <- create_lstm_data.2(
+   data = val_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_index .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> val_steps <- round((nrow(val_data)-lookback) / batch_size) 

> wrap_val <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- val_k(),
+       error = function(e) {
+     .... [TRUNCATED] 

> ##########################################################################
> # Con wavelets:
> # test_data <- as.matrix(test_code[,-c(1, 19)])
> # S .... [TRUNCATED] 

> test_k <-  create_lstm_data.2(
+   data = test_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_in .... [TRUNCATED] 

> wrap_test <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- test_k(),
+       error = function(e) {
+   .... [TRUNCATED] 

> # Cálculo del número de steps de test:
> test_steps <- round((nrow(test_data)-lookback) / batch_size) 

> #######################################################################
> 
> 
> # Model parameters-------------------------------------------------- .... [TRUNCATED] 

> # dynamic_features = 15 # Variables dinámicas (QC_RESULT + wavelets)
> static_features = 3  # Variables estáticas (ANALIZADOR, CODIGO_PRUEBA Y NIVEL .... [TRUNCATED] 

> lstm_units = FLAGS$lstm_units # Número de unidades en la capa LSTM 

> vocabulary_size = 7 # Número de niveles codificados de las variables estáticas.

> epochs <- 20

> optimizer <- "adam"

> loss <- "mae"

> # Define Model ------------------------------------------------------------
> 
> # Modelo LSTM:
> 
> # Capa de entrada de datos dinámicos:
> dynamic .... [TRUNCATED] 

> #dynamic_input_layer <- layer_masking(mask_value =
> #-99999)(dynamic_input_layer)
> # Capa de entrada de datos estáticos: 
> static_input_layer <-  .... [TRUNCATED] 

> # Aplanado de la capa de datos estáticos antes del embedding:
> flatten_static_layer <- layer_flatten()(static_input_layer)

> # Capa a de embedding para los datos estáticos:
> embedding_layer <- layer_embedding(
+   input_dim = vocabulary_size,
+   output_dim = lstm_units)( .... [TRUNCATED] 

> # Capa lambda para seleccionar los embeddings que modificarán los estados ocultos de la capa LSTM:
> embedding_layer <- layer_lambda(f = \(x) x[,1,] .... [TRUNCATED] 

> # Capa LSTM que recibe dos valores de estado oculto de la capa de embeddings con la información de las variables estáticas:
> lstm_layer_b1 <- layer .... [TRUNCATED] 

> #lstm_layer_b2 <- layer_lstm(units = lstm_units,
>  #                           name = "lstm_2", 
>   #                          dropout = FLAGS$dro .... [TRUNCATED] 

> # Capa de salida con una unidad de activación lineal para la predicción:  
> output_layer <- layer_dense(units = 1,
+                             ac .... [TRUNCATED] 

> # Create model-------------------------------------------------------------
> model <- 
+   keras_model(
+     inputs  = list(dynamic_input_layer, s .... [TRUNCATED] 

> # Compile-----------------------------------------------------------------
> model %<>% compile(
+   optimizer = optimizer,
+   loss      = loss,
+  .... [TRUNCATED] 

> # Save naïve model--------------------------------------------------------
> 
> # file_name <- paste0("lstm_QC_", 0, ".keras") 
> 
> save_model_weig .... [TRUNCATED] 

> # Callbacks---------------------------------------------------------------
> current_time <- format(Sys.time(), "%Y%m%d%H%M%S")

> file_name <- paste0("lstm_QC_op",current_time, ".keras")

> callbacks <- list(
+   callback_model_checkpoint(file.path(resultsdir, file_name),
+                             monitor = "rmse",
+                 .... [TRUNCATED] 

> # Define metric rmse ------------------------------------------------------
> 
> K <- keras::backend()

> rmse <- function(y_pred, y_true) {
+   sq_diff <- K$square(y_pred - y_true)
+   mean_sq_diff <- K$mean(sq_diff)
+   rmse_value <- K$sqrt(mean_sq_dif .... [TRUNCATED] 

> attr(rmse, "py_function_name") <- "rmse"

> # Train model--------------------------------------------------------------
> 
> history <- model %>%
+   fit(wrap_train,
+       epochs = epochs,
+ .... [TRUNCATED] 

> plot(history)

> # Evaluate model-----------------------------------------------------------
> 
> evaluation_result <- evaluate(model, wrap_test, steps = test_steps)

> evaluation_result
     loss       mae       mse      rmse 
0.5504798 0.5504798 0.6414208 0.7790030 

> library(tfruns)

> library(keras)

> # Hyperparameter flags ---------------------------------------------------
> 
> FLAGS <- flags(
+   flag_numeric("lookback", 10),
+   flag_numeric(" ..." ... [TRUNCATED] 

> # Generator functions------------------------------------------------------
> # Con wavelets:
> # train_data <- as.matrix(train_code[,-c(1,19)])
> # .... [TRUNCATED] 

> lookback <- FLAGS$lookback

> batch_size <- FLAGS$batch_size

> delay <- 1

> min_index <- 1

> predseries <- 1

> train_k <-  create_lstm_data.2(
+   data = train_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_ .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> train_steps <- round((nrow(train_data)-lookback) / batch_size) 

> wrap_train <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- train_k(),
+       error = function(e) {
+ .... [TRUNCATED] 

> #########################################################################
> # Con wavelets:
> # val_data <- as.matrix(val_code[,-c(1, 19)])
> # Sin  .... [TRUNCATED] 

> val_k <- create_lstm_data.2(
+   data = val_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_index .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> val_steps <- round((nrow(val_data)-lookback) / batch_size) 

> wrap_val <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- val_k(),
+       error = function(e) {
+     .... [TRUNCATED] 

> ##########################################################################
> # Con wavelets:
> # test_data <- as.matrix(test_code[,-c(1, 19)])
> # S .... [TRUNCATED] 

> test_k <-  create_lstm_data.2(
+   data = test_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_in .... [TRUNCATED] 

> wrap_test <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- test_k(),
+       error = function(e) {
+   .... [TRUNCATED] 

> # Cálculo del número de steps de test:
> test_steps <- round((nrow(test_data)-lookback) / batch_size) 

> #######################################################################
> 
> 
> # Model parameters-------------------------------------------------- .... [TRUNCATED] 

> # dynamic_features = 15 # Variables dinámicas (QC_RESULT + wavelets)
> static_features = 3  # Variables estáticas (ANALIZADOR, CODIGO_PRUEBA Y NIVEL .... [TRUNCATED] 

> lstm_units = FLAGS$lstm_units # Número de unidades en la capa LSTM 

> vocabulary_size = 7 # Número de niveles codificados de las variables estáticas.

> epochs <- 20

> optimizer <- "adam"

> loss <- "mae"

> # Define Model ------------------------------------------------------------
> 
> # Modelo LSTM:
> 
> # Capa de entrada de datos dinámicos:
> dynamic .... [TRUNCATED] 

> #dynamic_input_layer <- layer_masking(mask_value =
> #-99999)(dynamic_input_layer)
> # Capa de entrada de datos estáticos: 
> static_input_layer <-  .... [TRUNCATED] 

> # Aplanado de la capa de datos estáticos antes del embedding:
> flatten_static_layer <- layer_flatten()(static_input_layer)

> # Capa a de embedding para los datos estáticos:
> embedding_layer <- layer_embedding(
+   input_dim = vocabulary_size,
+   output_dim = lstm_units)( .... [TRUNCATED] 

> # Capa lambda para seleccionar los embeddings que modificarán los estados ocultos de la capa LSTM:
> embedding_layer <- layer_lambda(f = \(x) x[,1,] .... [TRUNCATED] 

> # Capa LSTM que recibe dos valores de estado oculto de la capa de embeddings con la información de las variables estáticas:
> lstm_layer_b1 <- layer .... [TRUNCATED] 

> #lstm_layer_b2 <- layer_lstm(units = lstm_units,
>  #                           name = "lstm_2", 
>   #                          dropout = FLAGS$dro .... [TRUNCATED] 

> # Capa de salida con una unidad de activación lineal para la predicción:  
> output_layer <- layer_dense(units = 1,
+                             ac .... [TRUNCATED] 

> # Create model-------------------------------------------------------------
> model <- 
+   keras_model(
+     inputs  = list(dynamic_input_layer, s .... [TRUNCATED] 

> # Compile-----------------------------------------------------------------
> model %<>% compile(
+   optimizer = optimizer,
+   loss      = loss,
+  .... [TRUNCATED] 

> # Save naïve model--------------------------------------------------------
> 
> # file_name <- paste0("lstm_QC_", 0, ".keras") 
> 
> save_model_weig .... [TRUNCATED] 

> # Callbacks---------------------------------------------------------------
> current_time <- format(Sys.time(), "%Y%m%d%H%M%S")

> file_name <- paste0("lstm_QC_op",current_time, ".keras")

> callbacks <- list(
+   callback_model_checkpoint(file.path(resultsdir, file_name),
+                             monitor = "rmse",
+                 .... [TRUNCATED] 

> # Define metric rmse ------------------------------------------------------
> 
> K <- keras::backend()

> rmse <- function(y_pred, y_true) {
+   sq_diff <- K$square(y_pred - y_true)
+   mean_sq_diff <- K$mean(sq_diff)
+   rmse_value <- K$sqrt(mean_sq_dif .... [TRUNCATED] 

> attr(rmse, "py_function_name") <- "rmse"

> # Train model--------------------------------------------------------------
> 
> history <- model %>%
+   fit(wrap_train,
+       epochs = epochs,
+ .... [TRUNCATED] 

> plot(history)

> # Evaluate model-----------------------------------------------------------
> 
> evaluation_result <- evaluate(model, wrap_test, steps = test_steps)

> evaluation_result
     loss       mae       mse      rmse 
0.7241047 0.7241047 0.7704155 0.8759989 

> library(tfruns)

> library(keras)

> # Hyperparameter flags ---------------------------------------------------
> 
> FLAGS <- flags(
+   flag_numeric("lookback", 10),
+   flag_numeric(" ..." ... [TRUNCATED] 

> # Generator functions------------------------------------------------------
> # Con wavelets:
> # train_data <- as.matrix(train_code[,-c(1,19)])
> # .... [TRUNCATED] 

> lookback <- FLAGS$lookback

> batch_size <- FLAGS$batch_size

> delay <- 1

> min_index <- 1

> predseries <- 1

> train_k <-  create_lstm_data.2(
+   data = train_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_ .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> train_steps <- round((nrow(train_data)-lookback) / batch_size) 

> wrap_train <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- train_k(),
+       error = function(e) {
+ .... [TRUNCATED] 

> #########################################################################
> # Con wavelets:
> # val_data <- as.matrix(val_code[,-c(1, 19)])
> # Sin  .... [TRUNCATED] 

> val_k <- create_lstm_data.2(
+   data = val_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_index .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> val_steps <- round((nrow(val_data)-lookback) / batch_size) 

> wrap_val <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- val_k(),
+       error = function(e) {
+     .... [TRUNCATED] 

> ##########################################################################
> # Con wavelets:
> # test_data <- as.matrix(test_code[,-c(1, 19)])
> # S .... [TRUNCATED] 

> test_k <-  create_lstm_data.2(
+   data = test_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_in .... [TRUNCATED] 

> wrap_test <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- test_k(),
+       error = function(e) {
+   .... [TRUNCATED] 

> # Cálculo del número de steps de test:
> test_steps <- round((nrow(test_data)-lookback) / batch_size) 

> #######################################################################
> 
> 
> # Model parameters-------------------------------------------------- .... [TRUNCATED] 

> # dynamic_features = 15 # Variables dinámicas (QC_RESULT + wavelets)
> static_features = 3  # Variables estáticas (ANALIZADOR, CODIGO_PRUEBA Y NIVEL .... [TRUNCATED] 

> lstm_units = FLAGS$lstm_units # Número de unidades en la capa LSTM 

> vocabulary_size = 7 # Número de niveles codificados de las variables estáticas.

> epochs <- 20

> optimizer <- "adam"

> loss <- "mae"

> # Define Model ------------------------------------------------------------
> 
> # Modelo LSTM:
> 
> # Capa de entrada de datos dinámicos:
> dynamic .... [TRUNCATED] 

> #dynamic_input_layer <- layer_masking(mask_value =
> #-99999)(dynamic_input_layer)
> # Capa de entrada de datos estáticos: 
> static_input_layer <-  .... [TRUNCATED] 

> # Aplanado de la capa de datos estáticos antes del embedding:
> flatten_static_layer <- layer_flatten()(static_input_layer)

> # Capa a de embedding para los datos estáticos:
> embedding_layer <- layer_embedding(
+   input_dim = vocabulary_size,
+   output_dim = lstm_units)( .... [TRUNCATED] 

> # Capa lambda para seleccionar los embeddings que modificarán los estados ocultos de la capa LSTM:
> embedding_layer <- layer_lambda(f = \(x) x[,1,] .... [TRUNCATED] 

> # Capa LSTM que recibe dos valores de estado oculto de la capa de embeddings con la información de las variables estáticas:
> lstm_layer_b1 <- layer .... [TRUNCATED] 

> #lstm_layer_b2 <- layer_lstm(units = lstm_units,
>  #                           name = "lstm_2", 
>   #                          dropout = FLAGS$dro .... [TRUNCATED] 

> # Capa de salida con una unidad de activación lineal para la predicción:  
> output_layer <- layer_dense(units = 1,
+                             ac .... [TRUNCATED] 

> # Create model-------------------------------------------------------------
> model <- 
+   keras_model(
+     inputs  = list(dynamic_input_layer, s .... [TRUNCATED] 

> # Compile-----------------------------------------------------------------
> model %<>% compile(
+   optimizer = optimizer,
+   loss      = loss,
+  .... [TRUNCATED] 

> # Save naïve model--------------------------------------------------------
> 
> # file_name <- paste0("lstm_QC_", 0, ".keras") 
> 
> save_model_weig .... [TRUNCATED] 

> # Callbacks---------------------------------------------------------------
> current_time <- format(Sys.time(), "%Y%m%d%H%M%S")

> file_name <- paste0("lstm_QC_op",current_time, ".keras")

> callbacks <- list(
+   callback_model_checkpoint(file.path(resultsdir, file_name),
+                             monitor = "rmse",
+                 .... [TRUNCATED] 

> # Define metric rmse ------------------------------------------------------
> 
> K <- keras::backend()

> rmse <- function(y_pred, y_true) {
+   sq_diff <- K$square(y_pred - y_true)
+   mean_sq_diff <- K$mean(sq_diff)
+   rmse_value <- K$sqrt(mean_sq_dif .... [TRUNCATED] 

> attr(rmse, "py_function_name") <- "rmse"

> # Train model--------------------------------------------------------------
> 
> history <- model %>%
+   fit(wrap_train,
+       epochs = epochs,
+ .... [TRUNCATED] 

> plot(history)

> # Evaluate model-----------------------------------------------------------
> 
> evaluation_result <- evaluate(model, wrap_test, steps = test_steps)

> evaluation_result
     loss       mae       mse      rmse 
0.6580042 0.6580042 0.7255682 0.8457913 

> library(tfruns)

> library(keras)

> # Hyperparameter flags ---------------------------------------------------
> 
> FLAGS <- flags(
+   flag_numeric("lookback", 10),
+   flag_numeric(" ..." ... [TRUNCATED] 

> # Generator functions------------------------------------------------------
> # Con wavelets:
> # train_data <- as.matrix(train_code[,-c(1,19)])
> # .... [TRUNCATED] 

> lookback <- FLAGS$lookback

> batch_size <- FLAGS$batch_size

> delay <- 1

> min_index <- 1

> predseries <- 1

> train_k <-  create_lstm_data.2(
+   data = train_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_ .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> train_steps <- round((nrow(train_data)-lookback) / batch_size) 

> wrap_train <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- train_k(),
+       error = function(e) {
+ .... [TRUNCATED] 

> #########################################################################
> # Con wavelets:
> # val_data <- as.matrix(val_code[,-c(1, 19)])
> # Sin  .... [TRUNCATED] 

> val_k <- create_lstm_data.2(
+   data = val_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_index .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> val_steps <- round((nrow(val_data)-lookback) / batch_size) 

> wrap_val <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- val_k(),
+       error = function(e) {
+     .... [TRUNCATED] 

> ##########################################################################
> # Con wavelets:
> # test_data <- as.matrix(test_code[,-c(1, 19)])
> # S .... [TRUNCATED] 

> test_k <-  create_lstm_data.2(
+   data = test_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_in .... [TRUNCATED] 

> wrap_test <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- test_k(),
+       error = function(e) {
+   .... [TRUNCATED] 

> # Cálculo del número de steps de test:
> test_steps <- round((nrow(test_data)-lookback) / batch_size) 

> #######################################################################
> 
> 
> # Model parameters-------------------------------------------------- .... [TRUNCATED] 

> # dynamic_features = 15 # Variables dinámicas (QC_RESULT + wavelets)
> static_features = 3  # Variables estáticas (ANALIZADOR, CODIGO_PRUEBA Y NIVEL .... [TRUNCATED] 

> lstm_units = FLAGS$lstm_units # Número de unidades en la capa LSTM 

> vocabulary_size = 7 # Número de niveles codificados de las variables estáticas.

> epochs <- 20

> optimizer <- "adam"

> loss <- "mae"

> # Define Model ------------------------------------------------------------
> 
> # Modelo LSTM:
> 
> # Capa de entrada de datos dinámicos:
> dynamic .... [TRUNCATED] 

> #dynamic_input_layer <- layer_masking(mask_value =
> #-99999)(dynamic_input_layer)
> # Capa de entrada de datos estáticos: 
> static_input_layer <-  .... [TRUNCATED] 

> # Aplanado de la capa de datos estáticos antes del embedding:
> flatten_static_layer <- layer_flatten()(static_input_layer)

> # Capa a de embedding para los datos estáticos:
> embedding_layer <- layer_embedding(
+   input_dim = vocabulary_size,
+   output_dim = lstm_units)( .... [TRUNCATED] 

> # Capa lambda para seleccionar los embeddings que modificarán los estados ocultos de la capa LSTM:
> embedding_layer <- layer_lambda(f = \(x) x[,1,] .... [TRUNCATED] 

> # Capa LSTM que recibe dos valores de estado oculto de la capa de embeddings con la información de las variables estáticas:
> lstm_layer_b1 <- layer .... [TRUNCATED] 

> #lstm_layer_b2 <- layer_lstm(units = lstm_units,
>  #                           name = "lstm_2", 
>   #                          dropout = FLAGS$dro .... [TRUNCATED] 

> # Capa de salida con una unidad de activación lineal para la predicción:  
> output_layer <- layer_dense(units = 1,
+                             ac .... [TRUNCATED] 

> # Create model-------------------------------------------------------------
> model <- 
+   keras_model(
+     inputs  = list(dynamic_input_layer, s .... [TRUNCATED] 

> # Compile-----------------------------------------------------------------
> model %<>% compile(
+   optimizer = optimizer,
+   loss      = loss,
+  .... [TRUNCATED] 

> # Save naïve model--------------------------------------------------------
> 
> # file_name <- paste0("lstm_QC_", 0, ".keras") 
> 
> save_model_weig .... [TRUNCATED] 

> # Callbacks---------------------------------------------------------------
> current_time <- format(Sys.time(), "%Y%m%d%H%M%S")

> file_name <- paste0("lstm_QC_op",current_time, ".keras")

> callbacks <- list(
+   callback_model_checkpoint(file.path(resultsdir, file_name),
+                             monitor = "rmse",
+                 .... [TRUNCATED] 

> # Define metric rmse ------------------------------------------------------
> 
> K <- keras::backend()

> rmse <- function(y_pred, y_true) {
+   sq_diff <- K$square(y_pred - y_true)
+   mean_sq_diff <- K$mean(sq_diff)
+   rmse_value <- K$sqrt(mean_sq_dif .... [TRUNCATED] 

> attr(rmse, "py_function_name") <- "rmse"

> # Train model--------------------------------------------------------------
> 
> history <- model %>%
+   fit(wrap_train,
+       epochs = epochs,
+ .... [TRUNCATED] 

> plot(history)

> # Evaluate model-----------------------------------------------------------
> 
> evaluation_result <- evaluate(model, wrap_test, steps = test_steps)

> evaluation_result
     loss       mae       mse      rmse 
0.7179361 0.7179361 0.7586920 0.8691987 

> library(tfruns)

> library(keras)

> # Hyperparameter flags ---------------------------------------------------
> 
> FLAGS <- flags(
+   flag_numeric("lookback", 10),
+   flag_numeric(" ..." ... [TRUNCATED] 

> # Generator functions------------------------------------------------------
> # Con wavelets:
> # train_data <- as.matrix(train_code[,-c(1,19)])
> # .... [TRUNCATED] 

> lookback <- FLAGS$lookback

> batch_size <- FLAGS$batch_size

> delay <- 1

> min_index <- 1

> predseries <- 1

> train_k <-  create_lstm_data.2(
+   data = train_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_ .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> train_steps <- round((nrow(train_data)-lookback) / batch_size) 

> wrap_train <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- train_k(),
+       error = function(e) {
+ .... [TRUNCATED] 

> #########################################################################
> # Con wavelets:
> # val_data <- as.matrix(val_code[,-c(1, 19)])
> # Sin  .... [TRUNCATED] 

> val_k <- create_lstm_data.2(
+   data = val_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_index .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> val_steps <- round((nrow(val_data)-lookback) / batch_size) 

> wrap_val <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- val_k(),
+       error = function(e) {
+     .... [TRUNCATED] 

> ##########################################################################
> # Con wavelets:
> # test_data <- as.matrix(test_code[,-c(1, 19)])
> # S .... [TRUNCATED] 

> test_k <-  create_lstm_data.2(
+   data = test_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_in .... [TRUNCATED] 

> wrap_test <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- test_k(),
+       error = function(e) {
+   .... [TRUNCATED] 

> # Cálculo del número de steps de test:
> test_steps <- round((nrow(test_data)-lookback) / batch_size) 

> #######################################################################
> 
> 
> # Model parameters-------------------------------------------------- .... [TRUNCATED] 

> # dynamic_features = 15 # Variables dinámicas (QC_RESULT + wavelets)
> static_features = 3  # Variables estáticas (ANALIZADOR, CODIGO_PRUEBA Y NIVEL .... [TRUNCATED] 

> lstm_units = FLAGS$lstm_units # Número de unidades en la capa LSTM 

> vocabulary_size = 7 # Número de niveles codificados de las variables estáticas.

> epochs <- 20

> optimizer <- "adam"

> loss <- "mae"

> # Define Model ------------------------------------------------------------
> 
> # Modelo LSTM:
> 
> # Capa de entrada de datos dinámicos:
> dynamic .... [TRUNCATED] 

> #dynamic_input_layer <- layer_masking(mask_value =
> #-99999)(dynamic_input_layer)
> # Capa de entrada de datos estáticos: 
> static_input_layer <-  .... [TRUNCATED] 

> # Aplanado de la capa de datos estáticos antes del embedding:
> flatten_static_layer <- layer_flatten()(static_input_layer)

> # Capa a de embedding para los datos estáticos:
> embedding_layer <- layer_embedding(
+   input_dim = vocabulary_size,
+   output_dim = lstm_units)( .... [TRUNCATED] 

> # Capa lambda para seleccionar los embeddings que modificarán los estados ocultos de la capa LSTM:
> embedding_layer <- layer_lambda(f = \(x) x[,1,] .... [TRUNCATED] 

> # Capa LSTM que recibe dos valores de estado oculto de la capa de embeddings con la información de las variables estáticas:
> lstm_layer_b1 <- layer .... [TRUNCATED] 

> #lstm_layer_b2 <- layer_lstm(units = lstm_units,
>  #                           name = "lstm_2", 
>   #                          dropout = FLAGS$dro .... [TRUNCATED] 

> # Capa de salida con una unidad de activación lineal para la predicción:  
> output_layer <- layer_dense(units = 1,
+                             ac .... [TRUNCATED] 

> # Create model-------------------------------------------------------------
> model <- 
+   keras_model(
+     inputs  = list(dynamic_input_layer, s .... [TRUNCATED] 

> # Compile-----------------------------------------------------------------
> model %<>% compile(
+   optimizer = optimizer,
+   loss      = loss,
+  .... [TRUNCATED] 

> # Save naïve model--------------------------------------------------------
> 
> # file_name <- paste0("lstm_QC_", 0, ".keras") 
> 
> save_model_weig .... [TRUNCATED] 

> # Callbacks---------------------------------------------------------------
> current_time <- format(Sys.time(), "%Y%m%d%H%M%S")

> file_name <- paste0("lstm_QC_op",current_time, ".keras")

> callbacks <- list(
+   callback_model_checkpoint(file.path(resultsdir, file_name),
+                             monitor = "rmse",
+                 .... [TRUNCATED] 

> # Define metric rmse ------------------------------------------------------
> 
> K <- keras::backend()

> rmse <- function(y_pred, y_true) {
+   sq_diff <- K$square(y_pred - y_true)
+   mean_sq_diff <- K$mean(sq_diff)
+   rmse_value <- K$sqrt(mean_sq_dif .... [TRUNCATED] 

> attr(rmse, "py_function_name") <- "rmse"

> # Train model--------------------------------------------------------------
> 
> history <- model %>%
+   fit(wrap_train,
+       epochs = epochs,
+ .... [TRUNCATED] 

> plot(history)

> # Evaluate model-----------------------------------------------------------
> 
> evaluation_result <- evaluate(model, wrap_test, steps = test_steps)

> evaluation_result
     loss       mae       mse      rmse 
0.6365979 0.6365979 0.7269349 0.8383657 

> library(tfruns)

> library(keras)

> # Hyperparameter flags ---------------------------------------------------
> 
> FLAGS <- flags(
+   flag_numeric("lookback", 10),
+   flag_numeric(" ..." ... [TRUNCATED] 

> # Generator functions------------------------------------------------------
> # Con wavelets:
> # train_data <- as.matrix(train_code[,-c(1,19)])
> # .... [TRUNCATED] 

> lookback <- FLAGS$lookback

> batch_size <- FLAGS$batch_size

> delay <- 1

> min_index <- 1

> predseries <- 1

> train_k <-  create_lstm_data.2(
+   data = train_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_ .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> train_steps <- round((nrow(train_data)-lookback) / batch_size) 

> wrap_train <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- train_k(),
+       error = function(e) {
+ .... [TRUNCATED] 

> #########################################################################
> # Con wavelets:
> # val_data <- as.matrix(val_code[,-c(1, 19)])
> # Sin  .... [TRUNCATED] 

> val_k <- create_lstm_data.2(
+   data = val_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_index .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> val_steps <- round((nrow(val_data)-lookback) / batch_size) 

> wrap_val <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- val_k(),
+       error = function(e) {
+     .... [TRUNCATED] 

> ##########################################################################
> # Con wavelets:
> # test_data <- as.matrix(test_code[,-c(1, 19)])
> # S .... [TRUNCATED] 

> test_k <-  create_lstm_data.2(
+   data = test_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_in .... [TRUNCATED] 

> wrap_test <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- test_k(),
+       error = function(e) {
+   .... [TRUNCATED] 

> # Cálculo del número de steps de test:
> test_steps <- round((nrow(test_data)-lookback) / batch_size) 

> #######################################################################
> 
> 
> # Model parameters-------------------------------------------------- .... [TRUNCATED] 

> # dynamic_features = 15 # Variables dinámicas (QC_RESULT + wavelets)
> static_features = 3  # Variables estáticas (ANALIZADOR, CODIGO_PRUEBA Y NIVEL .... [TRUNCATED] 

> lstm_units = FLAGS$lstm_units # Número de unidades en la capa LSTM 

> vocabulary_size = 7 # Número de niveles codificados de las variables estáticas.

> epochs <- 20

> optimizer <- "adam"

> loss <- "mae"

> # Define Model ------------------------------------------------------------
> 
> # Modelo LSTM:
> 
> # Capa de entrada de datos dinámicos:
> dynamic .... [TRUNCATED] 

> #dynamic_input_layer <- layer_masking(mask_value =
> #-99999)(dynamic_input_layer)
> # Capa de entrada de datos estáticos: 
> static_input_layer <-  .... [TRUNCATED] 

> # Aplanado de la capa de datos estáticos antes del embedding:
> flatten_static_layer <- layer_flatten()(static_input_layer)

> # Capa a de embedding para los datos estáticos:
> embedding_layer <- layer_embedding(
+   input_dim = vocabulary_size,
+   output_dim = lstm_units)( .... [TRUNCATED] 

> # Capa lambda para seleccionar los embeddings que modificarán los estados ocultos de la capa LSTM:
> embedding_layer <- layer_lambda(f = \(x) x[,1,] .... [TRUNCATED] 

> # Capa LSTM que recibe dos valores de estado oculto de la capa de embeddings con la información de las variables estáticas:
> lstm_layer_b1 <- layer .... [TRUNCATED] 

> #lstm_layer_b2 <- layer_lstm(units = lstm_units,
>  #                           name = "lstm_2", 
>   #                          dropout = FLAGS$dro .... [TRUNCATED] 

> # Capa de salida con una unidad de activación lineal para la predicción:  
> output_layer <- layer_dense(units = 1,
+                             ac .... [TRUNCATED] 

> # Create model-------------------------------------------------------------
> model <- 
+   keras_model(
+     inputs  = list(dynamic_input_layer, s .... [TRUNCATED] 

> # Compile-----------------------------------------------------------------
> model %<>% compile(
+   optimizer = optimizer,
+   loss      = loss,
+  .... [TRUNCATED] 

> # Save naïve model--------------------------------------------------------
> 
> # file_name <- paste0("lstm_QC_", 0, ".keras") 
> 
> save_model_weig .... [TRUNCATED] 

> # Callbacks---------------------------------------------------------------
> current_time <- format(Sys.time(), "%Y%m%d%H%M%S")

> file_name <- paste0("lstm_QC_op",current_time, ".keras")

> callbacks <- list(
+   callback_model_checkpoint(file.path(resultsdir, file_name),
+                             monitor = "rmse",
+                 .... [TRUNCATED] 

> # Define metric rmse ------------------------------------------------------
> 
> K <- keras::backend()

> rmse <- function(y_pred, y_true) {
+   sq_diff <- K$square(y_pred - y_true)
+   mean_sq_diff <- K$mean(sq_diff)
+   rmse_value <- K$sqrt(mean_sq_dif .... [TRUNCATED] 

> attr(rmse, "py_function_name") <- "rmse"

> # Train model--------------------------------------------------------------
> 
> history <- model %>%
+   fit(wrap_train,
+       epochs = epochs,
+ .... [TRUNCATED] 

> plot(history)

> # Evaluate model-----------------------------------------------------------
> 
> evaluation_result <- evaluate(model, wrap_test, steps = test_steps)

> evaluation_result
     loss       mae       mse      rmse 
0.6193009 0.6193009 0.6716270 0.7989147 

> library(tfruns)

> library(keras)

> # Hyperparameter flags ---------------------------------------------------
> 
> FLAGS <- flags(
+   flag_numeric("lookback", 10),
+   flag_numeric(" ..." ... [TRUNCATED] 

> # Generator functions------------------------------------------------------
> # Con wavelets:
> # train_data <- as.matrix(train_code[,-c(1,19)])
> # .... [TRUNCATED] 

> lookback <- FLAGS$lookback

> batch_size <- FLAGS$batch_size

> delay <- 1

> min_index <- 1

> predseries <- 1

> train_k <-  create_lstm_data.2(
+   data = train_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_ .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> train_steps <- round((nrow(train_data)-lookback) / batch_size) 

> wrap_train <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- train_k(),
+       error = function(e) {
+ .... [TRUNCATED] 

> #########################################################################
> # Con wavelets:
> # val_data <- as.matrix(val_code[,-c(1, 19)])
> # Sin  .... [TRUNCATED] 

> val_k <- create_lstm_data.2(
+   data = val_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_index .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> val_steps <- round((nrow(val_data)-lookback) / batch_size) 

> wrap_val <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- val_k(),
+       error = function(e) {
+     .... [TRUNCATED] 

> ##########################################################################
> # Con wavelets:
> # test_data <- as.matrix(test_code[,-c(1, 19)])
> # S .... [TRUNCATED] 

> test_k <-  create_lstm_data.2(
+   data = test_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_in .... [TRUNCATED] 

> wrap_test <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- test_k(),
+       error = function(e) {
+   .... [TRUNCATED] 

> # Cálculo del número de steps de test:
> test_steps <- round((nrow(test_data)-lookback) / batch_size) 

> #######################################################################
> 
> 
> # Model parameters-------------------------------------------------- .... [TRUNCATED] 

> # dynamic_features = 15 # Variables dinámicas (QC_RESULT + wavelets)
> static_features = 3  # Variables estáticas (ANALIZADOR, CODIGO_PRUEBA Y NIVEL .... [TRUNCATED] 

> lstm_units = FLAGS$lstm_units # Número de unidades en la capa LSTM 

> vocabulary_size = 7 # Número de niveles codificados de las variables estáticas.

> epochs <- 20

> optimizer <- "adam"

> loss <- "mae"

> # Define Model ------------------------------------------------------------
> 
> # Modelo LSTM:
> 
> # Capa de entrada de datos dinámicos:
> dynamic .... [TRUNCATED] 

> #dynamic_input_layer <- layer_masking(mask_value =
> #-99999)(dynamic_input_layer)
> # Capa de entrada de datos estáticos: 
> static_input_layer <-  .... [TRUNCATED] 

> # Aplanado de la capa de datos estáticos antes del embedding:
> flatten_static_layer <- layer_flatten()(static_input_layer)

> # Capa a de embedding para los datos estáticos:
> embedding_layer <- layer_embedding(
+   input_dim = vocabulary_size,
+   output_dim = lstm_units)( .... [TRUNCATED] 

> # Capa lambda para seleccionar los embeddings que modificarán los estados ocultos de la capa LSTM:
> embedding_layer <- layer_lambda(f = \(x) x[,1,] .... [TRUNCATED] 

> # Capa LSTM que recibe dos valores de estado oculto de la capa de embeddings con la información de las variables estáticas:
> lstm_layer_b1 <- layer .... [TRUNCATED] 

> #lstm_layer_b2 <- layer_lstm(units = lstm_units,
>  #                           name = "lstm_2", 
>   #                          dropout = FLAGS$dro .... [TRUNCATED] 

> # Capa de salida con una unidad de activación lineal para la predicción:  
> output_layer <- layer_dense(units = 1,
+                             ac .... [TRUNCATED] 

> # Create model-------------------------------------------------------------
> model <- 
+   keras_model(
+     inputs  = list(dynamic_input_layer, s .... [TRUNCATED] 

> # Compile-----------------------------------------------------------------
> model %<>% compile(
+   optimizer = optimizer,
+   loss      = loss,
+  .... [TRUNCATED] 

> # Save naïve model--------------------------------------------------------
> 
> # file_name <- paste0("lstm_QC_", 0, ".keras") 
> 
> save_model_weig .... [TRUNCATED] 

> # Callbacks---------------------------------------------------------------
> current_time <- format(Sys.time(), "%Y%m%d%H%M%S")

> file_name <- paste0("lstm_QC_op",current_time, ".keras")

> callbacks <- list(
+   callback_model_checkpoint(file.path(resultsdir, file_name),
+                             monitor = "rmse",
+                 .... [TRUNCATED] 

> # Define metric rmse ------------------------------------------------------
> 
> K <- keras::backend()

> rmse <- function(y_pred, y_true) {
+   sq_diff <- K$square(y_pred - y_true)
+   mean_sq_diff <- K$mean(sq_diff)
+   rmse_value <- K$sqrt(mean_sq_dif .... [TRUNCATED] 

> attr(rmse, "py_function_name") <- "rmse"

> # Train model--------------------------------------------------------------
> 
> history <- model %>%
+   fit(wrap_train,
+       epochs = epochs,
+ .... [TRUNCATED] 

> plot(history)

> # Evaluate model-----------------------------------------------------------
> 
> evaluation_result <- evaluate(model, wrap_test, steps = test_steps)

> evaluation_result
     loss       mae       mse      rmse 
0.6192608 0.6192608 0.6954437 0.8148577 

> library(tfruns)

> library(keras)

> # Hyperparameter flags ---------------------------------------------------
> 
> FLAGS <- flags(
+   flag_numeric("lookback", 10),
+   flag_numeric(" ..." ... [TRUNCATED] 

> # Generator functions------------------------------------------------------
> # Con wavelets:
> # train_data <- as.matrix(train_code[,-c(1,19)])
> # .... [TRUNCATED] 

> lookback <- FLAGS$lookback

> batch_size <- FLAGS$batch_size

> delay <- 1

> min_index <- 1

> predseries <- 1

> train_k <-  create_lstm_data.2(
+   data = train_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_ .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> train_steps <- round((nrow(train_data)-lookback) / batch_size) 

> wrap_train <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- train_k(),
+       error = function(e) {
+ .... [TRUNCATED] 

> #########################################################################
> # Con wavelets:
> # val_data <- as.matrix(val_code[,-c(1, 19)])
> # Sin  .... [TRUNCATED] 

> val_k <- create_lstm_data.2(
+   data = val_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_index .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> val_steps <- round((nrow(val_data)-lookback) / batch_size) 

> wrap_val <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- val_k(),
+       error = function(e) {
+     .... [TRUNCATED] 

> ##########################################################################
> # Con wavelets:
> # test_data <- as.matrix(test_code[,-c(1, 19)])
> # S .... [TRUNCATED] 

> test_k <-  create_lstm_data.2(
+   data = test_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_in .... [TRUNCATED] 

> wrap_test <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- test_k(),
+       error = function(e) {
+   .... [TRUNCATED] 

> # Cálculo del número de steps de test:
> test_steps <- round((nrow(test_data)-lookback) / batch_size) 

> #######################################################################
> 
> 
> # Model parameters-------------------------------------------------- .... [TRUNCATED] 

> # dynamic_features = 15 # Variables dinámicas (QC_RESULT + wavelets)
> static_features = 3  # Variables estáticas (ANALIZADOR, CODIGO_PRUEBA Y NIVEL .... [TRUNCATED] 

> lstm_units = FLAGS$lstm_units # Número de unidades en la capa LSTM 

> vocabulary_size = 7 # Número de niveles codificados de las variables estáticas.

> epochs <- 20

> optimizer <- "adam"

> loss <- "mae"

> # Define Model ------------------------------------------------------------
> 
> # Modelo LSTM:
> 
> # Capa de entrada de datos dinámicos:
> dynamic .... [TRUNCATED] 

> #dynamic_input_layer <- layer_masking(mask_value =
> #-99999)(dynamic_input_layer)
> # Capa de entrada de datos estáticos: 
> static_input_layer <-  .... [TRUNCATED] 

> # Aplanado de la capa de datos estáticos antes del embedding:
> flatten_static_layer <- layer_flatten()(static_input_layer)

> # Capa a de embedding para los datos estáticos:
> embedding_layer <- layer_embedding(
+   input_dim = vocabulary_size,
+   output_dim = lstm_units)( .... [TRUNCATED] 

> # Capa lambda para seleccionar los embeddings que modificarán los estados ocultos de la capa LSTM:
> embedding_layer <- layer_lambda(f = \(x) x[,1,] .... [TRUNCATED] 

> # Capa LSTM que recibe dos valores de estado oculto de la capa de embeddings con la información de las variables estáticas:
> lstm_layer_b1 <- layer .... [TRUNCATED] 

> #lstm_layer_b2 <- layer_lstm(units = lstm_units,
>  #                           name = "lstm_2", 
>   #                          dropout = FLAGS$dro .... [TRUNCATED] 

> # Capa de salida con una unidad de activación lineal para la predicción:  
> output_layer <- layer_dense(units = 1,
+                             ac .... [TRUNCATED] 

> # Create model-------------------------------------------------------------
> model <- 
+   keras_model(
+     inputs  = list(dynamic_input_layer, s .... [TRUNCATED] 

> # Compile-----------------------------------------------------------------
> model %<>% compile(
+   optimizer = optimizer,
+   loss      = loss,
+  .... [TRUNCATED] 

> # Save naïve model--------------------------------------------------------
> 
> # file_name <- paste0("lstm_QC_", 0, ".keras") 
> 
> save_model_weig .... [TRUNCATED] 

> # Callbacks---------------------------------------------------------------
> current_time <- format(Sys.time(), "%Y%m%d%H%M%S")

> file_name <- paste0("lstm_QC_op",current_time, ".keras")

> callbacks <- list(
+   callback_model_checkpoint(file.path(resultsdir, file_name),
+                             monitor = "rmse",
+                 .... [TRUNCATED] 

> # Define metric rmse ------------------------------------------------------
> 
> K <- keras::backend()

> rmse <- function(y_pred, y_true) {
+   sq_diff <- K$square(y_pred - y_true)
+   mean_sq_diff <- K$mean(sq_diff)
+   rmse_value <- K$sqrt(mean_sq_dif .... [TRUNCATED] 

> attr(rmse, "py_function_name") <- "rmse"

> # Train model--------------------------------------------------------------
> 
> history <- model %>%
+   fit(wrap_train,
+       epochs = epochs,
+ .... [TRUNCATED] 

> plot(history)

> # Evaluate model-----------------------------------------------------------
> 
> evaluation_result <- evaluate(model, wrap_test, steps = test_steps)

> evaluation_result
     loss       mae       mse      rmse 
0.6173851 0.6173851 0.7302942 0.8404605 

> library(tfruns)

> library(keras)

> # Hyperparameter flags ---------------------------------------------------
> 
> FLAGS <- flags(
+   flag_numeric("lookback", 10),
+   flag_numeric(" ..." ... [TRUNCATED] 

> # Generator functions------------------------------------------------------
> # Con wavelets:
> # train_data <- as.matrix(train_code[,-c(1,19)])
> # .... [TRUNCATED] 

> lookback <- FLAGS$lookback

> batch_size <- FLAGS$batch_size

> delay <- 1

> min_index <- 1

> predseries <- 1

> train_k <-  create_lstm_data.2(
+   data = train_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_ .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> train_steps <- round((nrow(train_data)-lookback) / batch_size) 

> wrap_train <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- train_k(),
+       error = function(e) {
+ .... [TRUNCATED] 

> #########################################################################
> # Con wavelets:
> # val_data <- as.matrix(val_code[,-c(1, 19)])
> # Sin  .... [TRUNCATED] 

> val_k <- create_lstm_data.2(
+   data = val_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_index .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> val_steps <- round((nrow(val_data)-lookback) / batch_size) 

> wrap_val <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- val_k(),
+       error = function(e) {
+     .... [TRUNCATED] 

> ##########################################################################
> # Con wavelets:
> # test_data <- as.matrix(test_code[,-c(1, 19)])
> # S .... [TRUNCATED] 

> test_k <-  create_lstm_data.2(
+   data = test_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_in .... [TRUNCATED] 

> wrap_test <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- test_k(),
+       error = function(e) {
+   .... [TRUNCATED] 

> # Cálculo del número de steps de test:
> test_steps <- round((nrow(test_data)-lookback) / batch_size) 

> #######################################################################
> 
> 
> # Model parameters-------------------------------------------------- .... [TRUNCATED] 

> # dynamic_features = 15 # Variables dinámicas (QC_RESULT + wavelets)
> static_features = 3  # Variables estáticas (ANALIZADOR, CODIGO_PRUEBA Y NIVEL .... [TRUNCATED] 

> lstm_units = FLAGS$lstm_units # Número de unidades en la capa LSTM 

> vocabulary_size = 7 # Número de niveles codificados de las variables estáticas.

> epochs <- 20

> optimizer <- "adam"

> loss <- "mae"

> # Define Model ------------------------------------------------------------
> 
> # Modelo LSTM:
> 
> # Capa de entrada de datos dinámicos:
> dynamic .... [TRUNCATED] 

> #dynamic_input_layer <- layer_masking(mask_value =
> #-99999)(dynamic_input_layer)
> # Capa de entrada de datos estáticos: 
> static_input_layer <-  .... [TRUNCATED] 

> # Aplanado de la capa de datos estáticos antes del embedding:
> flatten_static_layer <- layer_flatten()(static_input_layer)

> # Capa a de embedding para los datos estáticos:
> embedding_layer <- layer_embedding(
+   input_dim = vocabulary_size,
+   output_dim = lstm_units)( .... [TRUNCATED] 

> # Capa lambda para seleccionar los embeddings que modificarán los estados ocultos de la capa LSTM:
> embedding_layer <- layer_lambda(f = \(x) x[,1,] .... [TRUNCATED] 

> # Capa LSTM que recibe dos valores de estado oculto de la capa de embeddings con la información de las variables estáticas:
> lstm_layer_b1 <- layer .... [TRUNCATED] 

> #lstm_layer_b2 <- layer_lstm(units = lstm_units,
>  #                           name = "lstm_2", 
>   #                          dropout = FLAGS$dro .... [TRUNCATED] 

> # Capa de salida con una unidad de activación lineal para la predicción:  
> output_layer <- layer_dense(units = 1,
+                             ac .... [TRUNCATED] 

> # Create model-------------------------------------------------------------
> model <- 
+   keras_model(
+     inputs  = list(dynamic_input_layer, s .... [TRUNCATED] 

> # Compile-----------------------------------------------------------------
> model %<>% compile(
+   optimizer = optimizer,
+   loss      = loss,
+  .... [TRUNCATED] 

> # Save naïve model--------------------------------------------------------
> 
> # file_name <- paste0("lstm_QC_", 0, ".keras") 
> 
> save_model_weig .... [TRUNCATED] 

> # Callbacks---------------------------------------------------------------
> current_time <- format(Sys.time(), "%Y%m%d%H%M%S")

> file_name <- paste0("lstm_QC_op",current_time, ".keras")

> callbacks <- list(
+   callback_model_checkpoint(file.path(resultsdir, file_name),
+                             monitor = "rmse",
+                 .... [TRUNCATED] 

> # Define metric rmse ------------------------------------------------------
> 
> K <- keras::backend()

> rmse <- function(y_pred, y_true) {
+   sq_diff <- K$square(y_pred - y_true)
+   mean_sq_diff <- K$mean(sq_diff)
+   rmse_value <- K$sqrt(mean_sq_dif .... [TRUNCATED] 

> attr(rmse, "py_function_name") <- "rmse"

> # Train model--------------------------------------------------------------
> 
> history <- model %>%
+   fit(wrap_train,
+       epochs = epochs,
+ .... [TRUNCATED] 

> plot(history)

> # Evaluate model-----------------------------------------------------------
> 
> evaluation_result <- evaluate(model, wrap_test, steps = test_steps)

> evaluation_result
     loss       mae       mse      rmse 
0.7041011 0.7041011 0.7933918 0.8873649 

> library(tfruns)

> library(keras)

> # Hyperparameter flags ---------------------------------------------------
> 
> FLAGS <- flags(
+   flag_numeric("lookback", 10),
+   flag_numeric(" ..." ... [TRUNCATED] 

> # Generator functions------------------------------------------------------
> # Con wavelets:
> # train_data <- as.matrix(train_code[,-c(1,19)])
> # .... [TRUNCATED] 

> lookback <- FLAGS$lookback

> batch_size <- FLAGS$batch_size

> delay <- 1

> min_index <- 1

> predseries <- 1

> train_k <-  create_lstm_data.2(
+   data = train_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_ .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> train_steps <- round((nrow(train_data)-lookback) / batch_size) 

> wrap_train <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- train_k(),
+       error = function(e) {
+ .... [TRUNCATED] 

> #########################################################################
> # Con wavelets:
> # val_data <- as.matrix(val_code[,-c(1, 19)])
> # Sin  .... [TRUNCATED] 

> val_k <- create_lstm_data.2(
+   data = val_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_index .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> val_steps <- round((nrow(val_data)-lookback) / batch_size) 

> wrap_val <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- val_k(),
+       error = function(e) {
+     .... [TRUNCATED] 

> ##########################################################################
> # Con wavelets:
> # test_data <- as.matrix(test_code[,-c(1, 19)])
> # S .... [TRUNCATED] 

> test_k <-  create_lstm_data.2(
+   data = test_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_in .... [TRUNCATED] 

> wrap_test <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- test_k(),
+       error = function(e) {
+   .... [TRUNCATED] 

> # Cálculo del número de steps de test:
> test_steps <- round((nrow(test_data)-lookback) / batch_size) 

> #######################################################################
> 
> 
> # Model parameters-------------------------------------------------- .... [TRUNCATED] 

> # dynamic_features = 15 # Variables dinámicas (QC_RESULT + wavelets)
> static_features = 3  # Variables estáticas (ANALIZADOR, CODIGO_PRUEBA Y NIVEL .... [TRUNCATED] 

> lstm_units = FLAGS$lstm_units # Número de unidades en la capa LSTM 

> vocabulary_size = 7 # Número de niveles codificados de las variables estáticas.

> epochs <- 20

> optimizer <- "adam"

> loss <- "mae"

> # Define Model ------------------------------------------------------------
> 
> # Modelo LSTM:
> 
> # Capa de entrada de datos dinámicos:
> dynamic .... [TRUNCATED] 

> #dynamic_input_layer <- layer_masking(mask_value =
> #-99999)(dynamic_input_layer)
> # Capa de entrada de datos estáticos: 
> static_input_layer <-  .... [TRUNCATED] 

> # Aplanado de la capa de datos estáticos antes del embedding:
> flatten_static_layer <- layer_flatten()(static_input_layer)

> # Capa a de embedding para los datos estáticos:
> embedding_layer <- layer_embedding(
+   input_dim = vocabulary_size,
+   output_dim = lstm_units)( .... [TRUNCATED] 

> # Capa lambda para seleccionar los embeddings que modificarán los estados ocultos de la capa LSTM:
> embedding_layer <- layer_lambda(f = \(x) x[,1,] .... [TRUNCATED] 

> # Capa LSTM que recibe dos valores de estado oculto de la capa de embeddings con la información de las variables estáticas:
> lstm_layer_b1 <- layer .... [TRUNCATED] 

> #lstm_layer_b2 <- layer_lstm(units = lstm_units,
>  #                           name = "lstm_2", 
>   #                          dropout = FLAGS$dro .... [TRUNCATED] 

> # Capa de salida con una unidad de activación lineal para la predicción:  
> output_layer <- layer_dense(units = 1,
+                             ac .... [TRUNCATED] 

> # Create model-------------------------------------------------------------
> model <- 
+   keras_model(
+     inputs  = list(dynamic_input_layer, s .... [TRUNCATED] 

> # Compile-----------------------------------------------------------------
> model %<>% compile(
+   optimizer = optimizer,
+   loss      = loss,
+  .... [TRUNCATED] 

> # Save naïve model--------------------------------------------------------
> 
> # file_name <- paste0("lstm_QC_", 0, ".keras") 
> 
> save_model_weig .... [TRUNCATED] 

> # Callbacks---------------------------------------------------------------
> current_time <- format(Sys.time(), "%Y%m%d%H%M%S")

> file_name <- paste0("lstm_QC_op",current_time, ".keras")

> callbacks <- list(
+   callback_model_checkpoint(file.path(resultsdir, file_name),
+                             monitor = "rmse",
+                 .... [TRUNCATED] 

> # Define metric rmse ------------------------------------------------------
> 
> K <- keras::backend()

> rmse <- function(y_pred, y_true) {
+   sq_diff <- K$square(y_pred - y_true)
+   mean_sq_diff <- K$mean(sq_diff)
+   rmse_value <- K$sqrt(mean_sq_dif .... [TRUNCATED] 

> attr(rmse, "py_function_name") <- "rmse"

> # Train model--------------------------------------------------------------
> 
> history <- model %>%
+   fit(wrap_train,
+       epochs = epochs,
+ .... [TRUNCATED] 

> plot(history)

> # Evaluate model-----------------------------------------------------------
> 
> evaluation_result <- evaluate(model, wrap_test, steps = test_steps)

> evaluation_result
     loss       mae       mse      rmse 
0.5999749 0.5999749 0.6320631 0.7792133 

> library(tfruns)

> library(keras)

> # Hyperparameter flags ---------------------------------------------------
> 
> FLAGS <- flags(
+   flag_numeric("lookback", 10),
+   flag_numeric(" ..." ... [TRUNCATED] 

> # Generator functions------------------------------------------------------
> # Con wavelets:
> # train_data <- as.matrix(train_code[,-c(1,19)])
> # .... [TRUNCATED] 

> lookback <- FLAGS$lookback

> batch_size <- FLAGS$batch_size

> delay <- 1

> min_index <- 1

> predseries <- 1

> train_k <-  create_lstm_data.2(
+   data = train_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_ .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> train_steps <- round((nrow(train_data)-lookback) / batch_size) 

> wrap_train <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- train_k(),
+       error = function(e) {
+ .... [TRUNCATED] 

> #########################################################################
> # Con wavelets:
> # val_data <- as.matrix(val_code[,-c(1, 19)])
> # Sin  .... [TRUNCATED] 

> val_k <- create_lstm_data.2(
+   data = val_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_index .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> val_steps <- round((nrow(val_data)-lookback) / batch_size) 

> wrap_val <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- val_k(),
+       error = function(e) {
+     .... [TRUNCATED] 

> ##########################################################################
> # Con wavelets:
> # test_data <- as.matrix(test_code[,-c(1, 19)])
> # S .... [TRUNCATED] 

> test_k <-  create_lstm_data.2(
+   data = test_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_in .... [TRUNCATED] 

> wrap_test <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- test_k(),
+       error = function(e) {
+   .... [TRUNCATED] 

> # Cálculo del número de steps de test:
> test_steps <- round((nrow(test_data)-lookback) / batch_size) 

> #######################################################################
> 
> 
> # Model parameters-------------------------------------------------- .... [TRUNCATED] 

> # dynamic_features = 15 # Variables dinámicas (QC_RESULT + wavelets)
> static_features = 3  # Variables estáticas (ANALIZADOR, CODIGO_PRUEBA Y NIVEL .... [TRUNCATED] 

> lstm_units = FLAGS$lstm_units # Número de unidades en la capa LSTM 

> vocabulary_size = 7 # Número de niveles codificados de las variables estáticas.

> epochs <- 20

> optimizer <- "adam"

> loss <- "mae"

> # Define Model ------------------------------------------------------------
> 
> # Modelo LSTM:
> 
> # Capa de entrada de datos dinámicos:
> dynamic .... [TRUNCATED] 

> #dynamic_input_layer <- layer_masking(mask_value =
> #-99999)(dynamic_input_layer)
> # Capa de entrada de datos estáticos: 
> static_input_layer <-  .... [TRUNCATED] 

> # Aplanado de la capa de datos estáticos antes del embedding:
> flatten_static_layer <- layer_flatten()(static_input_layer)

> # Capa a de embedding para los datos estáticos:
> embedding_layer <- layer_embedding(
+   input_dim = vocabulary_size,
+   output_dim = lstm_units)( .... [TRUNCATED] 

> # Capa lambda para seleccionar los embeddings que modificarán los estados ocultos de la capa LSTM:
> embedding_layer <- layer_lambda(f = \(x) x[,1,] .... [TRUNCATED] 

> # Capa LSTM que recibe dos valores de estado oculto de la capa de embeddings con la información de las variables estáticas:
> lstm_layer_b1 <- layer .... [TRUNCATED] 

> #lstm_layer_b2 <- layer_lstm(units = lstm_units,
>  #                           name = "lstm_2", 
>   #                          dropout = FLAGS$dro .... [TRUNCATED] 

> # Capa de salida con una unidad de activación lineal para la predicción:  
> output_layer <- layer_dense(units = 1,
+                             ac .... [TRUNCATED] 

> # Create model-------------------------------------------------------------
> model <- 
+   keras_model(
+     inputs  = list(dynamic_input_layer, s .... [TRUNCATED] 

> # Compile-----------------------------------------------------------------
> model %<>% compile(
+   optimizer = optimizer,
+   loss      = loss,
+  .... [TRUNCATED] 

> # Save naïve model--------------------------------------------------------
> 
> # file_name <- paste0("lstm_QC_", 0, ".keras") 
> 
> save_model_weig .... [TRUNCATED] 

> # Callbacks---------------------------------------------------------------
> current_time <- format(Sys.time(), "%Y%m%d%H%M%S")

> file_name <- paste0("lstm_QC_op",current_time, ".keras")

> callbacks <- list(
+   callback_model_checkpoint(file.path(resultsdir, file_name),
+                             monitor = "rmse",
+                 .... [TRUNCATED] 

> # Define metric rmse ------------------------------------------------------
> 
> K <- keras::backend()

> rmse <- function(y_pred, y_true) {
+   sq_diff <- K$square(y_pred - y_true)
+   mean_sq_diff <- K$mean(sq_diff)
+   rmse_value <- K$sqrt(mean_sq_dif .... [TRUNCATED] 

> attr(rmse, "py_function_name") <- "rmse"

> # Train model--------------------------------------------------------------
> 
> history <- model %>%
+   fit(wrap_train,
+       epochs = epochs,
+ .... [TRUNCATED] 

> plot(history)

> # Evaluate model-----------------------------------------------------------
> 
> evaluation_result <- evaluate(model, wrap_test, steps = test_steps)

> evaluation_result
     loss       mae       mse      rmse 
0.5658047 0.5658047 0.6359127 0.7758410 

> library(tfruns)

> library(keras)

> # Hyperparameter flags ---------------------------------------------------
> 
> FLAGS <- flags(
+   flag_numeric("lookback", 10),
+   flag_numeric(" ..." ... [TRUNCATED] 

> # Generator functions------------------------------------------------------
> # Con wavelets:
> # train_data <- as.matrix(train_code[,-c(1,19)])
> # .... [TRUNCATED] 

> lookback <- FLAGS$lookback

> batch_size <- FLAGS$batch_size

> delay <- 1

> min_index <- 1

> predseries <- 1

> train_k <-  create_lstm_data.2(
+   data = train_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_ .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> train_steps <- round((nrow(train_data)-lookback) / batch_size) 

> wrap_train <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- train_k(),
+       error = function(e) {
+ .... [TRUNCATED] 

> #########################################################################
> # Con wavelets:
> # val_data <- as.matrix(val_code[,-c(1, 19)])
> # Sin  .... [TRUNCATED] 

> val_k <- create_lstm_data.2(
+   data = val_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_index .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> val_steps <- round((nrow(val_data)-lookback) / batch_size) 

> wrap_val <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- val_k(),
+       error = function(e) {
+     .... [TRUNCATED] 

> ##########################################################################
> # Con wavelets:
> # test_data <- as.matrix(test_code[,-c(1, 19)])
> # S .... [TRUNCATED] 

> test_k <-  create_lstm_data.2(
+   data = test_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_in .... [TRUNCATED] 

> wrap_test <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- test_k(),
+       error = function(e) {
+   .... [TRUNCATED] 

> # Cálculo del número de steps de test:
> test_steps <- round((nrow(test_data)-lookback) / batch_size) 

> #######################################################################
> 
> 
> # Model parameters-------------------------------------------------- .... [TRUNCATED] 

> # dynamic_features = 15 # Variables dinámicas (QC_RESULT + wavelets)
> static_features = 3  # Variables estáticas (ANALIZADOR, CODIGO_PRUEBA Y NIVEL .... [TRUNCATED] 

> lstm_units = FLAGS$lstm_units # Número de unidades en la capa LSTM 

> vocabulary_size = 7 # Número de niveles codificados de las variables estáticas.

> epochs <- 20

> optimizer <- "adam"

> loss <- "mae"

> # Define Model ------------------------------------------------------------
> 
> # Modelo LSTM:
> 
> # Capa de entrada de datos dinámicos:
> dynamic .... [TRUNCATED] 

> #dynamic_input_layer <- layer_masking(mask_value =
> #-99999)(dynamic_input_layer)
> # Capa de entrada de datos estáticos: 
> static_input_layer <-  .... [TRUNCATED] 

> # Aplanado de la capa de datos estáticos antes del embedding:
> flatten_static_layer <- layer_flatten()(static_input_layer)

> # Capa a de embedding para los datos estáticos:
> embedding_layer <- layer_embedding(
+   input_dim = vocabulary_size,
+   output_dim = lstm_units)( .... [TRUNCATED] 

> # Capa lambda para seleccionar los embeddings que modificarán los estados ocultos de la capa LSTM:
> embedding_layer <- layer_lambda(f = \(x) x[,1,] .... [TRUNCATED] 

> # Capa LSTM que recibe dos valores de estado oculto de la capa de embeddings con la información de las variables estáticas:
> lstm_layer_b1 <- layer .... [TRUNCATED] 

> #lstm_layer_b2 <- layer_lstm(units = lstm_units,
>  #                           name = "lstm_2", 
>   #                          dropout = FLAGS$dro .... [TRUNCATED] 

> # Capa de salida con una unidad de activación lineal para la predicción:  
> output_layer <- layer_dense(units = 1,
+                             ac .... [TRUNCATED] 

> # Create model-------------------------------------------------------------
> model <- 
+   keras_model(
+     inputs  = list(dynamic_input_layer, s .... [TRUNCATED] 

> # Compile-----------------------------------------------------------------
> model %<>% compile(
+   optimizer = optimizer,
+   loss      = loss,
+  .... [TRUNCATED] 

> # Save naïve model--------------------------------------------------------
> 
> # file_name <- paste0("lstm_QC_", 0, ".keras") 
> 
> save_model_weig .... [TRUNCATED] 

> # Callbacks---------------------------------------------------------------
> current_time <- format(Sys.time(), "%Y%m%d%H%M%S")

> file_name <- paste0("lstm_QC_op",current_time, ".keras")

> callbacks <- list(
+   callback_model_checkpoint(file.path(resultsdir, file_name),
+                             monitor = "rmse",
+                 .... [TRUNCATED] 

> # Define metric rmse ------------------------------------------------------
> 
> K <- keras::backend()

> rmse <- function(y_pred, y_true) {
+   sq_diff <- K$square(y_pred - y_true)
+   mean_sq_diff <- K$mean(sq_diff)
+   rmse_value <- K$sqrt(mean_sq_dif .... [TRUNCATED] 

> attr(rmse, "py_function_name") <- "rmse"

> # Train model--------------------------------------------------------------
> 
> history <- model %>%
+   fit(wrap_train,
+       epochs = epochs,
+ .... [TRUNCATED] 

> plot(history)

> # Evaluate model-----------------------------------------------------------
> 
> evaluation_result <- evaluate(model, wrap_test, steps = test_steps)

> evaluation_result
     loss       mae       mse      rmse 
0.5607206 0.5607206 0.6857957 0.8020919 

> library(tfruns)

> library(keras)

> # Hyperparameter flags ---------------------------------------------------
> 
> FLAGS <- flags(
+   flag_numeric("lookback", 10),
+   flag_numeric(" ..." ... [TRUNCATED] 

> # Generator functions------------------------------------------------------
> # Con wavelets:
> # train_data <- as.matrix(train_code[,-c(1,19)])
> # .... [TRUNCATED] 

> lookback <- FLAGS$lookback

> batch_size <- FLAGS$batch_size

> delay <- 1

> min_index <- 1

> predseries <- 1

> train_k <-  create_lstm_data.2(
+   data = train_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_ .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> train_steps <- round((nrow(train_data)-lookback) / batch_size) 

> wrap_train <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- train_k(),
+       error = function(e) {
+ .... [TRUNCATED] 

> #########################################################################
> # Con wavelets:
> # val_data <- as.matrix(val_code[,-c(1, 19)])
> # Sin  .... [TRUNCATED] 

> val_k <- create_lstm_data.2(
+   data = val_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_index .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> val_steps <- round((nrow(val_data)-lookback) / batch_size) 

> wrap_val <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- val_k(),
+       error = function(e) {
+     .... [TRUNCATED] 

> ##########################################################################
> # Con wavelets:
> # test_data <- as.matrix(test_code[,-c(1, 19)])
> # S .... [TRUNCATED] 

> test_k <-  create_lstm_data.2(
+   data = test_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_in .... [TRUNCATED] 

> wrap_test <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- test_k(),
+       error = function(e) {
+   .... [TRUNCATED] 

> # Cálculo del número de steps de test:
> test_steps <- round((nrow(test_data)-lookback) / batch_size) 

> #######################################################################
> 
> 
> # Model parameters-------------------------------------------------- .... [TRUNCATED] 

> # dynamic_features = 15 # Variables dinámicas (QC_RESULT + wavelets)
> static_features = 3  # Variables estáticas (ANALIZADOR, CODIGO_PRUEBA Y NIVEL .... [TRUNCATED] 

> lstm_units = FLAGS$lstm_units # Número de unidades en la capa LSTM 

> vocabulary_size = 7 # Número de niveles codificados de las variables estáticas.

> epochs <- 20

> optimizer <- "adam"

> loss <- "mae"

> # Define Model ------------------------------------------------------------
> 
> # Modelo LSTM:
> 
> # Capa de entrada de datos dinámicos:
> dynamic .... [TRUNCATED] 

> #dynamic_input_layer <- layer_masking(mask_value =
> #-99999)(dynamic_input_layer)
> # Capa de entrada de datos estáticos: 
> static_input_layer <-  .... [TRUNCATED] 

> # Aplanado de la capa de datos estáticos antes del embedding:
> flatten_static_layer <- layer_flatten()(static_input_layer)

> # Capa a de embedding para los datos estáticos:
> embedding_layer <- layer_embedding(
+   input_dim = vocabulary_size,
+   output_dim = lstm_units)( .... [TRUNCATED] 

> # Capa lambda para seleccionar los embeddings que modificarán los estados ocultos de la capa LSTM:
> embedding_layer <- layer_lambda(f = \(x) x[,1,] .... [TRUNCATED] 

> # Capa LSTM que recibe dos valores de estado oculto de la capa de embeddings con la información de las variables estáticas:
> lstm_layer_b1 <- layer .... [TRUNCATED] 

> #lstm_layer_b2 <- layer_lstm(units = lstm_units,
>  #                           name = "lstm_2", 
>   #                          dropout = FLAGS$dro .... [TRUNCATED] 

> # Capa de salida con una unidad de activación lineal para la predicción:  
> output_layer <- layer_dense(units = 1,
+                             ac .... [TRUNCATED] 

> # Create model-------------------------------------------------------------
> model <- 
+   keras_model(
+     inputs  = list(dynamic_input_layer, s .... [TRUNCATED] 

> # Compile-----------------------------------------------------------------
> model %<>% compile(
+   optimizer = optimizer,
+   loss      = loss,
+  .... [TRUNCATED] 

> # Save naïve model--------------------------------------------------------
> 
> # file_name <- paste0("lstm_QC_", 0, ".keras") 
> 
> save_model_weig .... [TRUNCATED] 

> # Callbacks---------------------------------------------------------------
> current_time <- format(Sys.time(), "%Y%m%d%H%M%S")

> file_name <- paste0("lstm_QC_op",current_time, ".keras")

> callbacks <- list(
+   callback_model_checkpoint(file.path(resultsdir, file_name),
+                             monitor = "rmse",
+                 .... [TRUNCATED] 

> # Define metric rmse ------------------------------------------------------
> 
> K <- keras::backend()

> rmse <- function(y_pred, y_true) {
+   sq_diff <- K$square(y_pred - y_true)
+   mean_sq_diff <- K$mean(sq_diff)
+   rmse_value <- K$sqrt(mean_sq_dif .... [TRUNCATED] 

> attr(rmse, "py_function_name") <- "rmse"

> # Train model--------------------------------------------------------------
> 
> history <- model %>%
+   fit(wrap_train,
+       epochs = epochs,
+ .... [TRUNCATED] 

> plot(history)

> # Evaluate model-----------------------------------------------------------
> 
> evaluation_result <- evaluate(model, wrap_test, steps = test_steps)

> evaluation_result
     loss       mae       mse      rmse 
0.5654462 0.5654462 0.6701084 0.7939776 

> library(tfruns)

> library(keras)

> # Hyperparameter flags ---------------------------------------------------
> 
> FLAGS <- flags(
+   flag_numeric("lookback", 10),
+   flag_numeric(" ..." ... [TRUNCATED] 

> # Generator functions------------------------------------------------------
> # Con wavelets:
> # train_data <- as.matrix(train_code[,-c(1,19)])
> # .... [TRUNCATED] 

> lookback <- FLAGS$lookback

> batch_size <- FLAGS$batch_size

> delay <- 1

> min_index <- 1

> predseries <- 1

> train_k <-  create_lstm_data.2(
+   data = train_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_ .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> train_steps <- round((nrow(train_data)-lookback) / batch_size) 

> wrap_train <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- train_k(),
+       error = function(e) {
+ .... [TRUNCATED] 

> #########################################################################
> # Con wavelets:
> # val_data <- as.matrix(val_code[,-c(1, 19)])
> # Sin  .... [TRUNCATED] 

> val_k <- create_lstm_data.2(
+   data = val_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_index .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> val_steps <- round((nrow(val_data)-lookback) / batch_size) 

> wrap_val <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- val_k(),
+       error = function(e) {
+     .... [TRUNCATED] 

> ##########################################################################
> # Con wavelets:
> # test_data <- as.matrix(test_code[,-c(1, 19)])
> # S .... [TRUNCATED] 

> test_k <-  create_lstm_data.2(
+   data = test_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_in .... [TRUNCATED] 

> wrap_test <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- test_k(),
+       error = function(e) {
+   .... [TRUNCATED] 

> # Cálculo del número de steps de test:
> test_steps <- round((nrow(test_data)-lookback) / batch_size) 

> #######################################################################
> 
> 
> # Model parameters-------------------------------------------------- .... [TRUNCATED] 

> # dynamic_features = 15 # Variables dinámicas (QC_RESULT + wavelets)
> static_features = 3  # Variables estáticas (ANALIZADOR, CODIGO_PRUEBA Y NIVEL .... [TRUNCATED] 

> lstm_units = FLAGS$lstm_units # Número de unidades en la capa LSTM 

> vocabulary_size = 7 # Número de niveles codificados de las variables estáticas.

> epochs <- 20

> optimizer <- "adam"

> loss <- "mae"

> # Define Model ------------------------------------------------------------
> 
> # Modelo LSTM:
> 
> # Capa de entrada de datos dinámicos:
> dynamic .... [TRUNCATED] 

> #dynamic_input_layer <- layer_masking(mask_value =
> #-99999)(dynamic_input_layer)
> # Capa de entrada de datos estáticos: 
> static_input_layer <-  .... [TRUNCATED] 

> # Aplanado de la capa de datos estáticos antes del embedding:
> flatten_static_layer <- layer_flatten()(static_input_layer)

> # Capa a de embedding para los datos estáticos:
> embedding_layer <- layer_embedding(
+   input_dim = vocabulary_size,
+   output_dim = lstm_units)( .... [TRUNCATED] 

> # Capa lambda para seleccionar los embeddings que modificarán los estados ocultos de la capa LSTM:
> embedding_layer <- layer_lambda(f = \(x) x[,1,] .... [TRUNCATED] 

> # Capa LSTM que recibe dos valores de estado oculto de la capa de embeddings con la información de las variables estáticas:
> lstm_layer_b1 <- layer .... [TRUNCATED] 

> #lstm_layer_b2 <- layer_lstm(units = lstm_units,
>  #                           name = "lstm_2", 
>   #                          dropout = FLAGS$dro .... [TRUNCATED] 

> # Capa de salida con una unidad de activación lineal para la predicción:  
> output_layer <- layer_dense(units = 1,
+                             ac .... [TRUNCATED] 

> # Create model-------------------------------------------------------------
> model <- 
+   keras_model(
+     inputs  = list(dynamic_input_layer, s .... [TRUNCATED] 

> # Compile-----------------------------------------------------------------
> model %<>% compile(
+   optimizer = optimizer,
+   loss      = loss,
+  .... [TRUNCATED] 

> # Save naïve model--------------------------------------------------------
> 
> # file_name <- paste0("lstm_QC_", 0, ".keras") 
> 
> save_model_weig .... [TRUNCATED] 

> # Callbacks---------------------------------------------------------------
> current_time <- format(Sys.time(), "%Y%m%d%H%M%S")

> file_name <- paste0("lstm_QC_op",current_time, ".keras")

> callbacks <- list(
+   callback_model_checkpoint(file.path(resultsdir, file_name),
+                             monitor = "rmse",
+                 .... [TRUNCATED] 

> # Define metric rmse ------------------------------------------------------
> 
> K <- keras::backend()

> rmse <- function(y_pred, y_true) {
+   sq_diff <- K$square(y_pred - y_true)
+   mean_sq_diff <- K$mean(sq_diff)
+   rmse_value <- K$sqrt(mean_sq_dif .... [TRUNCATED] 

> attr(rmse, "py_function_name") <- "rmse"

> # Train model--------------------------------------------------------------
> 
> history <- model %>%
+   fit(wrap_train,
+       epochs = epochs,
+ .... [TRUNCATED] 

> plot(history)

> # Evaluate model-----------------------------------------------------------
> 
> evaluation_result <- evaluate(model, wrap_test, steps = test_steps)

> evaluation_result
     loss       mae       mse      rmse 
0.6059645 0.6059645 0.6655343 0.7988586 

> library(tfruns)

> library(keras)

> # Hyperparameter flags ---------------------------------------------------
> 
> FLAGS <- flags(
+   flag_numeric("lookback", 10),
+   flag_numeric(" ..." ... [TRUNCATED] 

> # Generator functions------------------------------------------------------
> # Con wavelets:
> # train_data <- as.matrix(train_code[,-c(1,19)])
> # .... [TRUNCATED] 

> lookback <- FLAGS$lookback

> batch_size <- FLAGS$batch_size

> delay <- 1

> min_index <- 1

> predseries <- 1

> train_k <-  create_lstm_data.2(
+   data = train_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_ .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> train_steps <- round((nrow(train_data)-lookback) / batch_size) 

> wrap_train <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- train_k(),
+       error = function(e) {
+ .... [TRUNCATED] 

> #########################################################################
> # Con wavelets:
> # val_data <- as.matrix(val_code[,-c(1, 19)])
> # Sin  .... [TRUNCATED] 

> val_k <- create_lstm_data.2(
+   data = val_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_index .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> val_steps <- round((nrow(val_data)-lookback) / batch_size) 

> wrap_val <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- val_k(),
+       error = function(e) {
+     .... [TRUNCATED] 

> ##########################################################################
> # Con wavelets:
> # test_data <- as.matrix(test_code[,-c(1, 19)])
> # S .... [TRUNCATED] 

> test_k <-  create_lstm_data.2(
+   data = test_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_in .... [TRUNCATED] 

> wrap_test <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- test_k(),
+       error = function(e) {
+   .... [TRUNCATED] 

> # Cálculo del número de steps de test:
> test_steps <- round((nrow(test_data)-lookback) / batch_size) 

> #######################################################################
> 
> 
> # Model parameters-------------------------------------------------- .... [TRUNCATED] 

> # dynamic_features = 15 # Variables dinámicas (QC_RESULT + wavelets)
> static_features = 3  # Variables estáticas (ANALIZADOR, CODIGO_PRUEBA Y NIVEL .... [TRUNCATED] 

> lstm_units = FLAGS$lstm_units # Número de unidades en la capa LSTM 

> vocabulary_size = 7 # Número de niveles codificados de las variables estáticas.

> epochs <- 20

> optimizer <- "adam"

> loss <- "mae"

> # Define Model ------------------------------------------------------------
> 
> # Modelo LSTM:
> 
> # Capa de entrada de datos dinámicos:
> dynamic .... [TRUNCATED] 

> #dynamic_input_layer <- layer_masking(mask_value =
> #-99999)(dynamic_input_layer)
> # Capa de entrada de datos estáticos: 
> static_input_layer <-  .... [TRUNCATED] 

> # Aplanado de la capa de datos estáticos antes del embedding:
> flatten_static_layer <- layer_flatten()(static_input_layer)

> # Capa a de embedding para los datos estáticos:
> embedding_layer <- layer_embedding(
+   input_dim = vocabulary_size,
+   output_dim = lstm_units)( .... [TRUNCATED] 

> # Capa lambda para seleccionar los embeddings que modificarán los estados ocultos de la capa LSTM:
> embedding_layer <- layer_lambda(f = \(x) x[,1,] .... [TRUNCATED] 

> # Capa LSTM que recibe dos valores de estado oculto de la capa de embeddings con la información de las variables estáticas:
> lstm_layer_b1 <- layer .... [TRUNCATED] 

> #lstm_layer_b2 <- layer_lstm(units = lstm_units,
>  #                           name = "lstm_2", 
>   #                          dropout = FLAGS$dro .... [TRUNCATED] 

> # Capa de salida con una unidad de activación lineal para la predicción:  
> output_layer <- layer_dense(units = 1,
+                             ac .... [TRUNCATED] 

> # Create model-------------------------------------------------------------
> model <- 
+   keras_model(
+     inputs  = list(dynamic_input_layer, s .... [TRUNCATED] 

> # Compile-----------------------------------------------------------------
> model %<>% compile(
+   optimizer = optimizer,
+   loss      = loss,
+  .... [TRUNCATED] 

> # Save naïve model--------------------------------------------------------
> 
> # file_name <- paste0("lstm_QC_", 0, ".keras") 
> 
> save_model_weig .... [TRUNCATED] 

> # Callbacks---------------------------------------------------------------
> current_time <- format(Sys.time(), "%Y%m%d%H%M%S")

> file_name <- paste0("lstm_QC_op",current_time, ".keras")

> callbacks <- list(
+   callback_model_checkpoint(file.path(resultsdir, file_name),
+                             monitor = "rmse",
+                 .... [TRUNCATED] 

> # Define metric rmse ------------------------------------------------------
> 
> K <- keras::backend()

> rmse <- function(y_pred, y_true) {
+   sq_diff <- K$square(y_pred - y_true)
+   mean_sq_diff <- K$mean(sq_diff)
+   rmse_value <- K$sqrt(mean_sq_dif .... [TRUNCATED] 

> attr(rmse, "py_function_name") <- "rmse"

> # Train model--------------------------------------------------------------
> 
> history <- model %>%
+   fit(wrap_train,
+       epochs = epochs,
+ .... [TRUNCATED] 

> plot(history)

> # Evaluate model-----------------------------------------------------------
> 
> evaluation_result <- evaluate(model, wrap_test, steps = test_steps)

> evaluation_result
     loss       mae       mse      rmse 
0.6237218 0.6237218 0.6674231 0.8038767 

> library(tfruns)

> library(keras)

> # Hyperparameter flags ---------------------------------------------------
> 
> FLAGS <- flags(
+   flag_numeric("lookback", 10),
+   flag_numeric(" ..." ... [TRUNCATED] 

> # Generator functions------------------------------------------------------
> # Con wavelets:
> # train_data <- as.matrix(train_code[,-c(1,19)])
> # .... [TRUNCATED] 

> lookback <- FLAGS$lookback

> batch_size <- FLAGS$batch_size

> delay <- 1

> min_index <- 1

> predseries <- 1

> train_k <-  create_lstm_data.2(
+   data = train_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_ .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> train_steps <- round((nrow(train_data)-lookback) / batch_size) 

> wrap_train <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- train_k(),
+       error = function(e) {
+ .... [TRUNCATED] 

> #########################################################################
> # Con wavelets:
> # val_data <- as.matrix(val_code[,-c(1, 19)])
> # Sin  .... [TRUNCATED] 

> val_k <- create_lstm_data.2(
+   data = val_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_index .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> val_steps <- round((nrow(val_data)-lookback) / batch_size) 

> wrap_val <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- val_k(),
+       error = function(e) {
+     .... [TRUNCATED] 

> ##########################################################################
> # Con wavelets:
> # test_data <- as.matrix(test_code[,-c(1, 19)])
> # S .... [TRUNCATED] 

> test_k <-  create_lstm_data.2(
+   data = test_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_in .... [TRUNCATED] 

> wrap_test <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- test_k(),
+       error = function(e) {
+   .... [TRUNCATED] 

> # Cálculo del número de steps de test:
> test_steps <- round((nrow(test_data)-lookback) / batch_size) 

> #######################################################################
> 
> 
> # Model parameters-------------------------------------------------- .... [TRUNCATED] 

> # dynamic_features = 15 # Variables dinámicas (QC_RESULT + wavelets)
> static_features = 3  # Variables estáticas (ANALIZADOR, CODIGO_PRUEBA Y NIVEL .... [TRUNCATED] 

> lstm_units = FLAGS$lstm_units # Número de unidades en la capa LSTM 

> vocabulary_size = 7 # Número de niveles codificados de las variables estáticas.

> epochs <- 20

> optimizer <- "adam"

> loss <- "mae"

> # Define Model ------------------------------------------------------------
> 
> # Modelo LSTM:
> 
> # Capa de entrada de datos dinámicos:
> dynamic .... [TRUNCATED] 

> #dynamic_input_layer <- layer_masking(mask_value =
> #-99999)(dynamic_input_layer)
> # Capa de entrada de datos estáticos: 
> static_input_layer <-  .... [TRUNCATED] 

> # Aplanado de la capa de datos estáticos antes del embedding:
> flatten_static_layer <- layer_flatten()(static_input_layer)

> # Capa a de embedding para los datos estáticos:
> embedding_layer <- layer_embedding(
+   input_dim = vocabulary_size,
+   output_dim = lstm_units)( .... [TRUNCATED] 

> # Capa lambda para seleccionar los embeddings que modificarán los estados ocultos de la capa LSTM:
> embedding_layer <- layer_lambda(f = \(x) x[,1,] .... [TRUNCATED] 

> # Capa LSTM que recibe dos valores de estado oculto de la capa de embeddings con la información de las variables estáticas:
> lstm_layer_b1 <- layer .... [TRUNCATED] 

> #lstm_layer_b2 <- layer_lstm(units = lstm_units,
>  #                           name = "lstm_2", 
>   #                          dropout = FLAGS$dro .... [TRUNCATED] 

> # Capa de salida con una unidad de activación lineal para la predicción:  
> output_layer <- layer_dense(units = 1,
+                             ac .... [TRUNCATED] 

> # Create model-------------------------------------------------------------
> model <- 
+   keras_model(
+     inputs  = list(dynamic_input_layer, s .... [TRUNCATED] 

> # Compile-----------------------------------------------------------------
> model %<>% compile(
+   optimizer = optimizer,
+   loss      = loss,
+  .... [TRUNCATED] 

> # Save naïve model--------------------------------------------------------
> 
> # file_name <- paste0("lstm_QC_", 0, ".keras") 
> 
> save_model_weig .... [TRUNCATED] 

> # Callbacks---------------------------------------------------------------
> current_time <- format(Sys.time(), "%Y%m%d%H%M%S")

> file_name <- paste0("lstm_QC_op",current_time, ".keras")

> callbacks <- list(
+   callback_model_checkpoint(file.path(resultsdir, file_name),
+                             monitor = "rmse",
+                 .... [TRUNCATED] 

> # Define metric rmse ------------------------------------------------------
> 
> K <- keras::backend()

> rmse <- function(y_pred, y_true) {
+   sq_diff <- K$square(y_pred - y_true)
+   mean_sq_diff <- K$mean(sq_diff)
+   rmse_value <- K$sqrt(mean_sq_dif .... [TRUNCATED] 

> attr(rmse, "py_function_name") <- "rmse"

> # Train model--------------------------------------------------------------
> 
> history <- model %>%
+   fit(wrap_train,
+       epochs = epochs,
+ .... [TRUNCATED] 

> plot(history)

> # Evaluate model-----------------------------------------------------------
> 
> evaluation_result <- evaluate(model, wrap_test, steps = test_steps)

> evaluation_result
     loss       mae       mse      rmse 
0.6392087 0.6392087 0.7148142 0.8319004 

> library(tfruns)

> library(keras)

> # Hyperparameter flags ---------------------------------------------------
> 
> FLAGS <- flags(
+   flag_numeric("lookback", 10),
+   flag_numeric(" ..." ... [TRUNCATED] 

> # Generator functions------------------------------------------------------
> # Con wavelets:
> # train_data <- as.matrix(train_code[,-c(1,19)])
> # .... [TRUNCATED] 

> lookback <- FLAGS$lookback

> batch_size <- FLAGS$batch_size

> delay <- 1

> min_index <- 1

> predseries <- 1

> train_k <-  create_lstm_data.2(
+   data = train_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_ .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> train_steps <- round((nrow(train_data)-lookback) / batch_size) 

> wrap_train <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- train_k(),
+       error = function(e) {
+ .... [TRUNCATED] 

> #########################################################################
> # Con wavelets:
> # val_data <- as.matrix(val_code[,-c(1, 19)])
> # Sin  .... [TRUNCATED] 

> val_k <- create_lstm_data.2(
+   data = val_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_index .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> val_steps <- round((nrow(val_data)-lookback) / batch_size) 

> wrap_val <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- val_k(),
+       error = function(e) {
+     .... [TRUNCATED] 

> ##########################################################################
> # Con wavelets:
> # test_data <- as.matrix(test_code[,-c(1, 19)])
> # S .... [TRUNCATED] 

> test_k <-  create_lstm_data.2(
+   data = test_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_in .... [TRUNCATED] 

> wrap_test <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- test_k(),
+       error = function(e) {
+   .... [TRUNCATED] 

> # Cálculo del número de steps de test:
> test_steps <- round((nrow(test_data)-lookback) / batch_size) 

> #######################################################################
> 
> 
> # Model parameters-------------------------------------------------- .... [TRUNCATED] 

> # dynamic_features = 15 # Variables dinámicas (QC_RESULT + wavelets)
> static_features = 3  # Variables estáticas (ANALIZADOR, CODIGO_PRUEBA Y NIVEL .... [TRUNCATED] 

> lstm_units = FLAGS$lstm_units # Número de unidades en la capa LSTM 

> vocabulary_size = 7 # Número de niveles codificados de las variables estáticas.

> epochs <- 20

> optimizer <- "adam"

> loss <- "mae"

> # Define Model ------------------------------------------------------------
> 
> # Modelo LSTM:
> 
> # Capa de entrada de datos dinámicos:
> dynamic .... [TRUNCATED] 

> #dynamic_input_layer <- layer_masking(mask_value =
> #-99999)(dynamic_input_layer)
> # Capa de entrada de datos estáticos: 
> static_input_layer <-  .... [TRUNCATED] 

> # Aplanado de la capa de datos estáticos antes del embedding:
> flatten_static_layer <- layer_flatten()(static_input_layer)

> # Capa a de embedding para los datos estáticos:
> embedding_layer <- layer_embedding(
+   input_dim = vocabulary_size,
+   output_dim = lstm_units)( .... [TRUNCATED] 

> # Capa lambda para seleccionar los embeddings que modificarán los estados ocultos de la capa LSTM:
> embedding_layer <- layer_lambda(f = \(x) x[,1,] .... [TRUNCATED] 

> # Capa LSTM que recibe dos valores de estado oculto de la capa de embeddings con la información de las variables estáticas:
> lstm_layer_b1 <- layer .... [TRUNCATED] 

> #lstm_layer_b2 <- layer_lstm(units = lstm_units,
>  #                           name = "lstm_2", 
>   #                          dropout = FLAGS$dro .... [TRUNCATED] 

> # Capa de salida con una unidad de activación lineal para la predicción:  
> output_layer <- layer_dense(units = 1,
+                             ac .... [TRUNCATED] 

> # Create model-------------------------------------------------------------
> model <- 
+   keras_model(
+     inputs  = list(dynamic_input_layer, s .... [TRUNCATED] 

> # Compile-----------------------------------------------------------------
> model %<>% compile(
+   optimizer = optimizer,
+   loss      = loss,
+  .... [TRUNCATED] 

> # Save naïve model--------------------------------------------------------
> 
> # file_name <- paste0("lstm_QC_", 0, ".keras") 
> 
> save_model_weig .... [TRUNCATED] 

> # Callbacks---------------------------------------------------------------
> current_time <- format(Sys.time(), "%Y%m%d%H%M%S")

> file_name <- paste0("lstm_QC_op",current_time, ".keras")

> callbacks <- list(
+   callback_model_checkpoint(file.path(resultsdir, file_name),
+                             monitor = "rmse",
+                 .... [TRUNCATED] 

> # Define metric rmse ------------------------------------------------------
> 
> K <- keras::backend()

> rmse <- function(y_pred, y_true) {
+   sq_diff <- K$square(y_pred - y_true)
+   mean_sq_diff <- K$mean(sq_diff)
+   rmse_value <- K$sqrt(mean_sq_dif .... [TRUNCATED] 

> attr(rmse, "py_function_name") <- "rmse"

> # Train model--------------------------------------------------------------
> 
> history <- model %>%
+   fit(wrap_train,
+       epochs = epochs,
+ .... [TRUNCATED] 

> plot(history)

> # Evaluate model-----------------------------------------------------------
> 
> evaluation_result <- evaluate(model, wrap_test, steps = test_steps)

> evaluation_result
     loss       mae       mse      rmse 
0.6800498 0.6800498 0.7089529 0.8367643 

> library(tfruns)

> library(keras)

> # Hyperparameter flags ---------------------------------------------------
> 
> FLAGS <- flags(
+   flag_numeric("lookback", 10),
+   flag_numeric(" ..." ... [TRUNCATED] 

> # Generator functions------------------------------------------------------
> # Con wavelets:
> # train_data <- as.matrix(train_code[,-c(1,19)])
> # .... [TRUNCATED] 

> lookback <- FLAGS$lookback

> batch_size <- FLAGS$batch_size

> delay <- 1

> min_index <- 1

> predseries <- 1

> train_k <-  create_lstm_data.2(
+   data = train_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_ .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> train_steps <- round((nrow(train_data)-lookback) / batch_size) 

> wrap_train <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- train_k(),
+       error = function(e) {
+ .... [TRUNCATED] 

> #########################################################################
> # Con wavelets:
> # val_data <- as.matrix(val_code[,-c(1, 19)])
> # Sin  .... [TRUNCATED] 

> val_k <- create_lstm_data.2(
+   data = val_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_index .... [TRUNCATED] 

> # Cálculo del número de steps de training:
> val_steps <- round((nrow(val_data)-lookback) / batch_size) 

> wrap_val <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- val_k(),
+       error = function(e) {
+     .... [TRUNCATED] 

> ##########################################################################
> # Con wavelets:
> # test_data <- as.matrix(test_code[,-c(1, 19)])
> # S .... [TRUNCATED] 

> test_k <-  create_lstm_data.2(
+   data = test_data,
+   lookback = lookback,
+   delay = delay,
+   batch_size = batch_size,
+   min_index = min_in .... [TRUNCATED] 

> wrap_test <- function() {
+   seq <- NULL
+   
+   while (is.null(seq)) {
+     tryCatch(
+       seq <- test_k(),
+       error = function(e) {
+   .... [TRUNCATED] 

> # Cálculo del número de steps de test:
> test_steps <- round((nrow(test_data)-lookback) / batch_size) 

> #######################################################################
> 
> 
> # Model parameters-------------------------------------------------- .... [TRUNCATED] 

> # dynamic_features = 15 # Variables dinámicas (QC_RESULT + wavelets)
> static_features = 3  # Variables estáticas (ANALIZADOR, CODIGO_PRUEBA Y NIVEL .... [TRUNCATED] 

> lstm_units = FLAGS$lstm_units # Número de unidades en la capa LSTM 

> vocabulary_size = 7 # Número de niveles codificados de las variables estáticas.

> epochs <- 20

> optimizer <- "adam"

> loss <- "mae"

> # Define Model ------------------------------------------------------------
> 
> # Modelo LSTM:
> 
> # Capa de entrada de datos dinámicos:
> dynamic .... [TRUNCATED] 

> #dynamic_input_layer <- layer_masking(mask_value =
> #-99999)(dynamic_input_layer)
> # Capa de entrada de datos estáticos: 
> static_input_layer <-  .... [TRUNCATED] 

> # Aplanado de la capa de datos estáticos antes del embedding:
> flatten_static_layer <- layer_flatten()(static_input_layer)

> # Capa a de embedding para los datos estáticos:
> embedding_layer <- layer_embedding(
+   input_dim = vocabulary_size,
+   output_dim = lstm_units)( .... [TRUNCATED] 

> # Capa lambda para seleccionar los embeddings que modificarán los estados ocultos de la capa LSTM:
> embedding_layer <- layer_lambda(f = \(x) x[,1,] .... [TRUNCATED] 

> # Capa LSTM que recibe dos valores de estado oculto de la capa de embeddings con la información de las variables estáticas:
> lstm_layer_b1 <- layer .... [TRUNCATED] 

> #lstm_layer_b2 <- layer_lstm(units = lstm_units,
>  #                           name = "lstm_2", 
>   #                          dropout = FLAGS$dro .... [TRUNCATED] 

> # Capa de salida con una unidad de activación lineal para la predicción:  
> output_layer <- layer_dense(units = 1,
+                             ac .... [TRUNCATED] 

> # Create model-------------------------------------------------------------
> model <- 
+   keras_model(
+     inputs  = list(dynamic_input_layer, s .... [TRUNCATED] 

> # Compile-----------------------------------------------------------------
> model %<>% compile(
+   optimizer = optimizer,
+   loss      = loss,
+  .... [TRUNCATED] 

> # Save naïve model--------------------------------------------------------
> 
> # file_name <- paste0("lstm_QC_", 0, ".keras") 
> 
> save_model_weig .... [TRUNCATED] 

> # Callbacks---------------------------------------------------------------
> current_time <- format(Sys.time(), "%Y%m%d%H%M%S")

> file_name <- paste0("lstm_QC_op",current_time, ".keras")

> callbacks <- list(
+   callback_model_checkpoint(file.path(resultsdir, file_name),
+                             monitor = "rmse",
+                 .... [TRUNCATED] 

> # Define metric rmse ------------------------------------------------------
> 
> K <- keras::backend()

> rmse <- function(y_pred, y_true) {
+   sq_diff <- K$square(y_pred - y_true)
+   mean_sq_diff <- K$mean(sq_diff)
+   rmse_value <- K$sqrt(mean_sq_dif .... [TRUNCATED] 

> attr(rmse, "py_function_name") <- "rmse"

> # Train model--------------------------------------------------------------
> 
> history <- model %>%
+   fit(wrap_train,
+       epochs = epochs,
+ .... [TRUNCATED] 

> plot(history)

> # Evaluate model-----------------------------------------------------------
> 
> evaluation_result <- evaluate(model, wrap_test, steps = test_steps)

> evaluation_result
     loss       mae       mse      rmse 
0.6442061 0.6442061 0.6646430 0.8049376 
