---
title: 'Aplicación del aprendizaje automático al laboratorio de diagnóstico clínico:
  Detección temprana de series analíticas fuera de control en análisis inmunoquímicos
  de muestras de sangre mediante algoritmos de Machine Learning' 
author: "Sergio García Muñoz"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# MODELO LSTM_onehot_QC

```{r, include=FALSE}
# Definir los nombres de los paquetes que se desean instalar
packages <- c("readr", "readxl", "purrr", "dplyr", "filesstrings",
              "stringr", "tidyr", "lubridate", "ggplot2", "caret",
              "tictoc", "wavelets", "gridExtra", "reticulate", "abind",
              "tensorflow", "tfdatasets", "keras", "ModelMetrics")

# Función para instalar paquetes si no están ya instalados
install_packages <- function(package) {
  if (!require(package, character.only = TRUE)) {
    install.packages(package)
    }
}

# Aplicar la función para cada uno de los paquetes
lapply(packages, install_packages)

```

```{r}
require(readr)
require(readxl)
require(purrr)
require(dplyr)
require(filesstrings)
require(stringr)
require(tidyr)
require(lubridate)
require(ggplot2)
require(caret)
require(wavelets)
library(gridExtra)
require(tictoc)
require(reticulate)
require(abind)
require(tensorflow)
require(tfdatasets)
require(keras)
require(ModelMetrics)
```


```{r}
# Definir el directorio donde se encuentran los archivos de datos:
workingdir <- getwd()
datadir <- file.path(workingdir, "Datos/daily_s")
eventdir <- file.path(workingdir, "Datos/daily_s/Event")
eventdir_old <- file.path(workingdir, "Datos/daily_s/Event/old")
lotdir <- file.path(workingdir, "Datos/daily_s/Lot")
lotdir_old <- file.path(workingdir, "Datos/daily_s/Lot/old")
qcdir <- file.path(workingdir, "Datos/daily_s/qc")
qcdir_old <- file.path(workingdir, "Datos/daily_s/qc/old")
resultsdir <- file.path(workingdir, "Resultados")
figuresdir <- file.path(workingdir, "Resultados/Figuras")

# Definir el intervalo de tiempo en segundos entre las lecturas
interval <- 60
```

# Definición de funciones

## Función ```read_qc```.

Leer los archivos xls de valores de QC para cada técnica y cada máquina, añadir una columna con la id del equipo, otra con el código CLC de la prueba y crear un data frame final. La función contiene una condición if diseñada por el diferente formato de exportación de los archivos del control en uso y el histórico de controles inactivos:

```{r read_qc}
read_qc <- function() {
  # Crear una lista de archivos existentes en datadir que contienen los valores de QC, tanto en uso como inactivos:
  files_list <- list.files(qcdir, pattern = "CLC",
                           full.names = T) 
  
  # Lista vacía para contener los dataframes originados en el loop:
  list_df <- list()
 
  # Loop for para crear una lista de dataframes leyendo cada           archivo de qc:
  for(i in 1:length(files_list)){
    data <- read_xls(files_list[i], skip = 4)
  
    # Buscar y extraer el id del equipo en la primera fila:
    dev_ids <- c("DXI800 num 1", "DXI800 num 2",
             "DXI800 num 3")
    first_rows <- read_xls(files_list[i], 
                        col_names = F, n_max = 3) 
    match_dev <- which(grepl(paste(dev_ids, 
                                   collapse = "|"),
                                   first_rows))
    start_pos <- regexpr("(?<=DxI)\\S+",
                         first_rows[match_dev],
                         perl = TRUE)
    dev <- substring(first_rows[match_dev], start_pos)
    
    # Extraer el código CLC del nombre del archivo
    start_pos <- regexpr("(?<=CLC)\\S+",
                         files_list[i],
                         perl = TRUE)
    end_pos <- regexpr("\\)", files_list[i], start_pos)
    clc <- substring(files_list[i], start_pos, end_pos - 1)
    
    # Añadir como columnas "ANALIZADOR" y "CODIGO_PRUEBA" las           cadenas extraídas:
    data %<>% mutate("ANALIZADOR"= substr(dev, 13, 24),
                     "CODIGO_PRUEBA"= paste("CLC",clc, sep = "")) %>%
              relocate("ANALIZADOR", "CODIGO_PRUEBA", .after = 5)
    
    # En caso de que el archivo de datos sea del histórico, la    identificación del control no se encuentra como columna, sino en la cabecera, ya que los archivos han sido generados para cada  nivel de control individualmente. 
    # Identificación de este tipo de archivo, extracción de la identificación y creación de la columna "Control": 
    
    if(!("Control" %in% colnames(data))) {
      # Extraer el nombre del control de la fila 3, columna D:
      cont <- read_xls(files_list[i], 
                        col_names = F, range = "D2:D2")
      data %<>% mutate("Control"= as.character(cont)) %>%
              relocate("Control", .after = 4)
      }
    
    # Agrega el dataframe actual a la lista:
    list_df[[i]] <- data
  }  
    
  for (i in seq_along(list_df)) {
  if (!is.numeric(list_df[[i]][[3]])) {
    list_df[[i]][[3]] <- as.numeric(as.character(list_df[[i]][[3]]))
  }
}

  # Unir todos los dataframes en uno solo:
  df <- bind_rows(list_df)
  
  # Cambia el nombre a la columna Lote de reactivos:
  colnames(df)[10] <- "LOTE_REACTIVO"
  
  # Fusiona las columnas de los valores de concentraciones         encontrados en distintas unidades en una sola columna y cambia el tipo de valor a numérico:
   df <- unite(df, "Resultado", c(3, 13:21),
                          sep = "", na.rm = T)
   df[[3]] %<>% as.numeric()
   
  # Reordena el dataframe:
   df %<>% relocate("Sup/Inf Media", .after = 4)
   
  # Cambia el nombre a la columna Lote de reactivos:
   colnames(df)[3] <- "QC_RESULT"
  
  # Elimina filas con valores NA en LOTE_REACTIVO:
  
   df <- df[complete.cases(df$LOTE_REACTIVO), ]
  
   # Mueve el archivo leído a la carpeta old:
  # file.move(files_list, qc_old, overwrite = TRUE)
  
  return(df)
}
```

## Función ```qc_level```.

Función para detectar el nivel de cada control de calidad basándose en el string que contiene su nombre (detecta 1, 2 ó 3 al final del string o bien las palabras "alto", "medio" o "bajo"). Una vez detectado el nivel genera un valor numérico 1, 2, ó 3.

```{r QC level detection}
 
qc_level <- function(df) {
  # Creamos una nueva columna NIVEL:
  df$NIVEL <- NA
  
  # Extraemos el último caracter del string:
  last_char <- substr(df$Control, nchar(df$Control), nchar(df$Control))
  
  # Condiciones para establecer el nivel basado en el nombre del control:
  df$NIVEL <- ifelse(df$CODIGO_PRUEBA == "CLC00544",
                     # Corregir el caso específico donde de CLC00544, donde CARDIAC BIORAD 3 corresponde al nivel 3, hs-TnI alto al 2 y hs-TnI medio al 1:
                     ifelse(df$Control == "CARDIAC BIORAD 3", 3,
                            ifelse(df$Control == "hs-TnI alto", 2,
                                   ifelse(df$Control == "hs-TnI medio", 1, NA))),
                     ifelse(grepl("\\b(bajo|medio|alto)\\b", df$Control, ignore.case = TRUE),
                            ifelse(grepl("\\bmedio\\b", df$Control, ignore.case = TRUE), 1,
                                   ifelse(grepl("\\balto\\b", df$Control, ignore.case = TRUE), 2, 3)),
                            ifelse(last_char %in% c("1", "2", "3"), as.integer(last_char), NA))
  )
  
  # Se reordena el dataframe: 
  df %<>% relocate("NIVEL", .after = 6)
  
  return(df)
}

```


## Función ```figure_ext```.

Función para extraer datos numéricos de la columna media y sd.

```{r figure_ext}

figure_ext <- function(df, col_number) {
  # Extrae números presentes en la columna especificada:
  mean_val <- c()
  sd_val <- c()
  for (i in 1:nrow(df[col_number])){
    numbers <- str_extract_all(df[i, col_number],
                             "\\d+(\\.\\d+)?") 
  
    # Convertir a decimal y hallar media y sd:
    numbers <- as.double(unlist(numbers))
    mean_val <- rbind(mean_val, 
                        (numbers[1] + numbers[2])/2)
    sd_val <- rbind(sd_val, 
                      (numbers[2] - numbers[1])/2)
  }
  
  # Crear nuevas columnas media y sd:
  
  df %<>% mutate("Media" = mean_val, "SD" = sd_val) %>%
          relocate("Media", "SD", .after =3)
  
  # Borrar columna original
  df[[col_number]] <- NULL
  
  # Devolver el dataframe modificado
  return(df)
}
```

## Función ```merge_date```.

Función para fusionar columnas Fecha y Hora en una nueva columna Fecha_hora y borrar las columnas individuales.

```{r merge_date}

  merge_date <- function(df){
    df %<>%
      mutate("TIEMPO_QC" = 
               as.POSIXct(paste(Fecha, Hora),
                          format = "%d/%m/%y %H:%M:%S",
                          tz=Sys.timezone()),
             .before = 1)
    # Borrar columnas originales
    df[["Fecha"]] <- NULL
    df[["Hora"]] <- NULL
        
  return(df)
}
```

## Función ```read_event```.

Función para leer los archivos csv de eventos y crear un data frame.

```{r read_event}

read_event <- function() {
  # Crea una lista de archivos existentes en datadir            correspondientes a los eventos de QC diarios:
  files_list <- list.files(eventdir, pattern = "Evento_",
                           full.names = T) 
  
  # Especifica los tipos de datos que contiene cada columna:
  col_types <- cols(
  FECHA = col_character(),
  LOTE = col_integer())
  
  # Crea una lista de dataframes leyendo cada archivo:
  df_list <- map(files_list,
                 ~read_delim(.x,
                             delim = "|",
                             col_types = col_types, 
                             col_select = -c("LOTE"),
                             trim_ws = T))
  
  # Crea el dataframe final:
  df <- bind_rows(df_list)
  
  # Convierte la fecha leída como string en formato POSIXct y cambia   nombre a columna:
  df[[1]] <- as.POSIXct(gsub(",", ".", df[[1]]),
               format = "%d/%m/%y %H:%M:%S",
               tz=Sys.timezone())
  colnames(df)[[1]] <- "TIEMPO_EVENTO"
  
  # Elimina substring no deseado en el nombre del            analizador:
  df[[2]] <- substring(df[[2]],10, 21)

  # Mueve el archivo leído a la carpeta old:
  #file.move(files_list, eventdir_old, overwrite = TRUE)
  
  return(df)
}
```

## Función merge_qc

Función para unir los dataframes features día D-1 y qc_results día D en uno solo mediante Left outer join.

```{r merge_qc_events}
merge_qc_events <- function(df1, df2){
  # Crear timestamps con solo la fecha: 
  df1$rounded_date <- floor_date(as.Date(df1[[1]]),
                                 unit = "day")
  df2$rounded_date <- floor_date(as.Date(df2[[1]]),
                                 unit = "day")
  
  # Crear un nuevo dataframe con merge (equivale a left outer join):
  merged_df <- merge(df1, df2, 
                     by = c("rounded_date", "ANALIZADOR",
                            "CODIGO_PRUEBA"), all.x = T)
  
  # Borrar los timestamps con solo fecha:
  merged_df$rounded_date <- NULL
  merged_df$rounded_date <- NULL
  
  return(distinct(merged_df))

}
```

## Función ```wt``` y ```wavelet_tr```.

Funciones para aplicar la transformación wavelet a la variable QC_RESULT para cada grupo de técnica. La función ```wt``` calcula la transformación wavelet de tipo Maximal Overlap Discrete Wavelet Transformation (MODWT), que devuelve los coeficientes de wavelet y los coeficientes de escalado. La función ```wavelet_tr``` aplica la transformación a un conjunto de datos agrupado por una determinada variable.

```{r wavelet function}
wt <- function(x, wavefun){
  mod <- modwt(x, wavefun, boundary = "periodic")
  return(mod)
}
```


```{r wavelet transformation}
wavelet_tr <- function(df) {
  # Aplicar la función wt a cada grupo por separado
  df_wavelet <- df %>%
    group_by(ANALIZADOR, CODIGO_PRUEBA) %>%
    mutate(
      across(QC_RESULT, ~ cbind(.x, as.data.frame(wt(.x, "la20")@W),
                                as.data.frame(wt(.x, "la20")@V)))
    ) %>%
    ungroup() %>%
    unnest(cols = QC_RESULT)
  
  # Reordenar columnas
  df_wavelet <- df_wavelet %>%
    relocate(starts_with("QC_RESULTADO"), .after = 1) %>%
    relocate(starts_with("W"), .after = 2) %>%
    relocate(starts_with("V"), .before = ANALIZADOR)
  
  # Sustituir valores NA en los coeficientes wavelet por un valor arbitrario -99999 para su posterior filtrado
  wavelet_cols <- grep("^[WV]", colnames(df_wavelet), value = TRUE)
  
  
  return(df_wavelet)
}
```


## Función ```normalize_train```.

Función de normalización de las variables continuas y eliminación de coeficientes wavelet con valores NA. Se definen dos funciones, una para el set de training y otra para los sets de test y validación. La diferencia es que la de train devuelve vectores de medias y de desviaciones estándar que luego se usan como argumentos en las otras. De esta forma, el escalado y normalización se realiza con la misma media y desviación del set de entrenamiento para los otros dos datasets.

```{r train data normalization and wavelet clean up}

normalize_train <- function(data) {
   
  # Identificar las columnas que empiezan por W y la columna 'RESULTADO'
  wavelet_cols <- grep("^[WV]", colnames(data), value = TRUE)
  result_col <- "QC_RESULT"
  
  # Calcular medias y desviaciones estándar por grupos de pruebas y   analizador:
  means <- data %>%
    group_by(ANALIZADOR, CODIGO_PRUEBA) %>%
    summarise(across(c(result_col, wavelet_cols),
                     ~mean(., na.rm = T))) 
  
  std_devs <- data %>% 
    group_by(ANALIZADOR, CODIGO_PRUEBA) %>% 
    summarise(across(c(result_col, wavelet_cols),
                     ~sd(., na.rm = T)))
  
  # Normalizar las columnas de resultado y coeficientes wavelet
  norm_data <- data %>%
    group_by(ANALIZADOR, CODIGO_PRUEBA) %>%
    mutate(across(c(result_col, wavelet_cols), scale))%>%
    mutate(across(wavelet_cols,
                           ~ifelse(is.na(.), -99999, .))) %>%
    ungroup()
  
  # Devolver el dataframe escalado y las medias y desviaciones estándar:
  
  return(list(scaled_data = as.data.frame(norm_data), 
              means = means, stdev = std_devs))
}
```

## Función ```norm_test_val```.

Función de normalización para los datos de validación y test, con eliminación de valores NA en los wavelets.

```{r val and test data normalization and wavelet clean up}

norm_test_val <- function(data, means, stds) {
  
  # Identificar las columnas numéricas y seleccionarlas
  wavelet_cols <- grep("^[WV]", colnames(data), value = TRUE)
  result_col <- "QC_RESULT"
  test_cols <- "CODIGO_PRUEBA"
  
  cols <- c(result_col, wavelet_cols, test_cols)
  
  # Seleccionar medias y desviaciones de resultados y coeficientes   wavelet:
  means_r <- train_n$means
  stds_r <- train_n$stdev
  means_wl <- train_n$means[wavelet_cols]
  stds_wl <- train_n$stdev[wavelet_cols]
  
  # Normalizar por cada prueba las columnas de resultado y coeficientes wavelet usando las medias y desviaciones estándar de train suministradas:
  means <- data %>%
    group_by(ANALIZADOR, CODIGO_PRUEBA) %>%
    left_join(means_r, by = c("ANALIZADOR", "CODIGO_PRUEBA"), 
              suffix = c(".val", ".means")) %>%
     ungroup()
   
  stds <- data %>%
    group_by(ANALIZADOR, CODIGO_PRUEBA) %>%
     left_join(stds_r, by = c("ANALIZADOR", "CODIGO_PRUEBA"),
               suffix = c(".val", ".sd")) %>%
    ungroup()
     
    
  norm_data <- data %>%
    mutate(., RESULTADO.st =
             (.[,result_col]-
                select(means,
                       starts_with(result_col) &
                       ends_with(".means"))) /
                select(stds,
                       starts_with(result_col) &
                       ends_with(".sd"))
           ) %>%
    mutate(WAVELETS.st =
             across(all_of(wavelet_cols),
                    ~ (. - select(means, 
                                  starts_with(cur_column()) &
                                    ends_with(".means"))[[1]]) /
                      select(stds,
                             starts_with(cur_column()) &
                               ends_with(".sd"))[[1]])
           ) %>%
    select(-c(result_col, wavelet_cols)) %>%
    unnest(c(WAVELETS.st, RESULTADO.st)) %>%
    relocate(starts_with("QC_"), .after = 1) %>%
    relocate(starts_with("W"), .after = 2) %>%
    relocate(starts_with("V"), .before = ANALIZADOR) %>%
    mutate(across(wavelet_cols,
                           ~ifelse(is.na(.), -99999, .)))
    
  names(norm_data)[2]<-result_col
  
  return(as.data.frame(norm_data))
}

```

## Función ```code_cat_var```.

Función para convertir las variables categóricas tipo string en vectores numéricos y codificarlas (one hot), así como normalizar los valores de las variables continuas, creando una lista que retiene en un vector la información de los niveles de los factores

```{r one-hot coding and normalization}

code_cat_var <- function(data) {
  # Eliminación de la columna CODIGO_PRUEBA si solo tiene una          categoría:
  if(length(unique(data$CODIGO_PRUEBA))==1){
    data %<>% select(., -CODIGO_PRUEBA)
    }
  
  # Reconversión de NIVEL a string:
  data$NIVEL <- as.character(data$NIVEL)
    # Detección de strings:
  cat_vars <- which(sapply(data, is.character))
  # Conversión a factores:
  data_cat <- lapply(data[,cat_vars], as.factor)
  # Extracción de niveles:
  levels <- lapply(data_cat, levels)
  
  # Codificar one-hot todas las variables factor:
  encoded_cols <- lapply(seq_along(data_cat), function(i) {
    cols <- model.matrix(~ factor(data_cat[[i]]) - 1)
    colnames(cols) <- paste0(names(data)[cat_vars[i]], "_",
                             levels[[i]])
    cols
    })
  
  # Unir columnas codificadas con conjunto de datos original:
  if (!"CODIGO_PRUEBA" %in% names(data)) {
    data_encoded <- bind_cols(data %>% 
                                select(-all_of(cat_vars)),
                                encoded_cols)
    } else {
      data_encoded <-  bind_cols(data %>%
                                   select(c(-all_of(cat_vars),
                                            -"CODIGO_PRUEBA")),
                                 encoded_cols)
      }
  
  return(list(data = data_encoded, levels = levels))
}
```

## Función ```unique_lab_cases```.

Función para crear un código de casos únicos de combinaciones de ANALIZADOR, CÓDIGO_PRUEBA y NIVEL.

```{r, indexing function}

unique_lab_cases <- function(df, categorical_vars) {
  unique_groups <- unique(df[categorical_vars])
  
  code <- apply(unique_groups, 1, function(row) {
    paste0(ifelse(row == 1, 1, 0), collapse = "")
  })
  unique_groups %<>% mutate(., code)
  coded_df <- df %>%
    merge(unique_groups, ., by = colnames(unique_groups)
          [-length(colnames(unique_groups))])
  coded_df <- coded_df[, c(colnames(df), "code")]
  
  return(coded_df)
}
```

## Función ```create_lstm_data.2```.

Función para creación de lotes de series temporales de longitud k (lookback).

```{r, k length time series generator function}

create_lstm_data.2 <- 
  function(data, lookback, delay, min_index, max_index,
           shuffle = FALSE, batch_size, step = 1,
           predseries) {
  
  if (is.null(max_index)) max_index <- nrow(data)
  i <- min_index + lookback - 1
  function() {
    if (shuffle) {
      rows <- sample(c((min_index+lookback):max_index), 
                     size = batch_size)
    } else {
      if (i + batch_size > max_index)
        i <<- min_index + lookback
      rows <- c(i:min(i+batch_size-1, max_index))
      i <<- i + length(rows)
}
    samples <- array(0, dim = c(length(rows),
                                lookback / step,
                                dim(data)[[-1]]))
    targets <- array(0, dim = c(length(rows)))
    print(dim(targets))
    for (j in 1:length(rows)) {
      indices <- (rows[[j]] - lookback + 1):(rows[[j]])
      samples[j,,] <- data[indices, ]
      targets[[j]] <- data[rows[[j]] + delay,predseries]
    }
    list(samples, targets)
  }
}

```



### Aleatorizadas:

```{r random custom generator}
train_gen <- function() {
  indices <- sample(length(train_k))
  
  for (i in indices) {
    sequence <- tryCatch(
      train_k[[i]](),
      error = function(e) {
        warning("Empty sequence error occurred in the generator.
                Trying the next one.")
        return(NULL)
      }
    )
    
    if (!is.null(sequence)) {
      return(sequence)
    }
  }
  
  NULL
}


val_gen <- function() {
  indices <- sample(length(val_k))
  
  for (i in indices) {
    sequence <- tryCatch(
      val_k[[i]](),
      error = function(e) {
        warning("Empty sequence error occurred in the generator.
                Trying the next one.")
        return(NULL)
      }
    )
    
    if (!is.null(sequence)) {
      return(sequence)
    }
  }
  
  NULL
}

test_gen <- function() {
  indices <- sample(length(val_k))
  
  for (i in indices) {
    sequence <- tryCatch(
      test_k[[i]](),
      error = function(e) {
        warning("Empty sequence error occurred in the generator.
                Trying the next one.")
        return(NULL)
      }
    )
    
    if (!is.null(sequence)) {
      return(sequence)
    }
  }
  
  NULL
}

```

# Carga y preprocesado de los datos.

## Lectura de archivos

```{r training and testing data read, include=FALSE}
# Lectura de los datos que se usarán para training y test

  #Leer los archivos de eventos de QC y crear el data frame events:
  events <- read_event()
  events %<>% unique()


  
  #Leer los archivos de resultados de QC y crear el data frame qc_results. Se eliminan las entradas con valores NA en TIEMPO_QC:
  qc_results <- read_qc()
  
  qc_results %<>% qc_level()
  qc_results <- qc_results %>% merge_date()
  qc_results <- figure_ext(qc_results, 3)
  qc_results <- qc_results[!is.na(qc_results$TIEMPO_QC),]
  
  length(which(is.na(qc_results$TIEMPO_QC)))

  
```

## Creación de un diccionario de correspondencia entre códigos y nombres de pruebas. Eliminación de nombres de prueba con caracteres especiales

```{r inmunochemical tests names}

# Pruebas incluídas en los datos:
  iqtests <- unique(data.frame(events$NOMBRE_PRUEBA,
                               events$CODIGO_PRUEBA))
  
  # Corrección de caracteres incorrectos en NOMBRE_PRUEBA:
  iqtests$events.NOMBRE_PRUEBA <- 
    iconv(iqtests$events.NOMBRE_PRUEBA, to = "UTF-8")
  iqtests <- iqtests[complete.cases(iqtests),]
  names(iqtests) <- c("NOMBRE_PRUEBA", "CODIGO_PRUEBA")

  # Sustitución en data de los nombres de prueba incorrectos:
  events$NOMBRE_PRUEBA <-
    iqtests$NOMBRE_PRUEBA[match(events$CODIGO_PRUEBA,
                                iqtests$CODIGO_PRUEBA)]
  
  length(which(is.na(iqtests$NOMBRE_PRUEBA)))

```

```{r merge dataframe}
# Unir los dos dataframes qc_results y events en uno solo.
qc_event <- merge_qc_events(qc_results, events) 

# Completar nombres de pruebas ausentes:
qc_event$NOMBRE_PRUEBA <-
    iqtests$NOMBRE_PRUEBA[match(qc_event$CODIGO_PRUEBA,
                                iqtests$CODIGO_PRUEBA)]
length(which(is.na(qc_event$NOMBRE_PRUEBA)))

head(qc_event)

```
## Selección de características y transformación wavelet.

```{r features selection and wavelet transformation}
# Selección de características de interés y recodificación de variables:

qc_data_sel <- qc_event %>% subset(.,
                      select = c(TIEMPO_QC, QC_RESULT, ANALIZADOR,
                                 CODIGO_PRUEBA, NOMBRE_PRUEBA, NIVEL))%>%
  unique(.)
  
```


```{r}
# Aplicación de transformación wavelet:
qc_data_sel_wl <- qc_data_sel %>% wavelet_tr()

```

## Guardado de archivo csv.

```{r save csv}
# Guardado de datos en archivo csv.

write.csv(qc_data_sel_wl, row.names = F,
          file = file.path(resultsdir, "qc_data_sel_wl"))

```

## Carga de datos desde archivo csv.

```{r data_sel_tr read csv, eval=FALSE}

qc_data_sel_wl <- read.csv(file.path(resultsdir, "qc_data_sel_wl"),
                        sep = ",")

# Convertir fechas en formato POSIXct:
qc_data_sel_wl[,1] %<>% as.POSIXct(tz = "Europe/Madrid")
```


## Modelo ARMA aplicado a los datos temporales.

```{r autocorrelation, partial autocorrelation an ARMA}

# Crear un archivo PDF para guardar los gráficos
pdf(file.path(figuresdir, "graficos.pdf"))

# Configurar la distribución de los gráficos en la página
par(mfrow = c(2, 3))
  
# Generar los gráficos
  qc_data_sel_wl %>%
  group_by(NOMBRE_PRUEBA, NIVEL) %>%
  nest() %>%
  mutate(acf_plots = map(data, ~ {
    acf(.x$QC_RESULT, type = "correlation", main="")
    title(main = unique(NOMBRE_PRUEBA),
          sub = paste0("NIVEL:", unique(NIVEL)),
          outer = F)
  })) %>%
  pull(acf_plots) %>%
  grid.arrange(grobs = ., ncol = 10)

# Restaurar los parámetros gráficos a su valor original
par(mfrow = c(1, 1))

# Cerrar el archivo PDF
dev.off()
```

## División de los sets de entrenamiento, validación y prueba. Normalización.

```{r training, validation and test dataset}
# Se divide el dataset en 50% de datos para entrenamiento, 25% para test y 25% para validación. Para evitar fraccionar las unidades temporales diarias usamos percentiles de la secuencia temporal en días: 

time_point_50 <- as.POSIXct(as.Date(
  quantile(qc_data_sel_wl[[1]], 0.5)))

time_point_75 <- as.POSIXct(as.Date(
  quantile(qc_data_sel_wl[[1]], 0.75)))

time_point_50
time_point_75
```


```{r training dataset subsetting and normalization, include=FALSE}
# Muestra de entrenamiento:
train_n <- qc_data_sel_wl %>%
  subset(.[[1]] <= time_point_50) %>%
  # Normalización del dataset y eliminación de coeficientes
  # wavelet con valor NA:
  normalize_train()
```


```{r validation dataset subsetting and normalization}
# Muestra de validación:
val <- qc_data_sel_wl %>%
  subset(
    .[[1]] > time_point_50 &
    .[[1]] <= time_point_75)

  # Normalización del dataset y eliminación de coeficientes
  # wavelet con valor NA:
  val_n <- val %>% norm_test_val(means = train_n$means, stds = train_n$stdev)
```


```{r test dataset subsetting and normalization}
# Muestra de test:
test <- qc_data_sel_wl %>%
  subset(.[[1]] > time_point_75)
  # Normalización del dataset y eliminación de coeficientes
  # wavelet con valor NA:
test_n <- test %>% norm_test_val(means = train_n$means, stds = train_n$stdev)
```

## Codificación one-hot.

### Sin wavelets
```{r training validation and test dataset onehot coding}
# Codificación onehot 

train_code <- code_cat_var(
  train_n$scaled_data[,-c(3:16, 19)]) 

val_code <- code_cat_var(val_n[,-c(3:16, 19)])

test_code <- code_cat_var(test_n[,-c(3:16, 19)])

```

## Creación de las funciones generadoras para las series temporales etiquetadas de los datasets de training, validación y prueba.

```{r trainig set case grouping and k length time series creation}
# Creación de series temporales de tamaño k con los datos de training:

# Creación de los códigos identificativos únicos de combinaciones de variables categóricas para el set de training usando la función unique_lab_cases e indicando las posiciones de las variables categóricas en train_code$data que se deseen incluir (en este caso el analizador): 

categorical_vars <- c(17:48)
train_indx <- unique_lab_cases(train_code$data,
                               categorical_vars = categorical_vars)

# Extraemos los códigos para realizar un diccionario de traducción de códigos únicos a índices numéricos:
code_values <- unique(train_indx[,"code"])
code_dictionary <- data.frame(code = code_values)
code_dictionary %<>%  
  mutate(., index = as.numeric(factor(code_values, 
                                      levels = code_values)))

# Sustituimos los códigos de train_indx por sus índices numéricos y aseguramos que train_index queda ordenado por fecha y hora ascendente anted de eliminar esta variable:
train_indx$index <- code_dictionary$index[match(train_indx$code, code_dictionary$code)]
index_values <- unique(train_indx[,"index"])
train_indx$code <- NULL
train_indx %<>% arrange(., train_indx$TIEMPO_QC)
train_indx$TIEMPO_QC <- NULL

# Hiperparámetros para generar las series temporales:
lookback <- 10  # Longitud de la secuencia previa al target.
delay <- 1 # Escoge como target el valor k+1 en el futuro.
batch_size <- 400 # Tamaño de batch.
min_index <- 1 # Indice de partida de la serie temporal.
predseries <- 1 # Posición en el dataset de la variable a predecir (QC_RESULT)


# Creación de las series temporales de longitud k por categorías de combinaciones únicas de analizador, código de prueba y nivel de QC:
 
train_k <- lapply(index_values, function(idx) {
  create_lstm_data.2(
    data = as.matrix(train_indx[train_indx[,"index"] == idx,
                                -c(2:15,length(val_indx))]),
    lookback = lookback,
    delay = delay,
    batch_size = batch_size,
    min_index = min_index,
    max_index = 
      nrow(as.matrix(
        train_indx[train_indx[,"index"] == idx,]))-delay-1,
    predseries = predseries)
})

# Cálculo del número de steps de training:
train_steps <- round((nrow(train_code$data)-lookback) / batch_size)

```


```{r Validation set case grouping and k length time series creation}

val_indx <- unique_lab_cases(val_code$data, 
                             categorical_vars = categorical_vars)

# Usamos el diccionario de códigos creado anteriormente, ordenamos por TIEMPO_QC antes de eliminar esta variable: 

val_indx$index <- code_dictionary$index[match(val_indx$code, code_dictionary$code)]
index_values <- unique(val_indx[,"index"])
val_indx$code <- NULL
val_indx %<>% arrange(., val_indx$TIEMPO_QC)
val_indx$TIEMPO_QC <- NULL

val_k <- lapply(index_values, function(idx) {
  create_lstm_data.2(
    data = as.matrix(val_indx[val_indx[,"index"] == idx,
                              -c(2:15, length(val_indx))]),
    lookback = lookback,
    delay = delay,
    batch_size = batch_size,
    min_index = min_index,
    max_index = 
      nrow(as.matrix(val_indx[val_indx[,"index"] == idx,]))-delay-1,
    predseries = predseries)
})

# Cálculo del número de steps de validación:
val_steps <- round((nrow(val_code$data)-lookback) / batch_size)
```


```{r Test set case grouping and k length time series creation}

test_indx <- unique_lab_cases(test_code$data, 
                             categorical_vars = categorical_vars)

# Usamos el diccionario de códigos creado anteriormente, ordenamos por TIEMPO_QC. Se deja esta variable aunque no se introduzca en la función generadora, para posteriormente realizar tests seleccionando fechas: 

test_indx$index <- code_dictionary$index[match(test_indx$code, code_dictionary$code)]
index_values <- unique(test_indx[,"index"])
test_indx$code <- NULL
test_indx %<>% arrange(., test_indx$TIEMPO_QC)

test_k <- lapply(index_values, function(idx) {
  create_lstm_data.2(
    data = as.matrix(test_indx[test_indx[,"index"] == idx,
                              -c(1, 3:16, length(test_indx))]),
    lookback = lookback,
    delay = delay,
    batch_size = batch_size,
    min_index = min_index,
    max_index = 
      nrow(as.matrix(test_indx[test_indx[,"index"] == idx,]))-delay-1,
    predseries = predseries)
})

# Cálculo del número de steps de validación:
test_steps <- round((nrow(test_code$data)-lookback) / batch_size)
```

## Uso de generadoras directas:

```{r}
train_data <- as.matrix(train_code$data[,-1])
lookback <- 20
batch_size <- 100
delay <- 1
min_index <- 1
predseries <- 1

train_k <-  create_lstm_data.2(
    data = train_data,
    lookback = lookback,
    delay = delay,
    batch_size = batch_size,
    min_index = min_index,
    max_index = nrow(train_data),
    predseries = predseries)

# Cálculo del número de steps de training:
train_steps <- round((nrow(train_data)-lookback) / batch_size) 
```

```{r}
val_data <- as.matrix(val_code$data[,-1])

val_k <- create_lstm_data.2(
    data = val_data,
    lookback = lookback,
    delay = delay,
    batch_size = batch_size,
    min_index = min_index,
    max_index = nrow(val_data),
    predseries = predseries)

# Cálculo del número de steps de training:
val_steps <- round((nrow(val_data)-lookback) / batch_size) 
```

```{r}
test_data <- as.matrix(test_code$data[,-1])

test_k <-  create_lstm_data.2(
    data = test_data,
    lookback = lookback,
    delay = delay,
    batch_size = batch_size,
    min_index = min_index,
    max_index = nrow(test_data),
    predseries = predseries)


# Cálculo del número de steps de training:
test_steps <- round((nrow(test_data)-lookback) / batch_size) 
```

# Definición del modelo LSTM.

```{r model definition, eval=FALSE}
set.seed(123)

nkp <- dim(train_k()[[1]])

k <- nkp[2]
p <- nkp[3]

input_shape <- c(NA, k, p)

model_2 <- keras_model_sequential() %>%
   #layer_masking(mask_value = -99999, input_shape = c(NULL, k, p)) %>%
   layer_lstm(units = 8, 
              #dropout=0.8, 
            #  recurrent_dropout=0.1,
              #kernel_regularizer = regularizer_l2(l=0.01),
              return_sequences = F,
              input_shape = c(NULL, k, p)) %>%
    #layer_lstm(units = 64, 
     #          dropout=0.8, 
      #         recurrent_dropout=0.5) %>%
    layer_dense(units = 1,
                activation = "sigmoid")

```

## Compilación.

```{r custom rmse metrics function}
K <- keras::backend()

rmse <- function(y_pred, y_true) {
  sq_diff <- K$square(y_pred - y_true)
  mean_sq_diff <- K$mean(sq_diff)
  rmse_value <- K$sqrt(mean_sq_diff)
  return(rmse_value)
}

attr(rmse, "py_function_name") <- "rmse"
```


```{r model compilation and callbacks}
model_0 %<>% compile(
    optimizer = "adam",
    loss="mse", 
    metrics = list("mae",
                   "mse",
                   rmse
                   )
)

callbacks <- list(
  callback_tensorboard(log_dir=file.path(resultsdir, "run_a")
                      # write_grads = T,
                      # histogram_freq = 1,
                      # update_freq = "batch",
                      # write_images = T
                      ),
  callback_model_checkpoint(file.path(resultsdir, 
                                      paste0("lstm_2_",
                                             format(Sys.time(),
                                                    "%Y%m%d_%H%M%S"),
                                             ".keras")),
                            monitor= "rmse",
                            mode="min",
                            verbose=1,
                            save_best_only = TRUE),
  callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.1,
                                patience = 10, min_lr = 0.000001),
  callback_early_stopping(monitor = "val_loss", patience = 20,
                          verbose = 1)
  )
```

## Entrenamiento.

```{r model fit}
history_0 <- model_0 %>% fit_generator(
  generator = train_k,
  steps_per_epoch = train_steps,
  epochs = 10,
  validation_data = val_k,
  validation_steps = val_steps,
  callbacks = callbacks
)


plot(history_2)
```

# Evaluación

```{r, eval=FALSE}
set.seed(123)
# Evaluación del modelo:
path <- file.path(resultsdir, "lstm_2.keras/TRUE")
model <- load_model_hdf5(path, 
                         custom_objects = list("rmse" =
                                                 rmse),
                         compile = T) 
evaluation_result <- evaluate(model, test_k, steps = test_steps)

evaluation_result
```

### Línea base de sentido común

```{r common sense baseline}
# Función para calcular el MAE promedio para un modelo basado en predecir los targets del set de validación con las mismas muestras de validación, mediante la iteración de lotes de muestras. Esto sería equivalente a un modelo que predice el resultado futuro del QC con el actual y constituye el punto de partida para mejorar (common sense baseline).

evaluate_naive_method <- function() {
  batch_mae <- c()
  batch_mse <- c()
  batch_rmse <- c()
  for (step in 1:val_steps) {
    c(samples, targets) %<-% val_k()
    preds <- samples[,dim(samples)[[2]],2]
    mae <- mean(abs(preds - targets))
    mse <- mean((preds - targets)^2)
    rmse <- sqrt(mse)
    batch_mae <- c(batch_mae, mae)
    batch_mse <- c(batch_mse, mse)
    batch_rmse <- c(batch_rmse, rmse)
  }
  naive_mse <- mean(batch_mse)
  naive_mae <- mean(batch_mae) 
  naive_rmse <- mean(batch_rmse) 
  cat("naive_mae", mae, "\n", 
      "naive_mse", mse, "\n", 
      "naive_rmse", rmse)
  naive <- list(naive_mae, naive_mse, naive_rmse)
  names(naive) <- c("mae", "mse", "rmse")
  return(naive)
  
}


# mae y mse del modelo naive:
naive <- evaluate_naive_method()

# mae del modelo LSTM: 
mae <- evaluation_result[2]

eval_model <- cbind(train_n$stdev[[2]], train_n$stdev[[1]],
                    train_n$stdev[[3]], 
                    naive$mae*train_n$stdev[[3]],
                    mae*train_n$stdev[[3]])

colnames(eval_model) <- c("CODIGO_PRUEBA", "ANALIZADOR",
                          "SD","NAIVE_MAE*SD","MAE*SD")

# Imprimir los resultados de evaluación:

sink(file.path(resultsdir, "model_2.txt"))

naive
evaluation_result
eval_model

# Cerrar el archivo de salida
sink()

```

# Validación cruzada Day Forward Chain

Consiste en considerar cada unidad de tiempo del dataset como un punto para dividirlo, de manera que todos los puntos previos forman una cadena consecutiva de puntos que se usa como set de entrenamiento. Los puntos posteriores a ese set se utilizan como conjunto de test. A su vez, el set de entrenamiento se divide en un subset de entrenamiento y otro de validación y se entrena con ellos. En una nueva iteración se aumentará el tamaño de la cadena, repitiendo el proceso. Para simplificar la elección de los puntos donde se inicia la cadena, se usarán percentiles de la serite temporal en días, y se crearán funciones generadoras que tomarán min y max_index de manera acorde, realizando los entrenamientos todo en una sola función.

## Creación de funciones generadoras para el Day Forward Chain
Cada función tendrá asignado un rango temporal de datos según un esquema incremental en el una vez dividido el espacio muestral en percentiles, se asignan los primeros al dataset de training y el último al de validación, incrementándose el dataset de training en cada ciclo, y manteniéndose el tamaño del de validación.
```{r}
# Muestra de validación:
forward_chain_cv <- function(s){
  # quantiles dependientes del número de divisiones que se pase (s):
  q <- seq(1/s, 1, 1/s)
  
  # Cálculo de las fechas correspondientes a cada cuantil
  time_points <- 
    do.call(quantile,
            list(as.POSIXct(as.Date(qc_data_sel_wl[[1]])), q))
  
  # Listas para conener las funciones generadoras
  train_ks <- list()
  val_ks <- list()
  
  # Loop a lo largo de los percentiles de tiempo:
  for(i in 1:(length(time_points)-1)){
  
  # Workflow aplicado a cada forward chain:
    
    train_n <- qc_data_sel_wl %>%
      subset(.[[1]] < time_points[i]) %>%
      normalize_train(.)
    
    val_n <- qc_data_sel_wl %>% 
      subset(.[[1]] > time_points[i] & .[[1]] <= time_points[i+1]) %>%
      norm_test_val(., means = train_n$means, stds = train_n$stdev)
    ###################################################################
    train_code <- code_cat_var(train_n$scaled_data[,-c(3:16, 19)])
    val_code <- code_cat_var(val_n[,-c(3:16, 19)])
    ###################################################################
    
    # Crear una lista de funciones generadoras para cada cadena:
    
    train_k <- create_lstm_data.2(
        data = as.matrix(train_code$data[,-1]),
        lookback = lookback,
        delay = delay,
        batch_size = batch_size,
        min_index = 1,
        max_index = nrow(train_code$data),
        predseries = predseries)
    
    val_k <- create_lstm_data.2(
        data = as.matrix(val_code$data[,-1]),
        lookback = lookback,
        delay = delay,
        batch_size = batch_size,
        min_index = 1,
        max_index = nrow(val_code$data),
        predseries = predseries)
    
    train_ks[[i]] <- train_k
    val_ks[[i]] <- val_k
  
    train_ks %>% setNames(paste0("train_", i, sep = "")) 
  }
  chain <- list(train_ks, val_ks)
  names(chain) <- c("train", "val")

  return(chain)

}
  
  
chain <- forward_chain_cv(4)

```
## Entrenamiento y validación con las funciones generadoras Day Forward Chain:


```{r}

train_model_multiple <- function(model, chain, train_steps, val_steps,
                                 epochs, callbacks) {
  
  history <- list()  # Lista para almacenar los historiales de entrenamiento
  
  for (i in seq_along(chain)) {
    train_k <- chain$train[[i]]  # Obtener train_k desde chain
    val_k <- chain$val[[i]]  # Obtener val_k desde chain
    
    history[[i]] <- model %>% fit_generator(
      generator = train_k,
      steps_per_epoch = train_steps,
      epochs = epochs,
      validation_data = val_k,
      validation_steps = val_steps,
      callbacks = callbacks
    )
  }
  
  return(history)
}


cross_val_history <- train_model_multiple(model_0,
                                          chain = chain,
                                          train_steps = train_steps,
                                          val_steps = val_steps,
                                          epochs = 10,
                                          callbacks = callbacks)
```



### Evaluación por categorías.

```{r model evaluation per category}
# Función para seleccionar los índices en test_indx que corresponden a una combinación determinada de CODIGO_PRUEBA, NIVEL Y ANALIZADOR: 

index_select <- function() {
  
  # Opciones para elegir CLC, NIVEL y ANALIZADOR:
  clc_opt <- test_code$levels$CODIGO_PRUEBA
  level_opt <- test_code$levels$NIVEL
  anal_opt <- test_code$levels$ANALIZADOR
  
  # Introducción de la fecha por el usuario:
  #date <- readline(
   # "fecha del día anterior a la predicción (aaaa-mm-dd): ")
  #if(!date %in% as.Date(test_code$data$TIEMPO_QC))
   #  print("fecha no incluida en dataset")
  
  # Introducción de prueba, nivel y analizador:
  clc <- clc_opt[menu(clc_opt, 
                      title = "Código CLC de prueba: ",
                      graphics = T)]
  level <- level_opt[menu(level_opt, 
                          title = "Nivel de QC: ",
                          graphics = T)]
  anal <- anal_opt[menu(anal_opt, 
                        title = "Analizador: ",
                        graphics = T)]
    
  cat("Indice correspondiente a la prueba: ", clc, "\n", 
  "nivel: ", level, "\n",
  "analizador: ", anal, "\n")
  
  # Conversión de las variables introducidas en índice:
  
  col_clc <- which(grepl(clc, names(test_indx)))-16
  col_level <- which(grepl(paste0("NIVEL_", level), names(test_indx)))-16
  col_anal <- which(grepl(anal, names(test_indx)))-16
  pos <- list(col_clc, col_level, col_anal)
  
  code <- rep(0, length(categorical_vars+2))
  
  for (i in seq_along(pos)) {
    code[pos[[i]]] <- 1 
  }
  code <- paste0(as.character(code), collapse = "")
  
  index <- code_dictionary$index[match(code,
                                       code_dictionary$code)]
  print(index)
  return(index)
  
}

index_pred <- index_select()

  test_pred <- lapply(index_pred, function(idx) {
    create_lstm_data.2(
      data = as.matrix(test_indx[test_indx[,"index"] == idx,
                                 -c(1, 3:16, length(test_indx))]),
      lookback = lookback,
      delay = delay,
      batch_size = batch_size,
      min_index = min_index,
      max_index = 
      nrow(as.matrix(test_indx[test_indx[,"index"] == idx,]))-delay-1,
      predseries = predseries)
  }
  )

```


```{r model prediction and metrics}

# Obtener las series y sus predicciones utilizando la función generadora de tests por categorías y obtener la predicción del modelo:
x <- test_pred[[1]]()[[1]]
y_pred <-  predict(model, x = x)
y_true <- test_pred[[1]]()[[2]]

metrics <- list(
  MSE <- mse(actual = y_true, predicted = y_pred),
  MAE <- mae(actual = y_true, predicted = y_pred),
  Correlación <- cor(y_pred, y_true)
)
names(metrics) <- c("MSE", "MAE", "Coef. correlación")
metrics

# Crear el gráfico de dispersión entre y_pred e y_true
plot(y_true, y_pred, pch = 16, col = "blue", xlab = "y_true", ylab = "y_pred", 
     main = "Gráfico de dispersión: y_true vs. y_pred")

# Calcular el coeficiente de correlación
correlacion <- cor(y_true, y_pred)

# Agregar el coeficiente de correlación al gráfico
texto_cor <- paste("Correlación:", round(correlacion, 2))
mtext(texto_cor, side = 3, line = -2.5)


```

```{r all metrics by categories}
# Calcular las métricas de todas las categorías:

global_metrics <- list()

for(i in seq_along(test_k)){
  
  # Calcula x, y_true e y_pred a partir de la primera iteración de las funciones generadoras y el modelo:
  x <- test_k[[i]]()[[1]]
  y_true <- test_k[[i]]()[[2]]
  y_pred <-  predict(model, x = x)

  metrics <- list(
  MSE <- mse(actual = y_true, predicted = y_pred),
  MAE <- mae(actual = y_true, predicted = y_pred),
  Correlación <- cor(y_pred, y_true)
  )

  names(metrics) <- c("MSE", "MAE", "Coef. correlación")
  global_metrics[[i]] <- metrics

}

names(global_metrics) <- index_values

# Crear un gráfico para cada elemento de las sublistas en global_metrics
for (i in 1:3) {
  plot(1:length(global_metrics), 
       sapply(global_metrics, "[[", i),
       pch = 16, 
       xlab = "Index",
       ylab = names(global_metrics[[1]][i])
  )
}


```

```{r index search}
# Función para hacer la búsqueda inversa de analizador, prueba y nivel a partir de un index:

reverse_index_select <- function(index) {
  # Buscar el código correspondiente al índice
  code <- code_dictionary$code[match(index, code_dictionary$index)]
  
  # Extraer los dígitos de code
  digits <- strsplit(code, "")[[1]]
  
  # Encontrar las posiciones de los 1 en la secuencia de dígitos
  pos_anal <- which(digits == "1")[1]
  pos_clc <- which(digits == "1")[2]
  pos_level <- which(digits == "1")[3]
  
  # Obtener los valores de anal, clc y level
  anal <- names(test_indx)[16 + pos_anal]
  clc <- names(test_indx)[16 + pos_clc]
  level <- names(test_indx)[16 + pos_level]
  
  # Devolver los valores encontrados
  result <- list(CÓDIGO_PRUEBA = substr(clc, 15, 22),
                 NIVEL = level, 
                 ANALIZADOR = anal)
  return(result)
}




```

```{r problematic cases search}

# Búsqueda de casos con métricas por debajo de la media:

limit_mse <- mse
limit_mae <- mae
limit_coef <- 0.7

cases <- c()  # Lista para almacenar los índices

for (i in seq_along(global_metrics)) {
  
  sublist <- global_metrics[[i]]  
  
  # Extracción de métricas:
  mse_i <- sublist$MSE
  mae_i <- sublist$MAE
  coef_i <- sublist$`Coef. correlación` 
  
  if (mse_i >= limit_mse & mae_i >= limit_mae &
      coef_i <= limit_coef) {
    cases <- c(cases, i)  # Agregar el índice a la lista
  }
}

# Imprimir los índices encontrados

print(cases)

# Transformar índices a información de casos:

selection <-list()

for(i in seq_along(cases))
selection[[i]] <- reverse_index_select(cases[i])


```



